
% Default to the notebook output style

    


% Inherit from the specified cell style.




    
\documentclass[11pt]{article}

    
    
    \usepackage[T1]{fontenc}
    % Nicer default font (+ math font) than Computer Modern for most use cases
    \usepackage{mathpazo}

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % We will generate all images so they have a width \maxwidth. This means
    % that they will get their normal width if they fit onto the page, but
    % are scaled down if they would overflow the margins.
    \makeatletter
    \def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth
    \else\Gin@nat@width\fi}
    \makeatother
    \let\Oldincludegraphics\includegraphics
    % Set max figure width to be 80% of text width, for now hardcoded.
    \renewcommand{\includegraphics}[1]{\Oldincludegraphics[width=.8\maxwidth]{#1}}
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionLabelFormat{nolabel}{}
    \captionsetup{labelformat=nolabel}

    \usepackage{adjustbox} % Used to constrain images to a maximum size 
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range 
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    

    
    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatability definitions
    \def\gt{>}
    \def\lt{<}
    % Document parameters
    \title{Python for Linguists}
    
    
    

    % Pygments definitions
    
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % Exact colors from NB
    \definecolor{incolor}{rgb}{0.0, 0.0, 0.5}
    \definecolor{outcolor}{rgb}{0.545, 0.0, 0.0}



    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

    \begin{document}
    
    
    \maketitle
    \newpage
    \tableofcontents
    \newpage
    
    

    
    \hypertarget{python-and-nlp-for-linguists}{%
\section{Python and NLP for
Linguists}\label{python-and-nlp-for-linguists}}

I'm going to try something novel: giving this talk from a Jupyter
notebook so I can run code on the fly.

While I was writing up this notebook, this talk turned into a broad
introduction to NLP as well as Python. Oh well.

    \hypertarget{who-is-this-guy}{%
\subsection{Who is this guy?}\label{who-is-this-guy}}

\begin{itemize}
\tightlist
\item
  Henry Anderson
  (\href{mailto:henry.anderson@uta.edu}{\nolinkurl{henry.anderson@uta.edu}})
\item
  Data Scientist in the University Analytics department
\item
  Specialist in unstructured data (i.e., text), machine learning, and
  Natural Language Processing
\item
  First year Linguistics masters student, with interests in
  computational social science, digital language use, and the language
  of online communities and networks.
\end{itemize}

    \hypertarget{contents}{%
\subsection{Contents}\label{contents}}

\begin{itemize}
\tightlist
\item
  Who stands to gain the most
\item
  Why consider \emph{programming,} generally?
\item
  Why consider \emph{Python,} specifically?
\item
  Some demos:

  \begin{itemize}
  \tightlist
  \item
    Custom concordance code, with massive flexibility
  \item
    Quick n-gram analysis
  \item
    Automatic dependency parsing, POS-tagging, lemmatization,
    tokenization, etc. (i.e., text preprocessing)
  \item
    Topic models
  \item
    Word vectors
  \item
    Classification and regression tasks with text
  \end{itemize}
\end{itemize}

    \hypertarget{goals-of-this-talk}{%
\subsection{Goals of this talk}\label{goals-of-this-talk}}

\begin{itemize}
\tightlist
\item
  Put some basic computational/programming tools on your radar.
\item
  Give a sense of what \emph{type} of work can be done with these tools.
\item
  Make you generally aware of the scope and nature of computational
  tools.
\item
  Give some \emph{very} basic exposure to Python.

  \begin{itemize}
  \tightlist
  \item
    We'll walk through some very basic code examples, but we'll skim
    over most of them.
  \end{itemize}
\end{itemize}

\hypertarget{this-talk-is-not}{%
\subsection{\texorpdfstring{This talk is
\emph{not\ldots{}}}{This talk is not\ldots{}}}\label{this-talk-is-not}}

\begin{itemize}
\tightlist
\item
  A tutorial on Python, the dataset, or the libraries.

  \begin{itemize}
  \tightlist
  \item
    That can come later, if people are interested.
  \end{itemize}
\item
  A tutorial in natural language processing, text processing, or big
  data.
\item
  Really a tutorial in anything.
\end{itemize}

    \hypertarget{who-stands-to-gain}{%
\subsection{Who stands to gain}\label{who-stands-to-gain}}

\begin{itemize}
\tightlist
\item
  Anyone who deals with \emph{data:} people interested in corpus work,
  sociolinguistics, natural language processing, digital/online
  language, etc.
\item
  Anyone interested in \emph{computational social science} (CSS):
  i.e.~general social science approaches leveraging large datasets and
  computational horsepower.

  \begin{itemize}
  \tightlist
  \item
    CSS is currently exploding, and is a hugely important avenue for
    applied social science research.
  \item
    CSS is also massively interdisciplinary: programming, statistics,
    machine learning, AI, network analysis, linguistics, sociology,
    psychology, etc all combine to make CSS happen.
  \end{itemize}
\item
  If you deal mostly with theory, or are primarily an experimentalist,
  you may stand to gain less from this talk. (But you're still welcome!)
\end{itemize}

    \hypertarget{what-does-programming-offer}{%
\subsection{What does programming
offer?}\label{what-does-programming-offer}}

\begin{itemize}
\item
  (Quite literally) infinite control over your data processing: you're
  not limited by the features someone else decided to code into their
  program--you can change your code up to do anything you want.
\item
  Scalability and automation of your data work

  \begin{itemize}
  \tightlist
  \item
    Work with literally millions of documents and billions of words with
    relative ease.
  \item
    Automate steps from data collection through final analysis.
  \end{itemize}
\item
  Some types of analysis simply are infeasible to do by hand--network
  analysis, topic modeling, word embeddings, etc--and \emph{have} to be
  done computationally.
\item
  Marketable skills: even a little bit of Python, Java, or any other
  language can open doors in the job market.
\item
  You'll feel like a badass.
\end{itemize}

    \hypertarget{what-does-python-offer}{%
\subsection{\texorpdfstring{What does \emph{Python}
offer?}{What does Python offer?}}\label{what-does-python-offer}}

\begin{itemize}
\item
  Free (as in speech, not beer. But also as in beer), open-source,
  royalty-free. No licenses to sign, no royalties to pay, and
  \emph{essentially no restrictions} on what you can and can't do with
  it. (the \href{https://docs.python.org/3/license.html}{Python Software
  Foundation license} is an extremely permissive BSD-type license)
\item
  Easy-to-learn language.

  \begin{itemize}
  \tightlist
  \item
    Very user-friendly language with very friendly users.
  \item
    Great documentation and stupid amounts of free, high-quality
    learning resources.
  \item
    Among its core ideas:

    \begin{itemize}
    \tightlist
    \item
      Code is read far more than it is written, so the language should
      be \emph{human-readable.}
    \item
      ``There should be one, and preferably only one, obvious way to do
      it.'' I.e., the most straightforward approach is \emph{usually}
      the best. (This results in a lot of people writing
      straightforward, fairly easy-to-follow code).
    \end{itemize}
  \item
    Commonly taught as a first programming language, so there are LOTS
    of materials for eveyone from beginning programmers to seasoned
    professionals; the Python community is also very welcoming of
    newcomers.
  \end{itemize}
\item
  General purpose language: can do (almost) everything you want to make
  it to.

  \begin{itemize}
  \tightlist
  \item
    Compare to R, which is great for statistics, and a pain for a lot of
    other stuff.
  \item
    Or Matlab, which is great for being a broken, slow, difficult
    software environment, and isn't so good at being, well, good.

    \begin{itemize}
    \tightlist
    \item
      (this has been your mandatory ``Matlab is bad'' comment)
    \end{itemize}
  \end{itemize}
\item
  Rapidly becoming \emph{the} language for data science, displacing even
  R in most applications. (R is still dominant for raw statistics,
  though Python has plenty of packages that implement common statistical
  tests).

  \begin{itemize}
  \tightlist
  \item
    Though, keep an eye on a different language--Julia--over the next
    few years. It is truly a worthy contender, but has yet to hit
    version 1.0 as of this talk.
  \end{itemize}
\item
  \textbf{For linguists}: a \emph{huge} array of language processing
  functionality and libraries.

  \begin{itemize}
  \tightlist
  \item
    \href{https://spacy.io}{spaCy}, basically a Python version of
    Stanford's CoreNLP toolkit (lemmatization, tokenization, dependency
    parsing, POS tagging, and more).
  \item
    \href{https://radimrehurek.com/gensim/}{Gensim}, full of topic
    models and pretty bleeding-edge NLP tools.
  \item
    \href{http://www.nltk.org/}{Natural Language Toolkit (NLTK)}, a
    \emph{massive} library that's designed to teach a lot of NLP
    concepts (but can be used for some serious production work too).
  \item
    \href{http://pandas.pydata.org/}{Pandas} for R-like dataframes,
    statistics, and general tabular data management.
  \item
    \href{https://matplotlib.org/}{Matplotlib} (and others like
    \href{https://seaborn.pydata.org/}{Seaborn},
    \href{http://www.pygal.org/en/stable/}{PyGal},
    \href{https://bokeh.pydata.org/en/latest/}{Bokeh}, \ldots{}) for
    high-quality, powerful data visualization.
  \item
    \href{http://scikit-learn.org/stable/index.html}{scikit-learn} for
    non-neural machine learning (support vector machines, random
    forests, and a few text features like basic preprocessing)

    \begin{itemize}
    \tightlist
    \item
      Side note, the scikit-learn
      \href{http://scikit-learn.org/stable/user_guide.html}{User Guides}
      are an \emph{excellent} technical crash course in machine
      learning, even if you're not too interested in Python.
    \end{itemize}
  \item
    \href{https://networkx.github.io/}{Networkx} for performing network
    analysis.
  \item
    \href{https://www.tensorflow.org/}{Tensorflow}+\href{https://keras.io/}{Keras},
    for quickly and easily building neural networks.
  \item
    \href{http://pytorch.org/}{PyTorch}, an up-and-coming (but extremely
    exciting) neural network library.
  \item
    And dozens more.
  \end{itemize}
\end{itemize}

    \hypertarget{some-links}{%
\section{Some links}\label{some-links}}

\begin{itemize}
\tightlist
\item
  \href{https://www.python.org/}{Python.org}, the official Python
  website.

  \begin{itemize}
  \item
    \href{https://www.python.org/downloads/release/python-364/}{Download
    Python 3.6.4,} the latest releast as of writing this.
  \item
    \emph{Or,} download the \href{https://www.anaconda.com/}{Anaconda}
    distribution, which comes pre-packaged with common data science
    libraries and can make library management easier (at the expense of
    being a larger installation).
  \item
    \href{https://docs.python.org/3/library/index.html}{Python Standard
    Library reference}
  \item
    \href{https://docs.python.org/3/tutorial/index.html}{Python's
    official, basic tutorial}. It's not the best out there, but it's
    pretty good, and pretty quick. Highly recommended if you've already
    got a bit of programming experience.
  \item
  \end{itemize}
\item
  \href{https://www.jetbrains.com/pycharm/}{PyCharm}, an excellent
  Python editor (download the community edition, not the professional
  edition--it's free and still has more features than most people need)
\item
  \href{https://automatetheboringstuff.com/}{Automate the Boring Stuff
  with Python}, a free (Creative Commons-licensed) eBook that introduces
  Python via concrete, hands-on examples and projects.
\item
  \href{https://www.reddit.com/r/learnpython/}{r/learnpython}, a
  subreddit dedicated to people learning Python and asking questions
  about the language.
\item
  \href{https://projecteuler.net/archives}{Project Euler}, a collection
  of math problems designed to be used as programming practice (in any
  language).
\item
  \href{http://rosalind.info/problems/locations/}{Rosalind}, another
  collection of practice problems, but geared at genetics and
  bioinformatics.
\item
  PyCon's YouTube channels
  (\href{https://www.youtube.com/channel/UCrJhliKNQ8g0qoE_zvL8eVg}{2017},
  \href{https://www.youtube.com/channel/UCwTD5zJbsQGJN75MwbykYNw}{2016},
  etc) have a lot of good videos, including some long-form tutorials and
  workshops.
\item
  \href{https://www.youtube.com/user/PyDataTV}{PyData}, a data science
  themed Python conference/convention, posts almost all of their talks
  to YouTube.
\end{itemize}

    \hypertarget{running-this-notebook}{%
\section{Running this notebook}\label{running-this-notebook}}

To run this notebook, you will need to install the following Python
libraries: * Jupyter (to run/view the notebook itself) * Gensim * spaCy
* You'll need to download two of spaCy's language models:
\texttt{en\_core\_web\_sm} and \texttt{en\_core\_web\_lg}. Installation
instructions are \href{https://spacy.io/models/}{here.} * Numpy *
Scikit-learn (goes by sklearn when installing) * Matplotlib * Natural
Language Toolkit (NLTK) * tqdm * Pandas

Open Jupyter and navigate to this .ipynb file, then open it. Every major
section is designed to be able to run independently, minus the ``Setup
Code'' section, which should always be run first.

For the first part of the talk, you'll also need to download
\texttt{glen\ carrig.txt} from the Github repository and have it in the
same folder as this notebook.

For the second part of the talk, you'll need to download the
\href{http://u.cs.biu.ac.il/~koppel/BlogCorpus.htm}{Blog Authorship
Corpus} and unzip its files into a folder named ``blogs''
(case-sensitive) in the same folder as this notebook.

    \hypertarget{setup-code}{%
\section{Setup Code}\label{setup-code}}

    First, some necessary setup--we just need to have this program create
some folders to save stuff into.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}1}]:} \PY{k+kn}{import} \PY{n+nn}{os}
        
        \PY{k}{if} \PY{o+ow}{not} \PY{n}{os}\PY{o}{.}\PY{n}{path}\PY{o}{.}\PY{n}{isdir}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{corpus data files}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{p}{:}
            \PY{n}{os}\PY{o}{.}\PY{n}{mkdir}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{corpus data files}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
        \PY{k}{if} \PY{o+ow}{not} \PY{n}{os}\PY{o}{.}\PY{n}{path}\PY{o}{.}\PY{n}{isdir}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{model files}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{p}{:}
            \PY{n}{os}\PY{o}{.}\PY{n}{mkdir}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{model files}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}


    \hypertarget{some-basic-demos}{%
\section{Some basic demos}\label{some-basic-demos}}

    \hypertarget{concordances}{%
\subsection{Concordances}\label{concordances}}

    Concordances can be done with regular expressions and a teeny tiny bit
of legwork. (By the way: if you're working with text, you have no excuse
to not learn regular expressions. But that would be another talk all
unto itself). We'll work with the text of William Hope Hodgeson's novel
\href{https://en.wikipedia.org/wiki/The_Boats_of_the_\%22Glen_Carrig\%22}{\emph{The
Boats of the ``Glen Carrig''}}, a 1907 horror novel. The text was taken
\href{http://www.gutenberg.org/ebooks/10542}{from Project Gutenberg},
with the site's boilerplate text removed from the front and back.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}2}]:} \PY{o}{\PYZpc{}\PYZpc{}}\PY{k}{time}
        
        import re
        
        def concordance(text, token, window=50):
            pattern = re.compile(r\PYZdq{}\PYZbs{}b\PYZob{}\PYZcb{}\PYZbs{}b\PYZdq{}.format(token.strip()), re.IGNORECASE)
            \PYZsh{} convert all whitespaces to single space characters
            text = re.sub(r\PYZdq{}\PYZbs{}s+\PYZdq{}, \PYZdq{} \PYZdq{}, text)
            for i in pattern.finditer(text):
                print(
                    \PYZdq{}...\PYZdq{},
                    text[i.start() \PYZhy{} window:i.start()].rjust(window, \PYZdq{} \PYZdq{}),
                    text[i.start():i.start() + window].ljust(window, \PYZdq{} \PYZdq{}),
                    \PYZdq{}...\PYZdq{},
                    sep=\PYZdq{}\PYZdq{}
                )
        
        glen\PYZus{}carrig = open(\PYZdq{}glen carrig.txt\PYZdq{}, \PYZdq{}r\PYZdq{}, encoding=\PYZdq{}utf8\PYZdq{}).read()
        concordance(glen\PYZus{}carrig, \PYZdq{}think\PYZdq{}, window=30)
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\ldots}that land. For, indeed, now I think of it, I can remember th{\ldots}
{\ldots}, indeed, we had ever need to think more of such. And then, {\ldots}
{\ldots}ces on my throat he seemed to think but little, suggesting t{\ldots}
{\ldots}e; so that I knew not what to think, being near to doubting {\ldots}
{\ldots}rawling in the valley. Yet, I think the silences tried us th{\ldots}
{\ldots}her, and further than this, I think with truth I may say, we{\ldots}
{\ldots}fter so long upon the sand. I think, even thus early, I had {\ldots}
{\ldots}me thus armed, they seemed to think that I intended a jest, {\ldots}
{\ldots}ch that it seemed hopeless to think of success; but, for all{\ldots}
{\ldots}red in me; nor, could I, do I think I would; for were I succ{\ldots}
{\ldots}d so big and unwieldy. Now, I think that Jessop gathered som{\ldots}
{\ldots}were both of us young, and, I think, even thus early we attr{\ldots}
{\ldots}at no decent-minded man could think the worse of her; but th{\ldots}
{\ldots}ars, and this little thing, I think, brought back more clear{\ldots}
{\ldots}thing; for I had not dared to think upon that which already {\ldots}
{\ldots}our love one for the other, I think yet, and ponder how that{\ldots}
Wall time: 52 ms

    \end{Verbatim}

    And if we want to get \emph{really} clever, we can have our concordance
function search by stemmed forms. We'll revisit stemming in a bit more
detail shortly; for now, just know that stemming is the process of
determining an uninflected form of words, but it's based purely on
character patterns--so each word is treated completely in isolation,
with no information about parts of speech.

We need to stem the original text, then search for concordances of any
tokens that get stemmed to the same value as our input. Then we run the
previous concordance function on those tokens:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3}]:} \PY{o}{\PYZpc{}\PYZpc{}}\PY{k}{time}
        
        import re
        from gensim.parsing.preprocessing import stem\PYZus{}text
        
        def stem\PYZus{}concordance(text, token, window=50):
            text = re.sub(r\PYZdq{}\PYZbs{}s+\PYZdq{}, \PYZdq{} \PYZdq{}, text)
            \PYZsh{} get a unique list of all word\PYZhy{}like tokens using a basic regex
            word\PYZus{}finder = re.compile(r\PYZdq{}[A\PYZhy{}z0\PYZhy{}9]+\PYZdq{}, re.MULTILINE)
            vocab\PYZus{}to\PYZus{}stem = \PYZob{}
                i.lower():stem\PYZus{}text(i)
                for i in set(word\PYZus{}finder.findall(text))
            \PYZcb{}
            if token.lower().strip() not in vocab\PYZus{}to\PYZus{}stem:
                print(\PYZdq{}Token is not in the vocabulary.  Please try again.\PYZdq{})
                return
            \PYZsh{} now flip the dict to \PYZob{}stemmed\PYZus{}form:\PYZob{}set of unstemmed form\PYZcb{}\PYZcb{}
            stem\PYZus{}to\PYZus{}vocab = \PYZob{}i:set() for i in vocab\PYZus{}to\PYZus{}stem.values()\PYZcb{}
            for i in vocab\PYZus{}to\PYZus{}stem:
                stem\PYZus{}to\PYZus{}vocab[vocab\PYZus{}to\PYZus{}stem[i]].add(i.lower())
            \PYZsh{} look up other tokens that have same stem as input token
            stemmed\PYZus{}token = vocab\PYZus{}to\PYZus{}stem[token]
            possible\PYZus{}forms = stem\PYZus{}to\PYZus{}vocab[stemmed\PYZus{}token]
            
            \PYZsh{} and now run the previous concordance function.
            for i in possible\PYZus{}forms:
                concordance(text, i, window=window)
        
        glen\PYZus{}carrig = open(\PYZdq{}glen carrig.txt\PYZdq{}, \PYZdq{}r\PYZdq{}, encoding=\PYZdq{}utf8\PYZdq{}).read()
        stem\PYZus{}concordance(glen\PYZus{}carrig, \PYZdq{}think\PYZdq{}, window=30)
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
c:\textbackslash{}users\textbackslash{}andersonh\textbackslash{}appdata\textbackslash{}local\textbackslash{}programs\textbackslash{}python\textbackslash{}python36\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}gensim\textbackslash{}utils.py:1167: UserWarning: detected Windows; aliasing chunkize to chunkize\_serial
  warnings.warn("detected Windows; aliasing chunkize to chunkize\_serial")

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\ldots}bo'sun bade him keep silence, thinking it was but a piece of{\ldots}
{\ldots}ithering sounds. And at that, thinking a host of evil things{\ldots}
{\ldots}nation, we turned to descend, thinking that this would be th{\ldots}
{\ldots}h I spent much of my watch in thinking over the details of m{\ldots}
{\ldots} men had caught their fish--, thinking that, if Tompkins had{\ldots}
{\ldots}he proceeded to explain that, thinking they were cut off fro{\ldots}
{\ldots}that land. For, indeed, now I think of it, I can remember th{\ldots}
{\ldots}, indeed, we had ever need to think more of such. And then, {\ldots}
{\ldots}ces on my throat he seemed to think but little, suggesting t{\ldots}
{\ldots}e; so that I knew not what to think, being near to doubting {\ldots}
{\ldots}rawling in the valley. Yet, I think the silences tried us th{\ldots}
{\ldots}her, and further than this, I think with truth I may say, we{\ldots}
{\ldots}fter so long upon the sand. I think, even thus early, I had {\ldots}
{\ldots}me thus armed, they seemed to think that I intended a jest, {\ldots}
{\ldots}ch that it seemed hopeless to think of success; but, for all{\ldots}
{\ldots}red in me; nor, could I, do I think I would; for were I succ{\ldots}
{\ldots}d so big and unwieldy. Now, I think that Jessop gathered som{\ldots}
{\ldots}were both of us young, and, I think, even thus early we attr{\ldots}
{\ldots}at no decent-minded man could think the worse of her; but th{\ldots}
{\ldots}ars, and this little thing, I think, brought back more clear{\ldots}
{\ldots}thing; for I had not dared to think upon that which already {\ldots}
{\ldots}our love one for the other, I think yet, and ponder how that{\ldots}
Wall time: 1.93 s

    \end{Verbatim}

    That added less than a second to the total runtime. Nice.

Now let's do it again, but with spaCy instead of Gensim. spaCy has
built-in tokenization, lemmatization, and more that are all based on
large, pre-trained machine learning models. This will give us much
better accuracy--both with tokenizing and lemmatizing--but at the cost
of higher runtime. spaCy also has multiple models to choose from--for
English, there's small, medium, and large. The bigger the model, the
better its accuracy, but also the slower it runs. But since the
interface is exactly the same, we'll use the small model for speed. (the
small model is pretty darn good, anyways)

We'll also revisit lemmatization in a bit more detail shortly. The short
version: it's like stemming, but it returns a valid, real word
corresponding to the uninflected form of a token. (unlike stemming,
which might map ``today'' to the root form ``todai''--lemmatization
would correctly map this to ``today'').

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}4}]:} \PY{o}{\PYZpc{}\PYZpc{}}\PY{k}{time}
        
        import re
        import spacy
        
        def lemma\PYZus{}concordance(text, token, window=50):
            nlp = spacy.load(\PYZdq{}en\PYZus{}core\PYZus{}web\PYZus{}sm\PYZdq{}, disable=[\PYZdq{}tagger\PYZdq{}, \PYZdq{}parser\PYZdq{}, \PYZdq{}ner\PYZdq{}])
            
            \PYZsh{} directly get the mapping of raw form to lemma from spaCy\PYZsq{}s 
            \PYZsh{} tokenization/stemming
            word\PYZus{}finder = re.compile(r\PYZdq{}[A\PYZhy{}z0\PYZhy{}9]+\PYZdq{}, re.MULTILINE)
            vocab\PYZus{}to\PYZus{}stem = \PYZob{}
                i.lower\PYZus{}:i.lemma\PYZus{}
                for i in nlp(text)
            \PYZcb{}
            if token.lower().strip() not in vocab\PYZus{}to\PYZus{}stem:
                print(\PYZdq{}Token is not in the vocabulary.  Please try again.\PYZdq{})
                return
            \PYZsh{} now flip the dict to \PYZob{}stemmed\PYZus{}form:\PYZob{}set of unstemmed form\PYZcb{}\PYZcb{}
            stem\PYZus{}to\PYZus{}vocab = \PYZob{}i:set() for i in vocab\PYZus{}to\PYZus{}stem.values()\PYZcb{}
            for i in vocab\PYZus{}to\PYZus{}stem:
                stem\PYZus{}to\PYZus{}vocab[vocab\PYZus{}to\PYZus{}stem[i]].add(i.lower())
            
            \PYZsh{} Now get the stemmed form of the input token and look up
            \PYZsh{} the list of possible unstemmed forms\PYZhy{}\PYZhy{}this approximates
            \PYZsh{} finding other inflected forms of the same word.
            stemmed\PYZus{}token = vocab\PYZus{}to\PYZus{}stem[token]
            possible\PYZus{}forms = stem\PYZus{}to\PYZus{}vocab[stemmed\PYZus{}token]
            
            \PYZsh{} and now run the previous concordance function.
            for i in possible\PYZus{}forms:
                concordance(text, i, window=window)
        
        glen\PYZus{}carrig = open(\PYZdq{}glen carrig.txt\PYZdq{}, \PYZdq{}r\PYZdq{}, encoding=\PYZdq{}utf8\PYZdq{}).read()
        lemma\PYZus{}concordance(glen\PYZus{}carrig, \PYZdq{}think\PYZdq{}, window=30)
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\ldots} went hither and thither, the thought that IT--for that is h{\ldots}
{\ldots}ry of the spring, it might be thought that we should set up {\ldots}
{\ldots}upon the trunk. With a sudden thought that it would make me {\ldots}
{\ldots} our leaving, he had given no thought to take them with him;{\ldots}
{\ldots}n here, how that I had little thought all this while for the{\ldots}
{\ldots}aving no tides; but I had not thought to come upon such an o{\ldots}
{\ldots}estion, I grew full of solemn thought; for it seemed to me t{\ldots}
{\ldots}e like for the moment to have thought he had seen a second d{\ldots}
{\ldots}ght by the weed; and then the thought came to me of the end {\ldots}
{\ldots}quito will make; but I had no thought to blame any mosquito{\ldots}
{\ldots}rooted them. At least, so the thought came to me. And so we {\ldots}
{\ldots}er while than hitherto he had thought needful. Having conclu{\ldots}
{\ldots}s further ahead: but I had no thought for these when I perce{\ldots}
{\ldots}de. Then, having by this time thought a little upon the matt{\ldots}
{\ldots}is liking. But it must not be thought that he did naught but{\ldots}
{\ldots}y an enormous crab. Now I had thought the crab we had tried {\ldots}
{\ldots}ll breakers--which we had not thought needful to carry to th{\ldots}
{\ldots}zon, and there would come the thought to me of the terror of{\ldots}
{\ldots}hen, even as I fell upon this thought, the bo'sun clapped me{\ldots}
{\ldots}n in cooler minds, we had not thought strange, seeing that s{\ldots}
{\ldots}ed too sincerely to have much thought to watch the hulk, whi{\ldots}
{\ldots}were become so excited at the thought of fellow creatures al{\ldots}
{\ldots}he rope to us, and at this we thought more upon his saying; {\ldots}
{\ldots} happy to tell that we had no thought at this juncture but f{\ldots}
{\ldots}to a method of rescue. Then a thought came to me (waked perc{\ldots}
{\ldots} me a short spell of disquiet thought. It was in this wise:-{\ldots}
{\ldots}n such a sight, and indeed, I thought nothing more of it tha{\ldots}
{\ldots}ls, and further, I could have thought I perceived a flicker {\ldots}
{\ldots}ed it at all such parts as he thought in any way doubtful, a{\ldots}
{\ldots}tion, and then, suddenly, the thought came to me that the sc{\ldots}
{\ldots}w I was made to understand my thought of the previous night,{\ldots}
{\ldots}the first, and then, a sudden thought coming to me, I thrust{\ldots}
{\ldots}t remained hid. Then a sudden thought came into my brain, an{\ldots}
{\ldots}y, as a result of some little thought, he brought out from t{\ldots}
{\ldots}fore the evening. And, at the thought of this, we experience{\ldots}
{\ldots} the lesser rope; for that he thought they in the ship were {\ldots}
{\ldots}t break it, and then a second thought that something might b{\ldots}
{\ldots}ted within myself at this new thought, as, indeed, was the b{\ldots}
{\ldots}ly officer remaining to them, thought there might be good ch{\ldots}
{\ldots}ened the rope so much as they thought proper, they left it t{\ldots}
{\ldots}rface, and, at that, a sudden thought came to me which sent {\ldots}
{\ldots}her; but that I, for my part, thought rather the better, see{\ldots}
{\ldots}d which had beset them at the thought that they should all o{\ldots}
{\ldots}such interest since, that the thought of food had escaped me{\ldots}
{\ldots}r to have stayed, the which I thought, for a moment, had not{\ldots}
{\ldots}as very terrible, this sudden thought of failure (though it {\ldots}
{\ldots}ing gear about it, so that he thought it would be so safe as{\ldots}
{\ldots}of us in the ship that he had thought to go at that moment; {\ldots}
{\ldots}bo'sun bade him keep silence, thinking it was but a piece of{\ldots}
{\ldots}ithering sounds. And at that, thinking a host of evil things{\ldots}
{\ldots}nation, we turned to descend, thinking that this would be th{\ldots}
{\ldots}h I spent much of my watch in thinking over the details of m{\ldots}
{\ldots} men had caught their fish--, thinking that, if Tompkins had{\ldots}
{\ldots}he proceeded to explain that, thinking they were cut off fro{\ldots}
{\ldots}that land. For, indeed, now I think of it, I can remember th{\ldots}
{\ldots}, indeed, we had ever need to think more of such. And then, {\ldots}
{\ldots}ces on my throat he seemed to think but little, suggesting t{\ldots}
{\ldots}e; so that I knew not what to think, being near to doubting {\ldots}
{\ldots}rawling in the valley. Yet, I think the silences tried us th{\ldots}
{\ldots}her, and further than this, I think with truth I may say, we{\ldots}
{\ldots}fter so long upon the sand. I think, even thus early, I had {\ldots}
{\ldots}me thus armed, they seemed to think that I intended a jest, {\ldots}
{\ldots}ch that it seemed hopeless to think of success; but, for all{\ldots}
{\ldots}red in me; nor, could I, do I think I would; for were I succ{\ldots}
{\ldots}d so big and unwieldy. Now, I think that Jessop gathered som{\ldots}
{\ldots}were both of us young, and, I think, even thus early we attr{\ldots}
{\ldots}at no decent-minded man could think the worse of her; but th{\ldots}
{\ldots}ars, and this little thing, I think, brought back more clear{\ldots}
{\ldots}thing; for I had not dared to think upon that which already {\ldots}
{\ldots}our love one for the other, I think yet, and ponder how that{\ldots}
Wall time: 3.76 s

    \end{Verbatim}

    \hypertarget{n-gram-frequencies}{%
\subsection{N-gram frequencies}\label{n-gram-frequencies}}

Python has a number of ways we could find n-grams. The first is using a
pre-built tool, like NLTK's ngrams() function, or a Phrases()/Phraser()
combination from Gensim, which are actually used to find multi-word
phrases. Or, we could hack it together ourselves with a few lines of
code.

First, let's hack it together ourselves. Then we'll print out the top
most common N-grams and plot the frequencies by rank (on a logarithmic
scale, naturally. We're not monsters, after all). We'll use spaCy's
tokenization for maximum accuracy.

First, we'll tokenize our document and convert everything to lowercase
using spaCy. We'll also remove punctuation and stopwords while we're at
it.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}5}]:} \PY{o}{\PYZpc{}\PYZpc{}}\PY{k}{time}
        \PYZpc{}matplotlib inline
        
        from collections import Counter
        from pprint import pprint
        import re
        
        import matplotlib as mpl
        import matplotlib.pyplot as plt
        import spacy
        
        \PYZsh{} Change this number manually to change the N in the ngrams
        NGRAM\PYZus{}N = 2
        
        \PYZsh{} Preprocess the document
        nlp = spacy.load(\PYZdq{}en\PYZus{}core\PYZus{}web\PYZus{}sm\PYZdq{}, disable=[\PYZdq{}tagger\PYZdq{}, \PYZdq{}parser\PYZdq{}, \PYZdq{}ner\PYZdq{}])
        doc = open(\PYZdq{}glen carrig.txt\PYZdq{}, \PYZdq{}r\PYZdq{}, encoding=\PYZdq{}utf8\PYZdq{}).read()
        doc = re.sub(r\PYZdq{}\PYZbs{}s+\PYZdq{}, \PYZdq{} \PYZdq{}, doc)
        doc = [
            i.lower\PYZus{} 
            for i in nlp(doc)
            if not i.is\PYZus{}punct
            and not i.is\PYZus{}stop
        ]
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Wall time: 3.15 s

    \end{Verbatim}

    Now, we'll find n-grams by explicitly coding an n-gram finding bit of
Python.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}6}]:} \PY{o}{\PYZpc{}\PYZpc{}}\PY{k}{time}
        \PYZpc{}matplotlib inline
        
        \PYZsh{} change the default matplotlib figure size for Jupyter\PYZsq{}s sake
        mpl.rcParams[\PYZdq{}figure.figsize\PYZdq{}] = (10,10)
        
        \PYZsh{} Find the n\PYZhy{}grams with some manual code
        n\PYZus{}grams = [
            \PYZdq{}\PYZus{}\PYZdq{}.join(doc[i:i+NGRAM\PYZus{}N]) 
            for i in range(0, len(doc) \PYZhy{} NGRAM\PYZus{}N)
        ]
        n\PYZus{}grams = Counter(n\PYZus{}grams)
        
        \PYZsh{} do some prettier formatting than the default printing
        print(f\PYZdq{}\PYZob{}\PYZsq{}NGRAM\PYZsq{}:\PYZlt{}30s\PYZcb{}\PYZbs{}tCOUNT\PYZdq{})
        for i in n\PYZus{}grams.most\PYZus{}common(20):
            print(f\PYZdq{}\PYZob{}i[0]:\PYZlt{}30s\PYZcb{}\PYZbs{}t\PYZob{}i[1]\PYZcb{}\PYZdq{})
            
        \PYZsh{} Now, let\PYZsq{}s just plot the counts by rank.
        counts = sorted(n\PYZus{}grams.values(), reverse=True)
        plt.plot(counts)
        plt.yscale(\PYZdq{}log\PYZdq{})
        plt.xscale(\PYZdq{}log\PYZdq{})
        plt.title(\PYZdq{}Look at this beautiful Zipf distribution!\PYZdq{})
        plt.xlabel(\PYZdq{}Rank (log)\PYZdq{})
        plt.ylabel(\PYZdq{}Count (log)\PYZdq{})
        plt.show()
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
NGRAM                         	COUNT
i\_saw                         	51
and\_i                         	33
i\_discovered                  	28
i\_perceived                   	27
weed\_continent                	27
now\_i                         	26
yet\_i                         	26
mistress\_madison              	26
bo'sun\_bade                   	25
then\_i                        	25
at\_i                          	23
i\_found                       	22
i\_knew                        	19
second\_mate                   	19
bo'sun\_'s                     	18
i\_heard                       	18
i\_went                        	17
and\_presently                 	17
i\_come                        	16
captain\_'s                    	16

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Python for Linguists_files/Python for Linguists_23_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
Wall time: 729 ms

    \end{Verbatim}

    NLTK's ngrams() function will find ngrams for us, like we just did by
hand. It'll be more readable, but function exactly the same, and run
almost exactly as far--so in general, this method might be preferable
for most people.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}7}]:} \PY{o}{\PYZpc{}\PYZpc{}}\PY{k}{time}
        \PYZpc{}matplotlib inline
        
        from nltk import ngrams
        
        \PYZsh{} change the default matplotlib figure size for Jupyter\PYZsq{}s sake
        mpl.rcParams[\PYZdq{}figure.figsize\PYZdq{}] = (10,10)
        
        \PYZsh{} Find n\PYZhy{}grams
        n\PYZus{}grams = ngrams(doc, NGRAM\PYZus{}N)
        n\PYZus{}grams = Counter(n\PYZus{}grams)
        
        \PYZsh{} do some prettier formatting than the default printing
        print(f\PYZdq{}\PYZob{}\PYZsq{}NGRAM\PYZsq{}:\PYZlt{}30s\PYZcb{}\PYZbs{}tCOUNT\PYZdq{})
        for i in n\PYZus{}grams.most\PYZus{}common(20):
            \PYZsh{} okay, so the format string changes a little too
            print(f\PYZdq{}\PYZob{}\PYZsq{}\PYZus{}\PYZsq{}.join(i[0]):\PYZlt{}30s\PYZcb{}\PYZbs{}t\PYZob{}i[1]\PYZcb{}\PYZdq{})
            
        \PYZsh{} Now, let\PYZsq{}s just plot the counts by rank.
        counts = sorted(n\PYZus{}grams.values(), reverse=True)
        plt.plot(counts)
        plt.yscale(\PYZdq{}log\PYZdq{})
        plt.xscale(\PYZdq{}log\PYZdq{})
        plt.title(\PYZdq{}Look at this beautiful Zipf distribution!\PYZdq{})
        plt.xlabel(\PYZdq{}Rank (log)\PYZdq{})
        plt.ylabel(\PYZdq{}Count (log)\PYZdq{})
        plt.show()
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
NGRAM                         	COUNT
i\_saw                         	51
and\_i                         	33
i\_discovered                  	28
i\_perceived                   	27
weed\_continent                	27
now\_i                         	26
yet\_i                         	26
mistress\_madison              	26
bo'sun\_bade                   	25
then\_i                        	25
at\_i                          	23
i\_found                       	22
i\_knew                        	19
second\_mate                   	19
bo'sun\_'s                     	18
i\_heard                       	18
i\_went                        	17
and\_presently                 	17
i\_come                        	16
captain\_'s                    	16

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Python for Linguists_files/Python for Linguists_25_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
Wall time: 1.98 s

    \end{Verbatim}

    \hypertarget{phrase-findingcollocation-analysis}{%
\subsection{Phrase-finding/collocation
analysis}\label{phrase-findingcollocation-analysis}}

Ngrams are fun and all, but what if you want to find multi-word phrases
that appear more often than they should by random chance alone (i.e.,
collocates)? Well, as before, we could hack a bit of code together, or
we could use a pre-built tool from the amazing Gensin library: the
\href{https://radimrehurek.com/gensim/models/phrases.html}{Phrasing
tools!} These tools let you scan a(n already processed) corpus of texts,
and finds bigrams that are collocated more than the raw prior
distributions would indicate. Then, these tools let you transform your
original corpus, replacing these bigrams with a single token. You can
repeat this process all you want to find arbitrarily long phrases.

Before doing this, we should run our text through a basic preprocessing
pipeline in Gensim. We'll revisit this in a bit more detail later to
talk about what it does; for now, just know that it automates a lot of
the basic preprocessing steps for us, like lowercasing, removing
stopwords, stemming, etc.

We'll use all the default values for our phrasing models except for the
threshold (to guarnatee we find at least \emph{some} phrases for this
demo), but they'll be provided explicitly to show how much customization
there is. Note that the phraser expectes a \emph{list of sentences},
i.e.~a \emph{list of lists of words.} We don't strictly need to make
them the actual sentences; the only reason Gensim says to use sentences
is to avoid collocation across sentence boundaries.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}8}]:} \PY{k+kn}{import} \PY{n+nn}{re}
        
        \PY{k+kn}{from} \PY{n+nn}{gensim}\PY{n+nn}{.}\PY{n+nn}{models}\PY{n+nn}{.}\PY{n+nn}{phrases} \PY{k}{import} \PY{n}{Phrases}
        \PY{k+kn}{from} \PY{n+nn}{gensim}\PY{n+nn}{.}\PY{n+nn}{parsing}\PY{n+nn}{.}\PY{n+nn}{preprocessing} \PY{k}{import} \PY{n}{preprocess\PYZus{}string}
        
        \PY{n}{doc} \PY{o}{=} \PY{n+nb}{open}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{glen carrig.txt}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{r}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{encoding}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{utf8}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{o}{.}\PY{n}{read}\PY{p}{(}\PY{p}{)}
        \PY{c+c1}{\PYZsh{} clean up line breaks and other whitespace issues}
        \PY{n}{doc} \PY{o}{=} \PY{n}{re}\PY{o}{.}\PY{n}{sub}\PY{p}{(}\PY{l+s+sa}{r}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZbs{}}\PY{l+s+s2}{s+}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{doc}\PY{p}{)}
        \PY{n}{doc} \PY{o}{=} \PY{n}{preprocess\PYZus{}string}\PY{p}{(}\PY{n}{doc}\PY{p}{)}
        \PY{n}{phrasing} \PY{o}{=} \PY{n}{Phrases}\PY{p}{(}
            \PY{p}{[}\PY{n}{doc}\PY{p}{]}\PY{p}{,} \PY{c+c1}{\PYZsh{} phrases() expects a list of tokenized sentences/documents}
            \PY{n}{min\PYZus{}count}\PY{o}{=}\PY{l+m+mi}{5}\PY{p}{,}
            \PY{n}{threshold}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{,}
            \PY{n}{max\PYZus{}vocab\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{40000000}\PY{p}{,}
            \PY{n}{delimiter}\PY{o}{=}\PY{l+s+sa}{b}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZus{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{c+c1}{\PYZsh{} this has to be a byte string\PYZhy{}\PYZhy{}just a quirk of this model}
            \PY{n}{progress\PYZus{}per}\PY{o}{=}\PY{l+m+mi}{10000}\PY{p}{,}
            \PY{n}{scoring}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{default}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}
        \PY{p}{)}
\end{Verbatim}


    Now, we can look at some of the phrases that our phrase model
discovered. This will print out the phrases in the order they're found
in the text, so we might see duplicates.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}9}]:} \PY{k+kn}{from} \PY{n+nn}{pprint} \PY{k}{import} \PY{n}{pprint}
        
        \PY{n}{found\PYZus{}phrases} \PY{o}{=} \PY{n+nb}{list}\PY{p}{(}\PY{n}{phrasing}\PY{o}{.}\PY{n}{export\PYZus{}phrases}\PY{p}{(}\PY{p}{[}\PY{n}{doc}\PY{p}{]}\PY{p}{)}\PY{p}{)}
        \PY{n}{pprint}\PY{p}{(}\PY{n}{found\PYZus{}phrases}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{15}\PY{p}{]}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
[(b'shook head', 51.86585365853659),
 (b'shook head', 51.86585365853659),
 (b'sun bade', 21.133969389783342),
 (b'sun bade', 21.133969389783342),
 (b'main cabin', 139.67159277504103),
 (b'main cabin', 139.67159277504103),
 (b'captain cabin', 77.1869328493648),
 (b'big cabin', 33.3307210031348),
 (b'aboard hulk', 36.35042735042735),
 (b'main cabin', 139.67159277504103),
 (b'main cabin', 139.67159277504103),
 (b'tot rum', 253.1547619047619),
 (b'captain cabin', 77.1869328493648),
 (b'captain cabin', 77.1869328493648),
 (b'captain cabin', 77.1869328493648)]

    \end{Verbatim}

    And, we can transform our original document(s), replacing all of the
discovered bigrams with a single token (e.g.,
\texttt{{[}"the",\ boat"{]}} --\textgreater{} \texttt{"the\_boat"}).
Gensim likes to use the indexing syntax to do transformations--it's a
bit weird but you get used to it.

Note that we'll get a warning from Gensim (warnings are not
errors--they're more of a ``heads up, something looks weird here'' sort
of notice). Gensim also has Phraser() objects, which are initialized
from a Phrases() object, and are much faster at transforming a corpus.
This only really matters when you're dealing with \emph{massive} corpora
and datasets; for our single book, we don't really need to bother, but
I'll show how it would be done anyways.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}10}]:} \PY{k+kn}{from} \PY{n+nn}{gensim}\PY{n+nn}{.}\PY{n+nn}{models}\PY{n+nn}{.}\PY{n+nn}{phrases} \PY{k}{import} \PY{n}{Phraser}
         
         \PY{c+c1}{\PYZsh{} transform the text with the original phrases object...}
         \PY{n}{phrased\PYZus{}doc} \PY{o}{=} \PY{n}{phrasing}\PY{p}{[}\PY{n}{doc}\PY{p}{]}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{phrased\PYZus{}doc}\PY{p}{[}\PY{l+m+mi}{500}\PY{p}{:}\PY{l+m+mi}{550}\PY{p}{]}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} ...or by creating a new Phraser() from it first.}
         \PY{n}{phraser} \PY{o}{=} \PY{n}{Phraser}\PY{p}{(}\PY{n}{phrasing}\PY{p}{)}
         \PY{n}{phrased\PYZus{}doc} \PY{o}{=} \PY{n}{phraser}\PY{p}{[}\PY{n}{doc}\PY{p}{]}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{phrased\PYZus{}doc}\PY{p}{[}\PY{l+m+mi}{500}\PY{p}{:}\PY{l+m+mi}{550}\PY{p}{]}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
c:\textbackslash{}users\textbackslash{}andersonh\textbackslash{}appdata\textbackslash{}local\textbackslash{}programs\textbackslash{}python\textbackslash{}python36\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}gensim\textbackslash{}models\textbackslash{}phrases.py:486: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class
  warnings.warn("For a faster implementation, use the gensim.models.phrases.Phraser class")

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
['befal', 'georg', 'youngest', 'prentic', 'boi', 'seat', 'pluck', 'sleev', 'inquir', 'troubl', 'voic', 'knowledg', 'cry', 'portend', 'shook\_head', 'tell', 'know', 'comfort', 'said', 'wind', 'shook\_head', 'plain', 'agenc', 'stark', 'calm', 'scarc', 'end', 'remark', 'sad', 'cry', 'appear', 'come', 'far', 'creek', 'far', 'creek', 'inland', 'land', 'sea', 'fill', 'even', 'air', 'dole', 'wail', 'remark', 'curiou', 'sob', 'human', 'despair', 'cry']


 ['befal', 'georg', 'youngest', 'prentic', 'boi', 'seat', 'pluck', 'sleev', 'inquir', 'troubl', 'voic', 'knowledg', 'cry', 'portend', 'shook\_head', 'tell', 'know', 'comfort', 'said', 'wind', 'shook\_head', 'plain', 'agenc', 'stark', 'calm', 'scarc', 'end', 'remark', 'sad', 'cry', 'appear', 'come', 'far', 'creek', 'far', 'creek', 'inland', 'land', 'sea', 'fill', 'even', 'air', 'dole', 'wail', 'remark', 'curiou', 'sob', 'human', 'despair', 'cry']

    \end{Verbatim}

    (as you can see, the results of Phrases() and Phraser() are the
same--Phraser() will just be \emph{much} faster, and use much less
memory, for very large phrasing passes).

    \hypertarget{some-more-fun-demos-natural-language-processing-101}{%
\section{Some more fun demos: Natural Language Processing
101}\label{some-more-fun-demos-natural-language-processing-101}}

Now, let's do some more interesting demos. These will focus on more
sophisticated (but still ``entry-level'') NLP techniques and tools.

We will: * Go from raw data to cleaned, workable text. * Look at some of
spaCy and Gensim's text (pre)processing tools * Do some basic topic
modeling with Latent Dirichlet Allocation * Do some basic classification
and regression tasks with some text.

    \hypertarget{getting-our-data}{%
\subsection{Getting our data}\label{getting-our-data}}

    The previous demos were very simple (even simplistic) and don't really
leverage all the cool stuff Python--or programming in general--can do
for you. Let's work with a non-trivial dataset now and do some more
NLP-like work. We'll use the
\href{http://u.cs.biu.ac.il/~koppel/BlogCorpus.htm}{Blog Authorship
Corpus}. Download it to the same folder as this notebook, then unzip it
into a folder names ``blogs'' (case-sensitive!).

Each author's blog posts are stored as a .xml file, named in the
following format:

\texttt{{[}ID\ number{]}.{[}gender{]}.{[}age{]}.{[}industry\ of\ employment{]}.{[}astrological\ sign{]}.xml}

E.g., \texttt{11253.male.26.Technology.Aquarius.xml}

And they contain blog data that looks like this:

    ``` 20,July,2004

\begin{verbatim}
  About to go t bed late (again) got sucked into (another) late night film. Tonight was  urlLink Maybe Baby . It was really good made me think, but not about babies. The guy screws up his marriage and it made me think about making sure, everyday, that mine is tip top. If I'm honest there are areas that we are just getting by in - so I need to resolve to sort them out now before they are a problem. In the film they both keep diaries so I thought I should blog tonight.&nbsp;   Weekend was hectic but great fun. Not that long ago k and I had to work on spending time with other people as a couple. This weekend we never ate alone, except breakfast.&nbsp; K, P and I went for a very breif trip on the river saturday durring a gap in the weather. K stripped off and went for a swim, I love her so much.&nbsp;   Tonight we went out for dinner. It was a lovely evening, the first in weeks, so we ate at the Bridge and sat outside, next to the  urlLink river .     
 
\end{verbatim}

 ```

    First thing's first: we need to deal with the XML formatting.
Fortunately, Python has some excellent tools for that, e.g.
\texttt{xml.etree.ElementTree}. We'll also want to use a better data
structure to represent this text. There's an absolutely indispensible
library called Pandas, which gives you R-style dataframes to work with
(and Pandas is probably the single biggest reason that Python has taken
over the data science world, demoting R to second-place).

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}11}]:} \PY{k+kn}{import} \PY{n+nn}{os}
         
         \PY{n}{files} \PY{o}{=} \PY{p}{[}\PY{n}{i}\PY{o}{.}\PY{n}{path} \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n}{os}\PY{o}{.}\PY{n}{scandir}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{blogs}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{p}{]}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{files}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{5}\PY{p}{]}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
['blogs\textbackslash{}\textbackslash{}1000331.female.37.indUnk.Leo.xml', 'blogs\textbackslash{}\textbackslash{}1000866.female.17.Student.Libra.xml', 'blogs\textbackslash{}\textbackslash{}1004904.male.23.Arts.Capricorn.xml', 'blogs\textbackslash{}\textbackslash{}1005076.female.25.Arts.Cancer.xml', 'blogs\textbackslash{}\textbackslash{}1005545.male.25.Engineering.Sagittarius.xml']

    \end{Verbatim}

    Let's do a first pass where we just try to parse each file, to make sure
there are no problems. (Data validations is a step you \emph{absolutely
do not skip} if you're doing any real data work, after all!)

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}12}]:} \PY{k+kn}{from} \PY{n+nn}{xml}\PY{n+nn}{.}\PY{n+nn}{etree} \PY{k}{import} \PY{n}{ElementTree}
         
         \PY{c+c1}{\PYZsh{} sorted() just makes sure the files are always in the same order}
         \PY{c+c1}{\PYZsh{} for the demos}
         \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{sorted}\PY{p}{(}\PY{n}{files}\PY{p}{)}\PY{p}{:}
             \PY{k}{try}\PY{p}{:}
                 \PY{n}{ElementTree}\PY{o}{.}\PY{n}{parse}\PY{p}{(}\PY{n}{i}\PY{p}{)}
             \PY{k}{except} \PY{n+ne}{Exception} \PY{k}{as} \PY{n}{e}\PY{p}{:}
                 \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{EXCEPTION ON FILE:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{i}\PY{p}{)}
                 \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{EXCEPTION:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{e}\PY{p}{)}
                 \PY{k}{break}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]

EXCEPTION ON FILE: blogs\textbackslash{}1000866.female.17.Student.Libra.xml
EXCEPTION: not well-formed (invalid token): line 103, column 225

    \end{Verbatim}

    Uh-oh. Let's take a look at the file that broke and go see where the
problem is. Fortunately, the error above gave us a line \emph{and} a
column number, so we can go to the exact location where an issue was
encountered.

\begin{verbatim}
<Blog>
[...]

<date>14,January,2003</date>
<post>


      Hehe, just finished dinner! Yum! I'm so happy right now. I don't even know why, I just...am! I'm talking to a bunch of my friends while writing this, which is always fun. Plus I'm doing homework, PLUS I'm watching Law & Order...how massively talented am I? Well, my day was pretty kick butt. Umm, no band OR music theory...very cool, so Kristen and I sat together and did homework and discussed Winter. Next period I hung out with Chris and Kelly for a bit (Alex too for a bit, since he was in Gym) and I left eventually. They started working on music, and...I just always end up feeling left out when they do, so, I try to stay away from those two together in general.Oh well, I went and sat alone in a practice room. Darn...no good stories for the newspaper to write about me. That was a great story, no matter how angry people are about the band comment. I wish people would have read the story actually, instead of reaching paragraph two and deciding it was terrible. Well, anyway, umm...EPVM was interesting. I'm getting nervous about my final. My brother said it was difficult...my brother with the perfect ACT AND SAT scores...arg. I just...wow, I'm so afraid of that test. I'm completely going to fail. Gym...oh jeez...two words  Commando Crawl  OWWWWWWWWWWWWWW  Good lord...that was one of the most painful...oh jeez...just thinking about it. Seriously, if you ever by some chance do that for high ropes...WEAR PANTS! Well, yes...wear pants normally, but don't wear shorts, make sure they are pants, because...it's quite painful if you don't. I made it through though, so it's all good! It just hurt...a lot.   Math, boring, shock shock.  Intervening thought: Why do I always write these while talking to Alex??  Lunch...was interesting. Just hung out with Kelly and Emily some, then Chris and Alex. It was cool...not much to it. Comparative Religion was boring...just, meh, presenting projects...and finally chem...boring, shock shock, and then, I came home, and did stuff, and that is the end of my day...therefore, I shall leave, and go kill alex...I mean...wait...you know NOTHING. You have no evidence...:)

</post>
\end{verbatim}

    Column numbers aren't displayed in this notebook, but the error is at
the ampersand in \texttt{Law\ \&\ Order}. As it turns out, ampersands
are special characters in XML code, and need to be escaped specially (in
this case, as \texttt{\&amp;}). We could do some manual replacement of
characters with the appropriate XML escapes, but that sounds like a lot
of work and a lot of room for error.

Fortunately, our document structure is so simple that we can just hack
this together with regular expressions. This will be fast, but
\textbf{is not} how we should in general deal with problematic XML or
other markup files--we'd want to use some of Python's more rudimentary
tools, like the base HTML or XML parsers, and overwrite their
functionality (e.g.~by subclassing).

We'll create a list of dictionaries (think JSONs), which we can easily
pass into Pandas to make a big, beautiful, glorious Dataframe object
(which we'll save to a .csv so we can re-open it directly later). We'll
work with this Dataframe for most the rest of this demo.

We'll also create a second dataframe, where we only have one entry per
author, and concatenate all of their posts together. We'll use this at
the end of the talk for some regression and classification tasks where
we're predicting author-level variables, rather than working at the
document level.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}13}]:} \PY{o}{\PYZpc{}\PYZpc{}}\PY{k}{time}
         
         import os
         import re
         
         from tqdm import tqdm\PYZus{}notebook as tqdm
         import pandas as pd
         
         \PYZsh{} pre\PYZhy{}compile patterns we\PYZsq{}ll use a lot\PYZhy{}\PYZhy{}for speed
         date\PYZus{}finder = re.compile(r\PYZdq{}\PYZca{}\PYZlt{}date\PYZgt{}(.*?)\PYZlt{}/date\PYZgt{}\PYZdl{}\PYZdq{}, re.MULTILINE)
         post\PYZus{}finder = re.compile(r\PYZdq{}\PYZca{}\PYZlt{}post\PYZgt{}\PYZdl{}(.*?)\PYZca{}\PYZlt{}/post\PYZgt{}\PYZdl{}\PYZdq{}, re.MULTILINE|re.DOTALL)
         whitespace\PYZus{}cleaner = re.compile(r\PYZdq{}\PYZbs{}s+\PYZdq{}, re.MULTILINE)
         
         def process\PYZus{}file(infile, by\PYZus{}author=False):
             \PYZsh{} get the user metadata from the filename
             metadata = os.path.split(infile)[\PYZhy{}1].split(\PYZdq{}.\PYZdq{})
             text = open(infile, \PYZdq{}r\PYZdq{}, encoding=\PYZdq{}ISO\PYZhy{}8859\PYZhy{}1\PYZdq{}).read()
             
             \PYZsh{} Extract date and post content from the file
             dates = date\PYZus{}finder.findall(text)
             posts = post\PYZus{}finder.findall(text)
             
             \PYZsh{} assert check will crash our program if it fails\PYZhy{}\PYZhy{}
             \PYZsh{} this make sure our approach works.
             assert len(dates) == len(posts)
             if by\PYZus{}author == False:
                 blog\PYZus{}data = [
                     \PYZob{}
                          \PYZdq{}Author ID\PYZdq{} : metadata[0],
                          \PYZdq{}Gender\PYZdq{}    : metadata[1],
                          \PYZdq{}Age\PYZdq{}       : int(metadata[2]),
                          \PYZdq{}Industry\PYZdq{}  : metadata[3],
                          \PYZdq{}Sign\PYZdq{}      : metadata[4],
                          \PYZdq{}Date\PYZdq{}      : dates[i],
                          \PYZdq{}Post\PYZdq{}      : whitespace\PYZus{}cleaner.sub(\PYZdq{} \PYZdq{}, posts[i])
                     \PYZcb{}
                     for i in range(len(dates))
                 ]
             elif by\PYZus{}author == True:
                 posts = \PYZdq{} \PYZdq{}.join(
                     whitespace\PYZus{}cleaner.sub(\PYZdq{} \PYZdq{}, i)
                     for i in posts
                 )
                 blog\PYZus{}data = \PYZob{}
                      \PYZdq{}Author ID\PYZdq{} : metadata[0],
                      \PYZdq{}Gender\PYZdq{}    : metadata[1],
                      \PYZdq{}Age\PYZdq{}       : int(metadata[2]),
                      \PYZdq{}Industry\PYZdq{}  : metadata[3],
                      \PYZdq{}Sign\PYZdq{}      : metadata[4],
                      \PYZdq{}Posts\PYZdq{}     : posts
                 \PYZcb{}
             
             return blog\PYZus{}data
         
         blog\PYZus{}dataframe = pd.concat(
             pd.DataFrame(process\PYZus{}file(i, by\PYZus{}author=False))
             for i in tqdm(files, desc=\PYZdq{}Generating Dataframe\PYZdq{})
         )
         print(\PYZdq{}Casting Date column to datetime format.\PYZdq{})
         print(\PYZdq{}Saving blog dataframe to Blog Data.csv.\PYZdq{})
         blog\PYZus{}dataframe.to\PYZus{}csv(\PYZdq{}corpus data files/Blog Data.csv\PYZdq{}, index=False)
         print(blog\PYZus{}dataframe)
\end{Verbatim}


    
    \begin{verbatim}
HBox(children=(IntProgress(value=0, description='Generating Dataframe', max=19320), HTML(value='')))
    \end{verbatim}

    
    \begin{Verbatim}[commandchars=\\\{\}]

Casting Date column to datetime format.
Saving blog dataframe to Blog Data.csv.
    Age Author ID              Date  Gender  Industry  \textbackslash{}
0    37   1000331       31,May,2004  female    indUnk   
1    37   1000331       29,May,2004  female    indUnk   
2    37   1000331       28,May,2004  female    indUnk   
3    37   1000331       28,May,2004  female    indUnk   
4    37   1000331       28,May,2004  female    indUnk   
5    37   1000331       28,May,2004  female    indUnk   
6    37   1000331      21,June,2004  female    indUnk   
7    37   1000331      18,June,2004  female    indUnk   
8    37   1000331      15,June,2004  female    indUnk   
9    37   1000331      08,June,2004  female    indUnk   
10   37   1000331      03,June,2004  female    indUnk   
11   37   1000331      02,June,2004  female    indUnk   
12   37   1000331      03,July,2004  female    indUnk   
0    17   1000866  23,November,2002  female   Student   
1    17   1000866  20,November,2002  female   Student   
2    17   1000866  19,November,2002  female   Student   
3    17   1000866  18,November,2002  female   Student   
4    17   1000866  18,November,2002  female   Student   
5    17   1000866  18,November,2002  female   Student   
6    17   1000866  18,November,2002  female   Student   
7    17   1000866  20,December,2002  female   Student   
8    17   1000866  18,December,2002  female   Student   
9    17   1000866  18,December,2002  female   Student   
10   17   1000866  15,December,2002  female   Student   
11   17   1000866  15,December,2002  female   Student   
12   17   1000866   14,January,2003  female   Student   
13   17   1000866   13,January,2003  female   Student   
14   17   1000866   13,January,2003  female   Student   
15   17   1000866   12,January,2003  female   Student   
16   17   1000866   12,January,2003  female   Student   
..  {\ldots}       {\ldots}               {\ldots}     {\ldots}       {\ldots}   
28   27    998966      11,July,2004    male    indUnk   
0    25    999503      30,June,2004    male  Internet   
1    25    999503      28,June,2004    male  Internet   
2    25    999503      28,June,2004    male  Internet   
3    25    999503      27,June,2004    male  Internet   
4    25    999503      21,June,2004    male  Internet   
5    25    999503      19,June,2004    male  Internet   
6    25    999503      19,June,2004    male  Internet   
7    25    999503      15,June,2004    male  Internet   
8    25    999503      15,June,2004    male  Internet   
9    25    999503      14,June,2004    male  Internet   
10   25    999503      10,June,2004    male  Internet   
11   25    999503      09,June,2004    male  Internet   
12   25    999503      08,June,2004    male  Internet   
13   25    999503      08,June,2004    male  Internet   
14   25    999503      07,June,2004    male  Internet   
15   25    999503      06,June,2004    male  Internet   
16   25    999503      06,June,2004    male  Internet   
17   25    999503      06,June,2004    male  Internet   
18   25    999503      17,July,2004    male  Internet   
19   25    999503      15,July,2004    male  Internet   
20   25    999503      14,July,2004    male  Internet   
21   25    999503      11,July,2004    male  Internet   
22   25    999503      06,July,2004    male  Internet   
23   25    999503      05,July,2004    male  Internet   
24   25    999503      04,July,2004    male  Internet   
25   25    999503      03,July,2004    male  Internet   
26   25    999503      02,July,2004    male  Internet   
27   25    999503      01,July,2004    male  Internet   
28   25    999503      01,July,2004    male  Internet   

                                                 Post    Sign  
0    Well, everyone got up and going this morning{\ldots}     Leo  
1    My four-year old never stops talking. She'll {\ldots}     Leo  
2    Actually it's not raining yet, but I bought 1{\ldots}     Leo  
3    Ha! Just set up my RSS feed - that is so easy{\ldots}     Leo  
4    Oh, which just reminded me, we were talking a{\ldots}     Leo  
5    I've tried starting blog after blog and it ju{\ldots}     Leo  
6    My 20th high school urlLink reunion is this w{\ldots}     Leo  
7    We always have pizza on Friday nights. It tak{\ldots}     Leo  
8    Okay, I saw it this past weekend. Not as good{\ldots}     Leo  
9    I've been cataloguing film scripts at work. W{\ldots}     Leo  
10   Paul Martin promised today that if he is elec{\ldots}     Leo  
11   No, I still haven't seen it, but apparently i{\ldots}     Leo  
12   Well, it's over! It was good to see so many p{\ldots}     Leo  
0    Yeah, sorry for not writing for a whole there{\ldots}   Libra  
1    Yeah, so today was ok, late arrival. I'm not {\ldots}   Libra  
2    Yay, Tuesday{\ldots}no longer Monday! Whoopie! Plu{\ldots}   Libra  
3                                               RAR!    Libra  
4    Thought- OK{\ldots}so, I'm all for midgets and wha{\ldots}   Libra  
5    Yeah, so it's later. My parents found somethi{\ldots}   Libra  
6    Yeah, so this is just a place for me to vent {\ldots}   Libra  
7    Eventful day{\ldots}well, sort of. Eventful to me {\ldots}   Libra  
8    Eventful day{\ldots}well, sort of. Eventful to me {\ldots}   Libra  
9    :o) Hey{\ldots}it looks like Alex. You know, the n{\ldots}   Libra  
10   I'm baaack, well, sort of. I have a lesson so{\ldots}   Libra  
11   Wow, haven't written in a long time. Been pre{\ldots}   Libra  
12   Hehe, just finished dinner! Yum! I'm so happy{\ldots}   Libra  
13   I got so many compliments on my hair! It was {\ldots}   Libra  
14   Avoiding homework{\ldots}yet again, shock shock. p{\ldots}   Libra  
15   Welp{\ldots}haha, what a funny word, perhaps becau{\ldots}   Libra  
16   Yeah, so avoiding homework right now and talk{\ldots}   Libra  
..                                                {\ldots}     {\ldots}  
28   urlLink Awwww{\ldots} isnt Amy a cutie? Posted by {\ldots}  Taurus  
0    I have done such a good job for so long pusin{\ldots}  Cancer  
1    I never said this before, but my Dad is in Ja{\ldots}  Cancer  
2    Happy Birthday to me! I will be using urlLink{\ldots}  Cancer  
3    Tomorrow I turn 25. I have made some great pr{\ldots}  Cancer  
4    A bit of lyrical magic for all of you out in {\ldots}  Cancer  
5    urlLink Are all voluntary acts selfish? Whene{\ldots}  Cancer  
6    "How do you know the chosen ones? No greater {\ldots}  Cancer  
7                   All my friends are make believe.   Cancer  
8    Last couple of days have been hell at work, l{\ldots}  Cancer  
9    Wow, what a weekend. Aside from one very down{\ldots}  Cancer  
10   The realization Today on my way to lunch I fo{\ldots}  Cancer  
11   Bad workout today, I dunno I guess my mind ju{\ldots}  Cancer  
12   My new obsession. I realized that I have been{\ldots}  Cancer  
13   Nice busy day at work, lots of long boring me{\ldots}  Cancer  
14   If at first you dont succeed{\ldots} a) Destroy a{\ldots}  Cancer  
15   Have you ever stopped to think that maybe you{\ldots}  Cancer  
16   Feeling a bit lonely tonight, I will try and {\ldots}  Cancer  
17   This is the start of my blog. Its a place for{\ldots}  Cancer  
18   You shouldnt expect to learn about someones l{\ldots}  Cancer  
19   When I was a kid I used to pray every night f{\ldots}  Cancer  
20   Another day older, hair thinning, aches and b{\ldots}  Cancer  
21   I don't want to start any blasphemous rumours{\ldots}  Cancer  
22   urlLink something positive - the comic equive{\ldots}  Cancer  
23   Chillin to some groove salad, studying BGP co{\ldots}  Cancer  
24   Today we celebrate our independence day. In h{\ldots}  Cancer  
25   Ugh, I think I have allergies{\ldots} My nose has {\ldots}  Cancer  
26   "Science is like sex; occasionally something {\ldots}  Cancer  
27   urlLink Dog toy or marital aid I managed 10/1{\ldots}  Cancer  
28   I had a dream last night about a fight when I{\ldots}  Cancer  

[681288 rows x 7 columns]
Wall time: 3min 33s

    \end{Verbatim}

    And now, the by-author dataframe.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}14}]:} \PY{n}{author\PYZus{}dataframe} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{p}{[}
             \PY{n}{process\PYZus{}file}\PY{p}{(}\PY{n}{i}\PY{p}{,} \PY{n}{by\PYZus{}author}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)} 
             \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n}{tqdm}\PY{p}{(}\PY{n}{files}\PY{p}{,} \PY{n}{desc}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Generating Dataframe}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{p}{]}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Saving author dataframe to Author Data.csv.}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n}{author\PYZus{}dataframe}\PY{o}{.}\PY{n}{to\PYZus{}csv}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{corpus data files/Author Data.csv}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{index}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{author\PYZus{}dataframe}\PY{p}{)}
         \PY{k}{del} \PY{n}{author\PYZus{}dataframe}
\end{Verbatim}


    
    \begin{verbatim}
HBox(children=(IntProgress(value=0, description='Generating Dataframe', max=19320), HTML(value='')))
    \end{verbatim}

    
    \begin{Verbatim}[commandchars=\\\{\}]

Saving author dataframe to Author Data.csv.
       Age Author ID  Gender              Industry  \textbackslash{}
0       37   1000331  female                indUnk   
1       17   1000866  female               Student   
2       23   1004904    male                  Arts   
3       25   1005076  female                  Arts   
4       25   1005545    male           Engineering   
5       48   1007188    male              Religion   
6       26    100812  female          Architecture   
7       16   1008329  female               Student   
8       25   1009572    male                indUnk   
9       27   1011153  female            Technology   
10      25   1011289  female                indUnk   
11      17   1011311  female                indUnk   
12      17   1013637    male            RealEstate   
13      23   1015252  female                indUnk   
14      34   1015556    male            Technology   
15      41   1016560    male            Publishing   
16      26   1016738    male            Publishing   
17      24   1016787  female  Communications-Media   
18      27   1019224  female            RealEstate   
19      24   1019622  female                indUnk   
20      16   1019710    male               Student   
21      25   1021779  female                indUnk   
22      23   1022037    male                indUnk   
23      17   1022086  female               Student   
24      17   1024234  female                indUnk   
25      17   1025783  female               Student   
26      23   1026164  female             Education   
27      15   1026443  female               Student   
28      16   1028027  female                indUnk   
29      26   1028257    male             Education   
{\ldots}    {\ldots}       {\ldots}     {\ldots}                   {\ldots}   
19290   25    970774    male               Banking   
19291   24    971317  female                indUnk   
19292   34    973521  female              Religion   
19293   23     97387    male                  Arts   
19294   17    977947  female                indUnk   
19295   14    978748    male               Student   
19296   26    979795    male            Technology   
19297   25    980769    male                indUnk   
19298   16    980975  female                indUnk   
19299   16    983202    male                indUnk   
19300   24    983609    male            Non-Profit   
19301   27    984615  female            Technology   
19302   23    986006  female                indUnk   
19303   23    987163  female             Education   
19304   16    987614  female               Student   
19305   17    988941  female               Student   
19306   24    989359  female                  Arts   
19307   23    990045  female                indUnk   
19308   24    991110  female             Education   
19309   17    992078    male                indUnk   
19310   34     99290    male              Internet   
19311   25     99382    male                indUnk   
19312   25    993945  female        HumanResources   
19313   27    994348  female                indUnk   
19314   17    994616    male               Student   
19315   36    996147  female    Telecommunications   
19316   25    997488    male                indUnk   
19317   16    998237  female                indUnk   
19318   27    998966    male                indUnk   
19319   25    999503    male              Internet   

                                                   Posts         Sign  
0       Well, everyone got up and going this morning{\ldots}          Leo  
1       Yeah, sorry for not writing for a whole there{\ldots}        Libra  
2       cupid,please hear my cry, cupid, please let y{\ldots}    Capricorn  
3       and did i mention that i no longer have to de{\ldots}       Cancer  
4       B-Logs: The Business Blogs Paradox urlLink Hi{\ldots}  Sagittarius  
5       1/03 DrKioni.com Awarded ByRegion.net Healers{\ldots}        Libra  
6       Friday My dear wife was walking on her gradua{\ldots}        Aries  
7       Sorry, but I gotta..I couldn't remember the w{\ldots}       Pisces  
8       Planning the Marathon I checked Active.com, a{\ldots}       Cancer  
9       The astute among you will note that this run {\ldots}        Virgo  
10      MSN conversation: 11.17am Iggbalbollywall ( t{\ldots}        Libra  
11      Hey!!! Tonight kids, I saw Harry Potter and t{\ldots}      Scorpio  
12      Hey you clowns and kids with Down's. Sorry I {\ldots}        Virgo  
13      aint no such things as halfway crooks. this i{\ldots}       Pisces  
14      well, the retreat was a success, i think. at {\ldots}        Virgo  
15      In his urlLink February 14, 2003 post Ray Mat{\ldots}  Sagittarius  
16      as astute followers of the urlLink Assistant {\ldots}        Libra  
17      You love me{\ldots} I have you here by my side{\ldots} {\ldots}          Leo  
18      Dear Susan, You are a fat Wombat{\ldots} I hate yo{\ldots}        Libra  
19      yay! it changed! :) i think i get it now. you{\ldots}     Aquarius  
20      Yes i survived not eating for 24 hours, I am {\ldots}       Pisces  
21      IN THE SPACESHIP, THE SILVER SPACESHIP I f{\ldots}      Scorpio  
22      Aaaaaahhhh{\ldots} mornings! know that it is s{\ldots}       Cancer  
23      All right, I officially live in THE nut house{\ldots}       Cancer  
24      wait{\ldots}now dat i think about it{\ldots}dat GBC thi{\ldots}        Libra  
25      Lily's Lullaby- Part SIX It was cold that eve{\ldots}       Gemini  
26      I've posted a new test on my AIM profile{\ldots}yo{\ldots}        Aries  
27      Hello hello hello again. It's lovely to speak{\ldots}      Scorpio  
28      OK here goes nothing, I'm starting over. My o{\ldots}        Libra  
29      Wednesday of this week, my wife Elizabeth and{\ldots}        Aries  
{\ldots}                                                  {\ldots}          {\ldots}  
19290   hello   if im a sex and the city fanatic, beb{\ldots}       Taurus  
19291   My very own blog. Reading anya's stuff made m{\ldots}     Aquarius  
19292   The Canadian Anglican Church's top governing {\ldots}       Taurus  
19293   Folx the good news is that I am most probably{\ldots}  Sagittarius  
19294   Pondering the Mountaintop Hello everybody! I {\ldots}        Aries  
19295   Yeah, I decided I want to start blogging agai{\ldots}     Aquarius  
19296   One Hundred Fourteen: The divine beauty Of he{\ldots}       Taurus  
19297   In politics there are certain goals, such as {\ldots}    Capricorn  
19298   Well today was such a change in life{\ldots} I ha{\ldots}       Pisces  
19299   GARR!R!!!!!!! Ok, in my german class today we{\ldots}       Gemini  
19300   Tradition Every Halloween week, since I could{\ldots}       Gemini  
19301   Happy Thanksgiving! For those of us who are i{\ldots}     Aquarius  
19302   Can you name these totally awesome chicks fro{\ldots}       Taurus  
19303   Arun: I'm not deviant! Mell: You're Canadian!{\ldots}          Leo  
19304   urlLink cool anime pic.. haha=).. quite into {\ldots}       Gemini  
19305   No one has joined yet, because I've only just{\ldots}    Capricorn  
19306   I guess I was sort of trying to ignore it, bu{\ldots}  Sagittarius  
19307   (Read of the mo: The End of the Affair - Grah{\ldots}       Gemini  
19308   The other night a friend of mine commented th{\ldots}     Aquarius  
19309   Bush LIED ? That's not possible!!! If your sa{\ldots}      Scorpio  
19310   Another day another post. Anyone else really {\ldots}  Sagittarius  
19311   GLORP! You will be able to view the old site {\ldots}     Aquarius  
19312   I AM FREE, FREE AT LAST! Yes people, I have f{\ldots}          Leo  
19313   POX OFF! I was missed by Claudia for days, an{\ldots}       Pisces  
19314   urlLink mental garage (she's nubs) I dont kno{\ldots}        Libra  
19315   Today's Babbling It's grey. It's snowy. And I{\ldots}          Leo  
19316   i'm not feeling introspective today, so i wil{\ldots}       Cancer  
19317   yea well lets see tonight theres a show but i{\ldots}        Virgo  
19318   2:12AM 11/13. I am watching Martha Stewart ma{\ldots}       Taurus  
19319   I have done such a good job for so long pusin{\ldots}       Cancer  

[19320 rows x 6 columns]

    \end{Verbatim}

    There are a lot of entries in these dataframes. And the saved CSV is
765MB (!!!) on my machine.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}15}]:} \PY{n}{num\PYZus{}words} \PY{o}{=} \PY{n+nb}{sum}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{i}\PY{o}{.}\PY{n}{split}\PY{p}{(}\PY{p}{)}\PY{p}{)} \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n}{blog\PYZus{}dataframe}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Post}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{)}
         \PY{n}{num\PYZus{}authors} \PY{o}{=} \PY{n}{blog\PYZus{}dataframe}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Author ID}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{nunique}\PY{p}{(}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Number of posts:             }\PY{l+s+si}{\PYZob{}blog\PYZus{}dataframe.shape[0]:,\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Number of authors:           }\PY{l+s+si}{\PYZob{}num\PYZus{}authors:,\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Approximate number of words: }\PY{l+s+si}{\PYZob{}num\PYZus{}words:,\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Number of posts:             681,288
Number of authors:           19,320
Approximate number of words: 136,854,709

    \end{Verbatim}

    Now, let's look at some of the ways we can process this text with
various libraries. We'll use the very first blog post as an working
example to show what some of these processes do.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}16}]:} \PY{n}{demo\PYZus{}post} \PY{o}{=} \PY{n+nb}{list}\PY{p}{(}\PY{n}{blog\PYZus{}dataframe}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Post}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{demo\PYZus{}post}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
 Well, everyone got up and going this morning. It's still raining, but that's okay with me. Sort of suits my mood. I could easily have stayed home in bed with my book and the cats. This has been a lot of rain though! People have wet basements, there are lakes where there should be golf courses and fields, everything is green, green, green. But, it is supposed to be 26 degrees by Friday, so we'll be dealing with mosquitos next week. I heard Winnipeg described as an "Old Testament" city on urlLink CBC Radio One last week and it sort of rings true. Floods, infestations, etc., etc.. 

    \end{Verbatim}

    \hypertarget{before-we-continue-some-basic-nlp-ideas}{%
\subsection{Before we continue: some basic NLP
ideas}\label{before-we-continue-some-basic-nlp-ideas}}

    \hypertarget{the-big-challenge-sparsity}{%
\subsubsection{The big challenge:
Sparsity}\label{the-big-challenge-sparsity}}

NLP is fundamentally a branch of machine learning (ML) that focuses just
on language data. As such, it inherits a lot of ideas and concerns from
ML. One of the biggest ones is \emph{sparsity}. Sparsity is the
phenomenon of having a lot of features with ``null'' values for a lot of
your observations (this usually manifests as ``missing data'' or
``zero'').

In language, sparsity is everywhere, since \emph{most utterances don't
use most features.} So if we want to categorize any non-trivial corpus
of language data, we'll need a \emph{lot} of features, but most of them
won't appear in most of our utterances.

Sparsity is bad. It makes models have less data to work with for finding
patterns, and thus, models will tend to overfit--they'll fit very well
to the \emph{current data,} but will generalize poorly to new data.
Thus, almost all of the work in NLP is actually geared at finding ways
to reduce sparsity in language data.

    \hypertarget{data-at-scale}{%
\subsubsection{Data at scale}\label{data-at-scale}}

All of the models of language used in the NLP community are essentially
data-driven. Rules-based models are all but completely dead.

This means that models love data. More data is better than less, and
will make better models, since they're ultimately statistically-based
models.

But more data means more computing time. So, we strive to build models
that balance computational efficiency against power and accuracy. As it
turns out, one of the best ways to do this is to get a basic, but
working, model of language, and just train it on a massive dataset. Your
results may not be perfectly accurate for every single document, but
they should be pretty accurate for the whole dataset, and generally
accurate for most documents, if you do it right.

Bear in mind: computational approaches care about balancing precision
and recall, but they often come at the expense of the other.

    \hypertarget{a-good-enough-model-of-language-just-count-the-words}{%
\subsubsection{A good enough model of language*: Just count the
words!}\label{a-good-enough-model-of-language-just-count-the-words}}

* \emph{At least, for many basic purposes.}

One of the ways to deal with sparsity is to just ignore certain classes
of features and focus on a richer subset. As it turns out, we can do a
surprisingly large amount by ignoring every aspect of language except
for \emph{what lexical units are used} and \emph{how often they are
used}. I.e., ignore syntax, ignore pragmatics, ignore even
semantics--just count words.

It's a brutally simplistic model of language. But for many tasks, it
works, and it is useful. Consider: if my task is to find out ``what are
people talking about,'' e.g.~tracking discussions on Twitter at a large
scale, looking at the words used is probably going to be my fastest way
to get to that.

Equally important, this is a \emph{computationally} simple,
straightforward, and fast approach.

    \hypertarget{reducing-sparsity-through-preprocessing}{%
\subsubsection{Reducing sparsity through
preprocessing}\label{reducing-sparsity-through-preprocessing}}

One of the most important ways to reduce sparsity is through
preprocessing your data. We want to ``condense'' the data down to a
still meaningful representation that irons out all the noise. E.g., we
often want everything to be in the same casee, because computers think
that ``WORD'' (in uppercase) is different from ``word'', ``Word'',
``WoRd'', etc, and does not see any similarities between these tokens
unless we explicitly tell it.

When we're doing a word-counting type analysis, remember that we're
trying to get at a dense representation of the \emph{content} of our
data. Some common preprocessing steps to that end are: * Convert to
lowercase * (Sometimes, but not always) De-accent characters,
e.g.~convert ``'' to ``a''. * Tokenize, i.e.~split a doucment into a
list of tokens (words, punctuation, etc) * Remove tokens: *
\emph{Stopwords} (generally, \emph{function} words), e.g. ``the'',
``a'', ``to'', etc. * Words with very low frequencies (contain very
little information, and are not useful) * Words with extremely high
\emph{document-wise} frequencies (these don't help us discriminate
between documents in our corpus) * Stem, or lemmatize, the text (not
always--depends on the analysis!) * Stemming is significantly faster,
but lemmatization can be more accurate. * But for downstream tasks and
analysis, both stemming and lemmatization tend to perform about the
same. * Find multi-word phrases * This might add features, but those
features may be more meaningful than individual word counts.

In almost all cases, these steps are followed by generating a
\emph{vectorized} representation of the text--e.g.~Bag-of-Words,
Word2Vec, Doc2Vec, GloVe, etc. These vector representations can then be
used for any quantitative, statistical, or machine learning analyses,
e.g.~regressions and classifications.

For other analyses (rather than just counting words), we might be
interested in preserving more sophisticated, perhaps structural,
features of the text: * Identifying (named) entities * Identifying noun
chunks (more or less NPs/DPs) * Syntactic parsing (dependency and
constituent parsing are most common) * Part-of-speech tagging

As for the actual analyses we might do, they are many: * Topic modeling
* Document scoring/classification (e.g., author identification)

The rest of this notebook is a whirlwind tour through some of these
capabilities in Python.

    \hypertarget{preprocessing}{%
\subsection{Preprocessing}\label{preprocessing}}

We can use the two excellent libraries we've already seen: spaCy and
Gensim.

Gensim is a much \emph{faster} library for preprocessing, since it only
operates based on raw string patterns and is designed to be fast and
scale to massive datasets. There is the assumption that any error we
induce through the very simplistic approaches will be balanced out by
the amount of data we work with--generally a good, and correct,
assumption. Gensim does no syntactic parsing, POS tagging, or other such
structure-related analysis, but it's not designed for that.

spaCy is the much \emph{more accurate} library, since it parses text
based on large, powerful pre-trained models (think Stanford's CoreNLP
toolkit--it's very much analogous to that). While still very fast, spaCy
is painfully slow compared to Gensim. But, it has a far more robust
tokenizer, it can do part-of-speech tagging, lemmatization, dependency
parsing, entity and noun chunk identification, and it even has
pre-trained word vectors (and can easily compute vectors for strings of
words or entire documents).

    \hypertarget{gensim}{%
\subsubsection{Gensim}\label{gensim}}

Let's first look at Gensim's preprocessing. There's one
function--\texttt{gensim.parsing.preprocessing.preprocess\_string()}--which
encompasses almost all the basic functions we need: lowercasing,
de-accenting, tokenizing, stemming (with the Porter stemmer algorithm),
stopword removal, removal of numbers, and removal of very short words
(which are generally noise to use).

We can also use the Phrases()/Phraser() objects we saw before to find
multi-word phrases with ease, though given the size of our demo post, we
won't see any show up. We probably want to do this \emph{after} we run
the main preprocessing.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}17}]:} \PY{k+kn}{from} \PY{n+nn}{gensim}\PY{n+nn}{.}\PY{n+nn}{models}\PY{n+nn}{.}\PY{n+nn}{phrases} \PY{k}{import} \PY{n}{Phrases}
         \PY{k+kn}{from} \PY{n+nn}{gensim}\PY{n+nn}{.}\PY{n+nn}{parsing}\PY{n+nn}{.}\PY{n+nn}{preprocessing} \PY{k}{import} \PY{n}{preprocess\PYZus{}string}
         
         \PY{n}{processed} \PY{o}{=} \PY{n}{preprocess\PYZus{}string}\PY{p}{(}\PY{n}{demo\PYZus{}post}\PY{p}{)}
         \PY{n}{phrases} \PY{o}{=} \PY{n}{Phrases}\PY{p}{(}\PY{n}{processed}\PY{p}{)}
         \PY{n}{processed} \PY{o}{=} \PY{n+nb}{list}\PY{p}{(}\PY{n}{phrases}\PY{p}{[}\PY{n}{processed}\PY{p}{]}\PY{p}{)}
         
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Original post:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{demo\PYZus{}post}\PY{p}{)}
         
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{After Gensim preprocessing:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ }\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{join}\PY{p}{(}\PY{n}{processed}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
c:\textbackslash{}users\textbackslash{}andersonh\textbackslash{}appdata\textbackslash{}local\textbackslash{}programs\textbackslash{}python\textbackslash{}python36\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}gensim\textbackslash{}models\textbackslash{}phrases.py:486: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class
  warnings.warn("For a faster implementation, use the gensim.models.phrases.Phraser class")

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
Original post:
 Well, everyone got up and going this morning. It's still raining, but that's okay with me. Sort of suits my mood. I could easily have stayed home in bed with my book and the cats. This has been a lot of rain though! People have wet basements, there are lakes where there should be golf courses and fields, everything is green, green, green. But, it is supposed to be 26 degrees by Friday, so we'll be dealing with mosquitos next week. I heard Winnipeg described as an "Old Testament" city on urlLink CBC Radio One last week and it sort of rings true. Floods, infestations, etc., etc.. 

After Gensim preprocessing:
got go morn rain okai sort suit mood easili stai home bed book cat lot rain peopl wet basement lake golf cours field green green green suppos degre fridai deal mosquito week heard winnipeg describ old testament citi urllink cbc radio week sort ring true flood infest

    \end{Verbatim}

    Notice how the Porter Stemmer finds the uninflected forms of words. It
bases its processing \emph{purely} on the \emph{letters of a word}. This
makes it fast, but it doesn't always give real words or the most correct
forms, e.g. \texttt{"morning"} --\textgreater{} \texttt{"morn"}, and
\texttt{"okay"} --\textgreater{} \texttt{"okai"}. But, for most text
mining or NLP tasks, this is actually not that big of an issue. This
only \emph{really} matters, in any practical sense, for human
interpretation--which, admittedly, is a non-trivial concern.

    \hypertarget{spacy}{%
\subsubsection{spaCy}\label{spacy}}

spaCy requires us to load models in, as we saw earlier when doing
stem-based concordancing. As before, we'll use their small English
model, but we would just change the model name in \texttt{spacy.load()}
if we wanted a different model. The small model will generally be a bit
faster, which is all we need for this demo's purposes.

As before, applying the spaCy pipeline is easy, though we do need to
manually filter our stopwords and punctuation with some explicit checks.
And as with the Gensim examples, we'll run the Phrases() model on our
post, though as before we won't see any changes.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}18}]:} \PY{o}{\PYZpc{}\PYZpc{}}\PY{k}{time}
         
         from gensim.models.phrases import Phrases
         import spacy
         
         nlp = spacy.load(\PYZdq{}en\PYZus{}core\PYZus{}web\PYZus{}sm\PYZdq{})
         processed = [
             i.lemma\PYZus{}
             for i in nlp(demo\PYZus{}post)
             if i.is\PYZus{}stop == False
             and i.is\PYZus{}punct == False
         ]
         phrases = Phrases(processed)
         processed = list(phrases[processed])
         
         print(\PYZdq{}Original post:\PYZdq{})
         print(demo\PYZus{}post)
         
         print(\PYZdq{}\PYZbs{}nAfter spaCy preprocessing:\PYZdq{})
         print(\PYZdq{} \PYZdq{}.join(processed))
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
c:\textbackslash{}users\textbackslash{}andersonh\textbackslash{}appdata\textbackslash{}local\textbackslash{}programs\textbackslash{}python\textbackslash{}python36\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}gensim\textbackslash{}models\textbackslash{}phrases.py:486: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class
  warnings.warn("For a faster implementation, use the gensim.models.phrases.Phraser class")

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
Original post:
 Well, everyone got up and going this morning. It's still raining, but that's okay with me. Sort of suits my mood. I could easily have stayed home in bed with my book and the cats. This has been a lot of rain though! People have wet basements, there are lakes where there should be golf courses and fields, everything is green, green, green. But, it is supposed to be 26 degrees by Friday, so we'll be dealing with mosquitos next week. I heard Winnipeg described as an "Old Testament" city on urlLink CBC Radio One last week and it sort of rings true. Floods, infestations, etc., etc.. 

After spaCy preprocessing:
  well get go morning -PRON- be rain be okay sort suit mood -PRON- easily stay home bed book cat this lot rain people wet basement lake golf course field green green green but suppose 26 degree friday will deal mosquito week -PRON- hear winnipeg describe old testament city urllink cbc radio one week sort ring true flood infestation etc etc
Wall time: 1.03 s

    \end{Verbatim}

    Notice how the processed text is much more \emph{human-readable} with
this approach (this is due to the use of a lemmatizer, rather than a
stemmer). While nice for reporting and inspecting results, the extra
overhead in runtime (not evident in this small example) might make this
an unreasonable proposition for large datasets if time is an issue. And,
as mentioned earlier, if you're doing an \emph{automated analysis} of
your text later, there isn't always a big difference, if any, in how
well stemming versus lemmatization performs.

    \hypertarget{a-detour-through-linguistic-analysis-with-spacy}{%
\subsection{A Detour through Linguistic Analysis with
spaCy}\label{a-detour-through-linguistic-analysis-with-spacy}}

spaCy's language models have a LOT of functionality. Let's look at just
some of the most easily accessible ones.

First, we've already seen spaCy's ability to do lemmatization, stopword
tagging, and punctuation tagging.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}19}]:} \PY{k+kn}{import} \PY{n+nn}{spacy}
         
         \PY{n}{nlp} \PY{o}{=} \PY{n}{spacy}\PY{o}{.}\PY{n}{load}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{en\PYZus{}core\PYZus{}web\PYZus{}sm}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZob{}}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{Token}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{:\PYZlt{}15s\PYZcb{}}\PY{l+s+se}{\PYZbs{}t}\PY{l+s+s2}{\PYZob{}}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{Lemma}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{:\PYZlt{}15s\PYZcb{}}\PY{l+s+se}{\PYZbs{}t}\PY{l+s+s2}{\PYZob{}}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{Is stopword?}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{:\PYZlt{}15s\PYZcb{}}\PY{l+s+se}{\PYZbs{}t}\PY{l+s+s2}{ Is punctuation?}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n}{nlp}\PY{p}{(}\PY{n}{demo\PYZus{}post}\PY{p}{)}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{15}\PY{p}{]}\PY{p}{:}
             \PY{n}{token} \PY{o}{=} \PY{n}{i}\PY{o}{.}\PY{n}{text}
             \PY{n}{lemma} \PY{o}{=} \PY{n}{i}\PY{o}{.}\PY{n}{lemma\PYZus{}}
             \PY{n}{is\PYZus{}stop} \PY{o}{=} \PY{n+nb}{str}\PY{p}{(}\PY{n}{i}\PY{o}{.}\PY{n}{is\PYZus{}stop}\PY{p}{)}
             \PY{n}{is\PYZus{}punct} \PY{o}{=} \PY{n+nb}{str}\PY{p}{(}\PY{n}{i}\PY{o}{.}\PY{n}{is\PYZus{}punct}\PY{p}{)}
             \PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+si}{\PYZob{}token:\PYZlt{}15s\PYZcb{}}\PY{l+s+se}{\PYZbs{}t}\PY{l+s+si}{\PYZob{}lemma:\PYZlt{}15s\PYZcb{}}\PY{l+s+se}{\PYZbs{}t}\PY{l+s+si}{\PYZob{}is\PYZus{}stop:\PYZlt{}15s\PYZcb{}}\PY{l+s+se}{\PYZbs{}t}\PY{l+s+si}{\PYZob{}is\PYZus{}punct\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Token          	Lemma          	Is stopword?   	 Is punctuation?
               	               	False          	False
Well           	well           	False          	False
,              	,              	False          	True
everyone       	everyone       	True           	False
got            	get            	False          	False
up             	up             	True           	False
and            	and            	True           	False
going          	go             	False          	False
this           	this           	True           	False
morning        	morning        	False          	False
.              	.              	False          	True
It             	-PRON-         	False          	False
's             	be             	False          	False
still          	still          	True           	False
raining        	rain           	False          	False

    \end{Verbatim}

    And, we can also do part-of-speech tagging.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}20}]:} \PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZob{}}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{TOKEN}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{:\PYZlt{}15s\PYZcb{}}\PY{l+s+se}{\PYZbs{}t}\PY{l+s+s2}{\PYZob{}}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{COARSE POS}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{:\PYZlt{}17s\PYZcb{}}\PY{l+s+se}{\PYZbs{}t}\PY{l+s+s2}{\PYZob{}}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{FINE POS}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n}{nlp}\PY{p}{(}\PY{n}{demo\PYZus{}post}\PY{p}{)}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{15}\PY{p}{]}\PY{p}{:}
             \PY{n}{token} \PY{o}{=} \PY{n}{i}\PY{o}{.}\PY{n}{text}
             \PY{n}{coarse} \PY{o}{=} \PY{n}{i}\PY{o}{.}\PY{n}{pos\PYZus{}}
             \PY{n}{fine} \PY{o}{=} \PY{n}{i}\PY{o}{.}\PY{n}{tag\PYZus{}}
             \PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+si}{\PYZob{}token:\PYZlt{}15s\PYZcb{}}\PY{l+s+se}{\PYZbs{}t}\PY{l+s+si}{\PYZob{}coarse:\PYZlt{}17s\PYZcb{}}\PY{l+s+se}{\PYZbs{}t}\PY{l+s+si}{\PYZob{}fine\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
TOKEN          	COARSE POS       	FINE POS
               	SPACE            	
Well           	INTJ             	UH
,              	PUNCT            	,
everyone       	NOUN             	NN
got            	VERB             	VBD
up             	PART             	RP
and            	CCONJ            	CC
going          	VERB             	VBG
this           	DET              	DT
morning        	NOUN             	NN
.              	PUNCT            	.
It             	PRON             	PRP
's             	VERB             	VBZ
still          	ADV              	RB
raining        	VERB             	VBG

    \end{Verbatim}

    Entity recognition\ldots{}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}21}]:} \PY{k+kn}{from} \PY{n+nn}{pprint} \PY{k}{import} \PY{n}{pprint}
         \PY{n}{ents} \PY{o}{=} \PY{n}{nlp}\PY{p}{(}\PY{n}{demo\PYZus{}post}\PY{p}{)}\PY{o}{.}\PY{n}{ents}
         \PY{n}{pprint}\PY{p}{(}\PY{n+nb}{list}\PY{p}{(}\PY{n}{ents}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
[this morning,
 26 degrees,
 Friday,
 next week,
 Winnipeg,
 an "Old Testament",
 CBC Radio,
 One last week,
 Floods]

    \end{Verbatim}

    Noun chunk identification\ldots{}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}22}]:} \PY{k+kn}{from} \PY{n+nn}{pprint} \PY{k}{import} \PY{n}{pprint}
         \PY{n}{noun\PYZus{}chunks} \PY{o}{=} \PY{n}{nlp}\PY{p}{(}\PY{n}{demo\PYZus{}post}\PY{p}{)}\PY{o}{.}\PY{n}{noun\PYZus{}chunks}
         \PY{n}{pprint}\PY{p}{(}\PY{n+nb}{list}\PY{p}{(}\PY{n}{noun\PYZus{}chunks}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
[everyone,
 It,
 me,
 Sort of suits,
 my mood,
 I,
 bed,
 my book,
 the cats,
 a lot,
 rain,
 People,
 wet basements,
 lakes,
 golf courses,
 fields,
 everything,
 it,
 26 degrees,
 Friday,
 we,
 mosquitos,
 I,
 Winnipeg,
 an "Old Testament" city,
 it,
 Floods,
 infestations]

    \end{Verbatim}

    And, as we might suspect from the above information, spaCy also does
dependency parsing.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}23}]:} \PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZob{}}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{TOKEN}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{:\PYZlt{}15s\PYZcb{}}\PY{l+s+se}{\PYZbs{}t}\PY{l+s+s2}{\PYZob{}}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{HEAD}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{:\PYZlt{}15s\PYZcb{}}\PY{l+s+se}{\PYZbs{}t}\PY{l+s+s2}{\PYZob{}}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{DEPENDENCY RELATION}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{:\PYZlt{}20s\PYZcb{}}\PY{l+s+se}{\PYZbs{}t}\PY{l+s+s2}{\PYZob{}}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{CHILDREN}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n}{nlp}\PY{p}{(}\PY{n}{demo\PYZus{}post}\PY{p}{)}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{25}\PY{p}{]}\PY{p}{:}
             \PY{n}{token} \PY{o}{=} \PY{n}{i}\PY{o}{.}\PY{n}{text}
             \PY{n}{head} \PY{o}{=} \PY{n}{i}\PY{o}{.}\PY{n}{head}\PY{o}{.}\PY{n}{text}
             \PY{n}{dep} \PY{o}{=} \PY{n}{i}\PY{o}{.}\PY{n}{dep\PYZus{}}
             \PY{n}{children} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{, }\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{join}\PY{p}{(}\PY{n}{c}\PY{o}{.}\PY{n}{text} \PY{k}{for} \PY{n}{c} \PY{o+ow}{in} \PY{n}{i}\PY{o}{.}\PY{n}{children} \PY{k}{if} \PY{o+ow}{not} \PY{n}{c}\PY{o}{.}\PY{n}{is\PYZus{}punct}\PY{p}{)}
             \PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+si}{\PYZob{}token:\PYZlt{}15s\PYZcb{}}\PY{l+s+se}{\PYZbs{}t}\PY{l+s+si}{\PYZob{}head:\PYZlt{}15s\PYZcb{}}\PY{l+s+se}{\PYZbs{}t}\PY{l+s+si}{\PYZob{}dep:\PYZlt{}20s\PYZcb{}}\PY{l+s+se}{\PYZbs{}t}\PY{l+s+si}{\PYZob{}children\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
TOKEN          	HEAD           	DEPENDENCY RELATION 	CHILDREN
               	Well           	                    	
Well           	got            	intj                	 
,              	got            	punct               	
everyone       	got            	nsubj               	
got            	got            	ROOT                	Well, everyone, up, and, going
up             	got            	prt                 	
and            	got            	cc                  	
going          	got            	conj                	morning
this           	morning        	det                 	
morning        	going          	npadvmod            	this
.              	got            	punct               	
It             	raining        	nsubj               	
's             	raining        	aux                 	
still          	raining        	advmod              	
raining        	raining        	ROOT                	It, 's, still, but, 's
,              	raining        	punct               	
but            	raining        	cc                  	
that           	's             	nsubj               	
's             	raining        	conj                	that, okay, with
okay           	's             	acomp               	
with           	's             	prep                	me
me             	with           	pobj                	
.              	's             	punct               	
Sort           	of             	advmod              	
of             	suits          	advmod              	Sort

    \end{Verbatim}

    We can use the built-in disiplaCy tool to generate a visualization of
the dependency parse (though only of the first sentence, for space's
sake):

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}24}]:} \PY{k+kn}{from} \PY{n+nn}{spacy} \PY{k}{import} \PY{n}{displacy}
         \PY{n}{displacy}\PY{o}{.}\PY{n}{render}\PY{p}{(}
             \PY{n}{nlp}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Well, everyone got up and going this morning.}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{p}{,} 
             \PY{n}{style}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{dep}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}
             \PY{n}{jupyter}\PY{o}{=}\PY{k+kc}{True} \PY{c+c1}{\PYZsh{} to make this render correctly in the Jupyter notebook}
         \PY{p}{)}
\end{Verbatim}


    
    \begin{verbatim}
<IPython.core.display.HTML object>
    \end{verbatim}

    
    There's more that spaCy can do, and there are other models and libraries
available for doing this sort of automated parsing and annotation of
text (e.g., there are interfaces to Stanford's CoreNLP suite), but spaCy
is always a good bet since it's fast (for the amount of work it does),
pretty accurate, easy to use, and flexible.

    \hypertarget{topic-modeling}{%
\subsection{Topic Modeling}\label{topic-modeling}}

Topic Modeling refers to a wide variety of algorithms that are used to
explore and discover ``topics'' within a corpus. ``Topic'' is being used
with a very specific meaning here--a topic is a \emph{statistical
distribution of words}. You might already be familiar with Latent
Semantic Analysis (LSA; sometimes called Latent Semantic Indexing, or
LSI), which is an older model for this sort of analysis.

Most modern algorithms are based on
\href{http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf}{Latent
Dirichlet Allocation (LDA)}, which uses word co-occurrences within
documents to determine the topics. LDA has given rise to a number of
subsequent topic models: * Author-Topic models, which are LDA with
metadata (usually, but not always, the author of a piece) * Dynamic
topic models, which include time metadata in the modeling process *
Hierarchical Dirichlet Process, an extension of LDA that is
\emph{nonparametric} with regards to the number of topics (but can give
less clear results in some cases).

All of these models are implemented in Genim. Due to the size of our
corpus and the fact that this is a live demo, we'll use Gensim's
speedier preprocessing tools to work with our data and prepate it for
topic modeling. And we'll only show LDA, since the others do the same
basic thing and look basically the same in code.

Gensim requires that the corpus be in a \emph{bag of words} format for
topic modeling, so we'll need to put our documents in that format first.
Fortunately this requires little code: just do our preprocessing, then
use some pre-built tools from Gensim to do the rest.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}25}]:} \PY{o}{\PYZpc{}\PYZpc{}}\PY{k}{time}
         
         from gensim.corpora import Dictionary
         from gensim.corpora.mmcorpus import MmCorpus
         from gensim.models.phrases import Phrases, Phraser
         from gensim.parsing.preprocessing import preprocess\PYZus{}string
         from tqdm import tqdm\PYZus{}notebook as tqdm
         
         \PYZsh{} preprocess our corpus
         corpus = [
             preprocess\PYZus{}string(i) 
             for i in tqdm(blog\PYZus{}dataframe[\PYZdq{}Post\PYZdq{}], desc=\PYZdq{}Preprocessing\PYZdq{})
         ]
         phrases = Phrases(tqdm(corpus, desc=\PYZdq{}Phrase\PYZhy{}finding\PYZdq{}), min\PYZus{}count=100)
         print(\PYZdq{}Generating Phraser() object for faster phrasing.\PYZdq{})
         phrases = Phraser(phrases)
         corpus = list(phrases[tqdm(corpus, desc=\PYZdq{}Phrasing\PYZdq{})])
         id2word = Dictionary(tqdm(corpus, desc=\PYZdq{}id2word\PYZdq{}))
         vocabsize = len(id2word)
         \PYZsh{} remove tokens with extremely high or low frequencies
         id2word.filter\PYZus{}extremes(
             no\PYZus{}above=.5,  \PYZsh{} remove tokens in \PYZgt{} 50\PYZpc{} of the documents (default)
             no\PYZus{}below=5,   \PYZsh{} remove tokens in \PYZlt{} 5 documents (default)
             keep\PYZus{}n=500000 \PYZsh{} only keep 500k tokens, max\PYZhy{}\PYZhy{}up from default 100k for good measure
         )
         \PYZsh{} Reset index spacings for better efficiency
         id2word.compactify()
         print(f\PYZdq{}Removed \PYZob{}vocabsize \PYZhy{} len(id2word)\PYZcb{} tokens based on frequency criteria.\PYZdq{})
         corpus = [
             id2word.doc2bow(i) 
             for i in tqdm(corpus, desc=\PYZdq{}BoW\PYZdq{})
         ]
\end{Verbatim}


    
    \begin{verbatim}
HBox(children=(IntProgress(value=0, description='Preprocessing', max=681288), HTML(value='')))
    \end{verbatim}

    
    \begin{Verbatim}[commandchars=\\\{\}]


    \end{Verbatim}

    
    \begin{verbatim}
HBox(children=(IntProgress(value=0, description='Phrase-finding', max=681288), HTML(value='')))
    \end{verbatim}

    
    \begin{Verbatim}[commandchars=\\\{\}]

Generating Phraser() object for faster phrasing.

    \end{Verbatim}

    
    \begin{verbatim}
HBox(children=(IntProgress(value=0, description='Phrasing', max=681288), HTML(value='')))
    \end{verbatim}

    
    \begin{Verbatim}[commandchars=\\\{\}]


    \end{Verbatim}

    
    \begin{verbatim}
HBox(children=(IntProgress(value=0, description='id2word', max=681288), HTML(value='')))
    \end{verbatim}

    
    \begin{Verbatim}[commandchars=\\\{\}]

Removed 507524 tokens based on frequency criteria.

    \end{Verbatim}

    
    \begin{verbatim}
HBox(children=(IntProgress(value=0, description='BoW', max=681288), HTML(value='')))
    \end{verbatim}

    
    \begin{Verbatim}[commandchars=\\\{\}]

Wall time: 22min 17s

    \end{Verbatim}

    Around this time we should also notice that we're using a \emph{huge}
amount of RAM. So much, in fact, that the computer might be close to
running out. (It turns out that it's almost entirely from the dataframe
still in memory, but let's just pretend for the moment that it's our
actual corpus). How do we deal with this? Simple: \emph{streaming.}
Gensim is built around the concept of working with data one chunk at a
time--so we can save our data to a file, then read one ``chunk'' of that
file at a time! When dealing with a few hundred, or a few thousand,
documents this isn't needed. But when dealing with \emph{millions} of
documents or more, it's absolutely required, unless you want to spend
thousands of dollars on extremely high-end computing hardware or cloud
computing time/space.

Our corpus doesn't actually require this. (The cell below will show it's
only using a few megabytes--perfectly reasonable). But we'll do it
anyways, just for demonstration's sake, since this same idea and
approach would scale up nicely.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}26}]:} \PY{o}{\PYZpc{}\PYZpc{}}\PY{k}{time}
         
         from sys import getsizeof
         
         \PYZsh{} getsizeof returns bytes\PYZhy{}\PYZhy{}convert to megabytes
         \PYZsh{} nb: factors of 1024, not 1000
         id2word\PYZus{}size = getsizeof(id2word) / 1024
         corpus\PYZus{}size = getsizeof(corpus) / 1048576
         blog\PYZus{}size = getsizeof(blog\PYZus{}dataframe) / 1048576
         phrases\PYZus{}size = getsizeof(phrases) / 1048576
         
         print(f\PYZdq{}id2word dictionary size in memory: \PYZob{}id2word\PYZus{}size:.2f\PYZcb{}KB\PYZdq{})
         print(f\PYZdq{}Gensim corpus size in memory: \PYZob{}corpus\PYZus{}size:.2f\PYZcb{}MB\PYZdq{})
         print(f\PYZdq{}Blog Dataframe size in memory: \PYZob{}blog\PYZus{}size:.2f\PYZcb{}MB\PYZdq{})
         print(f\PYZdq{}Phrases model size in memory: \PYZob{}phrases\PYZus{}size:.2f\PYZcb{}MB\PYZdq{})
         
         \PYZsh{} Delete that massive blog dataframe first\PYZhy{}\PYZhy{}we already saved
         \PYZsh{} it to file.
         del blog\PYZus{}dataframe
         
         print(\PYZdq{}Saving id2word dictionary.\PYZdq{})
         id2word.save(\PYZdq{}corpus data files/id2word\PYZdq{})
         print(\PYZdq{}Serializing bag\PYZhy{}of\PYZhy{}words corpus.\PYZdq{})
         MmCorpus.serialize(
             fname=\PYZdq{}corpus data files/bow\PYZus{}corpus.mm\PYZdq{},
             corpus=corpus,
             id2word=id2word
         )
         print(\PYZdq{}Corpus and id2word dict saved.\PYZdq{})
         
         \PYZsh{} we can reload the id2word and corpus data from the files
         \PYZsh{} we just saved, too.
         del corpus
         del id2word
         
         \PYZsh{} Phrases can be retrained in a few minutes, and
         \PYZsh{} the training is faster than the appliction of
         \PYZsh{} a trained model, so we can delete this.
         del phrases
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
id2word dictionary size in memory: 0.05KB
Gensim corpus size in memory: 5.83MB
Blog Dataframe size in memory: 969.19MB
Phrases model size in memory: 0.00MB
Saving id2word dictionary.
Serializing bag-of-words corpus.
Corpus and id2word dict saved.
Wall time: 1min 13s

    \end{Verbatim}

    When re-reload the corpus, we'll see that it's \emph{much} smaller in
memory. This is because, using Gensim's tools, we're only looking at the
size of the \emph{thing that accesses the data,} but which does not
currently store any of the data--it's all in a file on disk.

Accessing data from disk will be slow, though. Even the fastest SSDs are
still at least 10x slower than even slow RAM. So our runtime will be
limited by \emph{disk access speed}. BUT, disk space is
\emph{significantly} cheaper than RAM space! At the time of writing, 16
gigabytes of RAM costs about \$200*. Meanwhile, \$200 can get you
between 7 and 10 \emph{terabytes} of hard drive space (less if you want
SSDs, though). So while reading corpora off disk is very slow, it lets
us work with \emph{much} larger datasets.

We can re-load the corpus and dictionary we just saved. The dictionary
didn't take up much memory at all, but saving a copy was a good idea
anyways. The corpus also didn't take much space.

*\emph{RAM prices are currently (as of early 2018) very highly inflated,
though; normally this much RAM should only cost about \$100. But the
point still stands: RAM is expensive, hard drives are dirt cheap.}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}27}]:} \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Loading id2word dictionary.}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n}{id2word} \PY{o}{=} \PY{n}{Dictionary}\PY{o}{.}\PY{n}{load}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{corpus data files/id2word}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Loading bag\PYZhy{}of\PYZhy{}words corpus.}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n}{corpus} \PY{o}{=} \PY{n}{MmCorpus}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{corpus data files/bow\PYZus{}corpus.mm}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Loading id2word dictionary.
Loading bag-of-words corpus.

    \end{Verbatim}

    \hypertarget{aside-spacy-bag-of-words-pipeline}{%
\subsubsection{Aside: spaCy bag-of-words
pipeline}\label{aside-spacy-bag-of-words-pipeline}}

We won't run this during this demo, but just for comparison, here's a
spaCy preprocessing pipeline that does the same thing. The only thing
that changes is the first pass through the corpus (the first
\texttt{corpus\ =\ {[}...{]}} bit)--the bag-of-words steps are as
before. The change is shown below--all the other code would be the same.

\begin{verbatim}
import spacy

nlp = spacy.load("en_core_web_sm", disable=["parser", "tagger", "ner"])
corpus = [
    [
        i.lemma_
        for i in nlp(j)
        if i.is_stop == False
        and i.is_punct == False
    ]
    for j in tqdm(texts, desc="Preprocessing")
]
\end{verbatim}

When I ran this on my computer, it took about an hour and a half to run
(\textasciitilde{}200 documents per second), compared to the Gensim
pipeline which took about 11 minutes (\textasciitilde{}1000 documents
per second). Of course, if you remove the \texttt{disable} line, you'll
get much higher-quality results, but it will run about 10x slower
(\textasciitilde{}20 documents per second on my computer, nearly 12
hours total runtime).

However, given the size of the corpus we're working from, the
differences aren't actually that significant in terms of how they'll
affect our results, so we can go with the considerably faster Gensim
approach.

    \hypertarget{back-to-topic-models}{%
\subsubsection{Back to topic models}\label{back-to-topic-models}}

Now, let's run some of these topic models. We won't bother tweaking any
of the default settings (except for chunksize, which should give us a
bit more speed), meaning each one will search for 100 topics. This is
the most important parameter in the models, by far--and sadly, the only
really good way to find a good value is to run it at a range of
different topic numbers and see what gives you useful results. You
\emph{can} look at the \emph{coherence} of each topic (calculated by
Gensum automatically) and use that to evaluate your model, but the
be-all-end-all is the human interpretability and the usefulness of your
topics.

These models will take a while. So we can skip down to the next cell and
just load the models back up from disk.

We also need to do a
\emph{\href{https://en.wikipedia.org/wiki/Tf\%E2\%80\%93idf}{term
frequency-inverse document frequency}} transform on the corpus. This is
a weighting scheme that converts the raw word counts into values that
\emph{decrease} the weight of a word based on how many documents it
appears in (more documents --\textgreater{} proves less informatio about
any individual document, so decrease the weight), and \emph{increases}
it based on how often it appears \emph{within the current document}
(more occurrences --\textgreater{} more important to the document).

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}28}]:} \PY{k+kn}{from} \PY{n+nn}{gensim}\PY{n+nn}{.}\PY{n+nn}{corpora} \PY{k}{import} \PY{n}{Dictionary}
         \PY{k+kn}{from} \PY{n+nn}{gensim}\PY{n+nn}{.}\PY{n+nn}{corpora}\PY{n+nn}{.}\PY{n+nn}{mmcorpus} \PY{k}{import} \PY{n}{MmCorpus}
         \PY{k+kn}{from} \PY{n+nn}{gensim}\PY{n+nn}{.}\PY{n+nn}{models}\PY{n+nn}{.}\PY{n+nn}{ldamulticore} \PY{k}{import} \PY{n}{LdaMulticore}
         \PY{k+kn}{from} \PY{n+nn}{gensim}\PY{n+nn}{.}\PY{n+nn}{models}\PY{n+nn}{.}\PY{n+nn}{atmodel} \PY{k}{import} \PY{n}{AuthorTopicModel}
         \PY{k+kn}{from} \PY{n+nn}{gensim}\PY{n+nn}{.}\PY{n+nn}{models}\PY{n+nn}{.}\PY{n+nn}{hdpmodel} \PY{k}{import} \PY{n}{HdpModel}
         \PY{k+kn}{from} \PY{n+nn}{gensim}\PY{n+nn}{.}\PY{n+nn}{models}\PY{n+nn}{.}\PY{n+nn}{tfidfmodel} \PY{k}{import} \PY{n}{TfidfModel}
         \PY{k+kn}{from} \PY{n+nn}{tqdm} \PY{k}{import} \PY{n}{tqdm\PYZus{}notebook} \PY{k}{as} \PY{n}{tqdm}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}29}]:} \PY{n}{id2word} \PY{o}{=} \PY{n}{Dictionary}\PY{o}{.}\PY{n}{load}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{corpus data files/id2word}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n}{corpus} \PY{o}{=} \PY{n}{MmCorpus}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{corpus data files/bow\PYZus{}corpus.mm}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n}{tfidf\PYZus{}model} \PY{o}{=} \PY{n}{TfidfModel}\PY{p}{(}\PY{n}{tqdm}\PY{p}{(}\PY{n}{corpus}\PY{p}{,} \PY{n}{desc}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{TFIDF Fitting}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{p}{)}
         \PY{c+c1}{\PYZsh{} TfidfModel returns a generator.  We want it as a list to }
         \PY{c+c1}{\PYZsh{} re\PYZhy{}use it for all the models.}
         \PY{n}{corpus} \PY{o}{=} \PY{n+nb}{list}\PY{p}{(}\PY{n}{tfidf\PYZus{}model}\PY{p}{[}\PY{n}{tqdm}\PY{p}{(}\PY{n}{corpus}\PY{p}{,} \PY{n}{desc}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{TFIDF Transforming}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{p}{]}\PY{p}{)}
\end{Verbatim}


    
    \begin{verbatim}
HBox(children=(IntProgress(value=0, description='TFIDF Fitting', max=681288), HTML(value='')))
    \end{verbatim}

    
    \begin{Verbatim}[commandchars=\\\{\}]


    \end{Verbatim}

    
    \begin{verbatim}
HBox(children=(IntProgress(value=0, description='TFIDF Transforming', max=681288), HTML(value='')))
    \end{verbatim}

    
    \begin{Verbatim}[commandchars=\\\{\}]

Serializing tf-idf corpus.

    \end{Verbatim}

    Gensim has a multi-threaded LDA implementation that can take advantage
of multi-core processors for significant speedups; we'll use that.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}30}]:} \PY{o}{\PYZpc{}\PYZpc{}}\PY{k}{time}
         
         \PYZsh{} corpus = MmCorpus.load(\PYZdq{}corpus data files/tfidf\PYZus{}corpus.mm\PYZdq{})
         print(\PYZdq{}Running LDA model.\PYZdq{})
         lda = LdaMulticore(corpus, workers=3, id2word=id2word, chunksize=50000)
         print(\PYZdq{}Saving LDA model to file.\PYZdq{})
         lda.save(\PYZdq{}model files/LDA.model\PYZdq{})
         print(\PYZdq{}Done.\PYZdq{})
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Running LDA model.

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
c:\textbackslash{}users\textbackslash{}andersonh\textbackslash{}appdata\textbackslash{}local\textbackslash{}programs\textbackslash{}python\textbackslash{}python36\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}gensim\textbackslash{}models\textbackslash{}ldamodel.py:802: RuntimeWarning: divide by zero encountered in log
  diff = np.log(self.expElogbeta)

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
Saving LDA model to file.
Done.
Wall time: 15min 6s

    \end{Verbatim}

    Let's look at some of the LDA outputs and see if we can interpret them.
We'll look at only the top ten highest-likelihood topics, sorted by
decreasing likelihood. Since we did no tweaking of the model parameters
(most notably, the number of topics), we shouldn't expect to see very
coherent topics. Of course, tweaking these parameters is largely a
guessing game, involving tweaks followed by manually inspecting the
results. This is beyond the scope of this talk, so for now it's enough
to show how a basic LDA model is run in Gensim.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}31}]:} \PY{k+kn}{from} \PY{n+nn}{pprint} \PY{k}{import} \PY{n}{pprint}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Loading LDA model from file.}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n}{lda} \PY{o}{=} \PY{n}{LdaMulticore}\PY{o}{.}\PY{n}{load}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{model files/LDA.model}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n}{pprint}\PY{p}{(}\PY{n}{lda}\PY{o}{.}\PY{n}{top\PYZus{}topics}\PY{p}{(}\PY{n}{corpus}\PY{p}{,} \PY{n}{topn}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{)}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{10}\PY{p}{]}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Delete the model and corpus to conserve RAM space.}
         \PY{c+c1}{\PYZsh{} We\PYZsq{}ve already saved them to disk, so it can be reloaded.}
         \PY{k}{del} \PY{n}{lda}
         \PY{k}{del} \PY{n}{corpus}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Loading LDA model from file.
[([(0.0032867447, 'urllink'),
   (0.0027863914, 'like'),
   (0.0026201669, 'know'),
   (0.0025476436, 'love'),
   (0.0024109564, 'want'),
   (0.0023818174, 'feel'),
   (0.0023229425, 'time'),
   (0.0022997775, 'thing'),
   (0.0022506749, 'think'),
   (0.0022419214, 'dai')],
  -0.991923335075992),
 ([(0.0044276407, 'urllink'),
   (0.0023756386, 'like'),
   (0.0022402934, 'know'),
   (0.0020682053, 'dai'),
   (0.0020471383, 'think'),
   (0.0020066646, 'thing'),
   (0.001993226, 'look'),
   (0.0019889951, 'time'),
   (0.0019226717, 'want'),
   (0.0018823334, 'good')],
  -1.0021072909489532),
 ([(0.0031452833, 'urllink'),
   (0.002618549, 'like'),
   (0.0023778444, 'love'),
   (0.0023155215, 'know'),
   (0.0022554197, 'time'),
   (0.0022332505, 'dai'),
   (0.0022041786, 'thing'),
   (0.0021714524, 'think'),
   (0.0020662425, 'want'),
   (0.0020051461, 'feel')],
  -1.0371516683648605),
 ([(0.0037324338, 'urllink'),
   (0.0025839917, 'like'),
   (0.0023313707, 'know'),
   (0.0022363507, 'time'),
   (0.0022093586, 'dai'),
   (0.0021959213, 'think'),
   (0.0020698688, 'want'),
   (0.0020404994, 'thing'),
   (0.0020326914, 'peopl'),
   (0.0020270168, 'work')],
  -1.037523835694815),
 ([(0.0033950717, 'urllink'),
   (0.002547115, 'like'),
   (0.0024895007, 'know'),
   (0.002475114, 'love'),
   (0.0022665106, 'time'),
   (0.0021941478, 'want'),
   (0.00216542, 'dai'),
   (0.0021385716, 'thing'),
   (0.0021323161, 'think'),
   (0.0020106426, 'feel')],
  -1.0389515068966084),
 ([(0.0040940056, 'urllink'),
   (0.0025974775, 'like'),
   (0.0022480788, 'know'),
   (0.0022426376, 'dai'),
   (0.0021083711, 'time'),
   (0.002065473, 'work'),
   (0.002014777, 'new'),
   (0.001946062, 'want'),
   (0.0019344733, 'think'),
   (0.0018878003, 'thing')],
  -1.0416680131366436),
 ([(0.0039211577, 'urllink'),
   (0.0035663845, 'nbsp'),
   (0.0026781426, 'work'),
   (0.0025530022, 'like'),
   (0.0023340273, 'dai'),
   (0.0022568388, 'know'),
   (0.002200635, 'think'),
   (0.002124303, 'thing'),
   (0.0020629035, 'time'),
   (0.002020318, 'go')],
  -1.0648945292671508),
 ([(0.0033071246, 'nbsp'),
   (0.0031024385, 'urllink'),
   (0.0029407777, 'like'),
   (0.0027964371, 'know'),
   (0.0024651678, 'dai'),
   (0.0024367403, 'think'),
   (0.002396396, 'work'),
   (0.0023568869, 'time'),
   (0.00229206, 'thing'),
   (0.0022025094, 'want')],
  -1.0649106448359777),
 ([(0.003832554, 'nbsp'),
   (0.0035766826, 'urllink'),
   (0.0027839274, 'like'),
   (0.0026074082, 'know'),
   (0.0024505686, 'think'),
   (0.0023473722, 'want'),
   (0.0023310697, 'work'),
   (0.002326278, 'thing'),
   (0.0022509075, 'peopl'),
   (0.002218816, 'time')],
  -1.0716893580334352),
 ([(0.0032079602, 'urllink'),
   (0.0028091543, 'like'),
   (0.0023741648, 'know'),
   (0.002320525, 'blog'),
   (0.0023021041, 'want'),
   (0.0022548395, 'dai'),
   (0.002238216, 'think'),
   (0.0022324405, 'time'),
   (0.0022161962, 'work'),
   (0.0021896486, 'thing')],
  -1.0718338146744584)]

    \end{Verbatim}

    \hypertarget{word-embeddings}{%
\subsection{Word Embeddings}\label{word-embeddings}}

Word embeddings have absolutely taken over the entire field of NLP,
starting with Mikolov et al's 2013 paper on
\href{https://arxiv.org/abs/1301.3781}{Word2Vec}. The basic idea of
these embeddings: * Some notion of ``meaning'' is recoverable from the
\emph{contexts} that a word occurs in * It is possible to generate a
\emph{vector} representation of a word such that \emph{words appearing
in similar contexts have similar vectors} (or, more intuitively, ``are
close to each other'').

Word vectors are used for almost every kind of task: document scoring
and classification, sentiment analysis, document and word clustering,
machine translation (though less commmonly), and more. Plus, they've
been demonstrated to excel at a number of lexical similarity and analogy
tasks.

Gensim has an impelementation of Word2Vec that we can use on our corpus.
spaCy, meanwhile, comes pre-bundled with word vectors computed using
Stanford's GloVe algorithm (which works differently under the hood, but
in terms of how well the vectors actually perform in any application, is
basically identical).

\hypertarget{brief-aside-poincare-embeddings}{%
\subsubsection{Brief aside: Poincare
Embeddings}\label{brief-aside-poincare-embeddings}}

In May 2017, Maximilian Nickel and Douwe Kiela published
\href{https://arxiv.org/pdf/1705.08039}{an extremely exciting paper} on
embeddings performed in \emph{hyperbolic space} rather than
\emph{Euclidean space} (all prior models were in Euclidean space). These
embeddings--while not yet well-suited to extracting word similarities
and meanings from large bodies of unlabeled text--show an extreme
aptitude for embedding \emph{graph}- and \emph{network}-like data,
e.g.~WordNet, and can capture various relations like hypernymy/hyponymy
and synonymy quite well. Keep an eye on ``Poincare Embeddings''--lots of
interesting things are sure to happen there soon.

\hypertarget{spacy-word-vectors}{%
\subsubsection{spaCy word vectors}\label{spacy-word-vectors}}

We'll work with spaCy's pre-trained vectors just for time's
sake--training word2vec models in Gensim is surprisingly fast, but we'll
be doing basically the same thing downstream. First, as always, we need
to load our data and our model. We'll just use a single blog post, and
we'll turn off a bunch of the spaCy parsing stuff for speed.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}32}]:} \PY{k+kn}{import} \PY{n+nn}{pandas} \PY{k}{as} \PY{n+nn}{pd}
         \PY{k+kn}{import} \PY{n+nn}{spacy}
         
         \PY{n}{nlp} \PY{o}{=} \PY{n}{spacy}\PY{o}{.}\PY{n}{load}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{en\PYZus{}core\PYZus{}web\PYZus{}lg}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} just use the very first post for demos}
         \PY{n}{demo\PYZus{}post} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}csv}\PY{p}{(}
             \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{corpus data files/Blog Data.csv}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}
             \PY{n}{usecols}\PY{o}{=}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Post}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{,}
             \PY{n}{nrows}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,}
             \PY{n}{squeeze}\PY{o}{=}\PY{k+kc}{True}
         \PY{p}{)}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
         \PY{n}{demo\PYZus{}post} \PY{o}{=} \PY{n}{nlp}\PY{p}{(}\PY{n}{demo\PYZus{}post}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}33}]:} \PY{n+nb}{print}\PY{p}{(}\PY{n}{demo\PYZus{}post}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{demo\PYZus{}post}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{vector}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Well
[-1.2486e-01  6.9180e-02 -3.1364e-01 -3.1354e-01  1.4388e-01  1.6573e-01
 -4.0073e-02 -3.4590e-01 -1.7483e-01  2.6147e+00  9.1120e-03  1.8054e-02
  8.3494e-02 -9.3186e-02 -1.0852e-01 -1.4856e-01  1.4402e-01  1.1995e+00
 -5.1814e-01 -4.3844e-02 -2.8039e-01  1.3527e-01 -3.6054e-02 -1.3734e-01
  3.1807e-02  1.7668e-02  4.7540e-02 -3.0738e-02  1.7169e-01 -1.0349e-01
  1.0784e-01  1.9757e-02  6.9675e-02 -1.5200e-01 -1.9508e-01 -1.7867e-01
  1.1583e-01  4.7459e-02 -4.5048e-02 -1.0148e-02 -6.7003e-02  3.0717e-02
 -9.5259e-02 -2.2538e-02  8.2868e-02  1.9983e-01 -1.2923e-01 -1.3680e-01
  2.9010e-02  1.8272e-01  2.2101e-02  1.5804e-01  3.7986e-02  3.1765e-02
 -3.0443e-03  3.1779e-02 -9.1168e-02  3.6951e-02  4.4161e-02  1.0407e-01
  1.5687e-02 -9.7470e-02  4.2405e-02  2.6701e-01 -1.0596e-01 -1.4289e-01
 -6.7763e-02  1.7992e-01  2.6175e-01  4.5349e-02  2.5674e-01  1.2484e-01
  3.6875e-01  7.5486e-02 -1.4867e-01  8.2897e-03  8.9685e-02 -1.3242e-01
 -2.4698e-01  2.3017e-01 -2.4372e-03  1.8298e-01 -2.2386e-01  1.7260e-01
 -2.6223e-02 -3.0136e-01 -1.8803e-01  1.0426e-01 -4.1197e-02  6.9884e-02
 -3.3139e-03 -1.4249e-01  4.4817e-02  3.6930e-01  5.2759e-01 -1.4461e-01
  2.2260e-01 -2.1296e-01 -2.1575e-01 -2.1324e-02 -2.3094e-01 -9.1427e-02
 -1.3952e-01 -6.5416e-02  9.9365e-02 -1.1914e+00 -1.0889e-01 -3.6120e-01
 -6.4289e-02 -1.4312e-01  9.1272e-03 -1.6777e-01  2.1744e-01 -4.6958e-02
  7.1629e-03 -8.1238e-03  4.3175e-02 -5.4813e-02 -1.1981e-01  4.2927e-02
  2.3405e-01  1.0804e-01  1.9480e-01 -5.6464e-02  1.2108e-01  1.0770e-01
 -9.7876e-02 -2.4924e-01  2.4288e-01 -2.4311e-02 -8.8623e-02 -3.1197e-01
 -1.8214e-01  2.1236e-01  2.4771e-01  3.9045e-01  1.9372e-01 -4.0742e-01
  1.3977e-01  1.5621e-01 -1.2266e+00  2.0881e-01  4.3454e-01 -1.0045e-01
 -1.3093e-01 -2.2994e-01 -2.5303e-01 -5.7255e-02  4.4188e-02  3.9772e-03
 -2.1284e-01  2.0268e-01  1.5135e-01 -8.1819e-03 -1.3508e-01 -6.2223e-02
  1.2250e-01 -7.0757e-02 -5.4248e-02 -4.5870e-01  2.6266e-01  1.4630e-01
  4.0067e-02 -1.7087e-01 -2.3487e-02 -2.6435e-01  5.8678e-02 -7.8438e-02
  3.0261e-01 -1.4065e-01 -5.5911e-02  1.8330e-01 -1.0979e-01 -7.1047e-02
 -1.7407e-01 -1.4674e-01 -2.8062e-01  1.3984e-01  1.1339e-01  1.0495e-01
 -1.4357e-01 -1.2663e-01 -3.4565e-01 -1.4041e-01 -1.2706e-01 -2.8720e-01
 -2.9107e-02  5.2271e-02  1.9180e-01 -5.9188e-02  1.1118e-02 -3.6953e-02
 -4.6103e-02 -5.5561e-02 -4.2408e-02 -5.4556e-02 -8.5381e-02 -2.5072e-01
  1.3272e-01  8.9385e-02  5.0679e-02 -1.4402e-01  1.5704e-02  8.2595e-02
  2.8950e-01  1.1995e-01  8.2218e-03  8.6943e-02 -1.4487e-01 -6.4140e-02
 -2.8995e-01 -4.3396e-02  1.0147e-01 -3.6916e-01  1.3197e-01  2.2112e-01
  2.2586e-01 -1.3024e-01  3.4924e-02  1.7595e-01 -5.5850e-04  1.4002e-01
  7.2031e-02  9.4518e-02  2.5377e-01  1.5240e-01 -9.7878e-02  8.5805e-02
  3.9108e-02  1.6801e-01 -2.7371e-01 -8.4246e-02 -1.9891e-01  1.9488e-01
  1.0244e-01 -2.1371e-01  1.9158e-01 -4.9702e-01 -7.6887e-02  1.3054e-01
  1.4384e-01  9.2271e-02 -2.4776e-01  2.9863e-01  4.5713e-01  6.3761e-02
 -2.8826e-01 -1.8834e-01 -6.4870e-02  2.0241e-01  2.9338e-01 -1.1416e-01
 -2.9192e-01 -1.3655e-01  5.2752e-02  2.9894e-02  3.6916e-01  1.2743e-02
 -2.2503e-01  8.0966e-02  3.2966e-01  1.3930e-01 -7.8549e-02  1.1004e-01
 -9.0624e-02 -1.9984e-02  2.2853e-02  3.1763e-03  6.1005e-01  2.6677e-01
  4.9331e-02 -2.5631e-01 -2.4592e-01 -3.0870e-01 -4.1584e-01  3.6741e-01
  1.0777e-01 -1.3235e-02 -8.0141e-02  4.4847e-01  2.7414e-01  1.1039e-01
 -8.1114e-02 -1.6639e-01  1.8136e-02  7.6002e-02  2.0605e-01 -1.8203e-01
  2.9575e-01  5.4778e-02 -4.6968e-01  1.5817e-02 -2.2619e-01  1.1062e-02
  1.8545e-01 -1.1914e-01  2.1583e-01 -4.0342e-01  1.7759e-01  8.9240e-02]

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}34}]:} \PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZob{}}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{Token}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{:\PYZlt{}15s\PYZcb{}}\PY{l+s+se}{\PYZbs{}t}\PY{l+s+s2}{Similarity to }\PY{l+s+s2}{\PYZsq{}}\PY{l+s+si}{\PYZob{}demo\PYZus{}post[1].text\PYZcb{}}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         
         \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n}{demo\PYZus{}post}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{15}\PY{p}{]}\PY{p}{:}
             \PY{c+c1}{\PYZsh{} try/except because some token comparisons throw errors on spaCy\PYZsq{}s end}
             \PY{k}{try}\PY{p}{:}
                 \PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+si}{\PYZob{}i.text:\PYZlt{}15s\PYZcb{}}\PY{l+s+se}{\PYZbs{}t}\PY{l+s+s2}{\PYZob{}}\PY{l+s+s2}{demo\PYZus{}post[1].similarity(i):.3f\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
             \PY{k}{except}\PY{p}{:}
                 \PY{k}{pass}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Token          	Similarity to 'Well'
Well           	1.000
everyone       	0.648
got            	0.552
up             	0.602
and            	0.682
going          	0.631
this           	0.604
morning        	0.387
It             	0.714
's             	0.412
still          	0.710
raining        	0.262

    \end{Verbatim}

    Word vectors can also be extended to computer vectors for entire
documents. Usually, this is done by simply adding or averaging the
vectors for each individual word in the document. (Incidentally, this
exact same approach lets you get a vector for any arbitrary bit of
text!)

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}35}]:} \PY{n+nb}{print}\PY{p}{(}\PY{n}{demo\PYZus{}post}\PY{o}{.}\PY{n}{vector}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
[ 1.31642865e-02  2.37451762e-01 -1.33002967e-01 -1.58166736e-01
  9.16091949e-02  4.56015505e-02 -1.51400210e-03 -1.62576497e-01
 -7.07884729e-02  2.12497878e+00 -1.87663123e-01  2.05625482e-02
  7.54114017e-02 -6.97415769e-02 -1.99725553e-01 -5.24708331e-02
 -2.69564018e-02  1.06616330e+00 -1.82011917e-01 -4.86053340e-02
 -5.35848886e-02  2.18726844e-02 -5.59682995e-02 -3.07797678e-02
  2.45223865e-02 -1.28578171e-02 -1.20025635e-01 -3.46809365e-02
  7.66581818e-02 -7.82760903e-02 -2.33661868e-02  3.36482115e-02
 -4.14343439e-02  2.24824902e-02  5.23805320e-02 -4.10539694e-02
  4.96976711e-02  3.10765095e-02 -5.10399900e-02 -3.25182676e-02
 -1.58267170e-02  3.63390930e-02  2.94042882e-02 -3.33312154e-02
  2.69403644e-02  1.00509606e-01 -1.64681092e-01 -4.72940765e-02
  5.93143664e-02 -5.76425642e-02 -5.66710643e-02  9.40750092e-02
 -2.73930281e-02  2.85059139e-02  4.11496833e-02 -6.96792779e-03
 -1.54412789e-02 -4.68339510e-02  5.63570932e-02 -3.10750045e-02
 -6.35085851e-02 -5.31633161e-02 -4.81112249e-04  1.96307719e-01
  1.30728194e-02 -7.01612234e-02 -2.12039668e-02  4.46476564e-02
 -2.95005795e-02  1.79688618e-01  4.91093583e-02  1.00777403e-01
  1.98810115e-01 -6.47725305e-03  8.86943787e-02  5.91295697e-02
  9.21648890e-02  3.84680904e-03 -6.41978085e-02  1.93143263e-01
 -4.62955497e-02  8.67331773e-02 -6.86863884e-02 -1.38426665e-02
  4.33658510e-02 -2.46838838e-01  1.19494922e-01 -7.30767846e-02
  2.77736813e-01  1.34293601e-01 -1.80763267e-02 -1.45396953e-02
  4.63578245e-03  5.40753379e-02  1.41394556e-01 -2.58688144e-02
  1.91447865e-02  5.76385530e-03 -5.65039255e-02  1.07032163e-02
 -2.87960656e-02  4.17623743e-02 -1.27336815e-01 -5.02941720e-02
  6.15321919e-02 -5.78176796e-01  5.08880280e-02 -7.64341326e-03
  1.52477883e-02  1.23977093e-02  3.75351422e-02 -1.55868232e-01
  1.34658828e-01 -1.35198385e-01 -4.05474156e-02 -4.07232419e-02
  1.43934898e-02 -8.09895433e-03 -1.22347800e-02 -9.73605067e-02
  6.38979748e-02  2.94547584e-02  1.99218541e-02 -7.03123733e-02
  7.94450473e-03  7.23707601e-02  1.59296431e-02 -1.17330894e-01
  1.45509187e-02 -4.16304134e-02  2.19151489e-02 -8.63423571e-02
 -7.60464519e-02  5.95238097e-02  1.16800264e-01  4.88185436e-02
 -1.56893209e-03 -1.40665406e-02  2.85443738e-02  2.45357323e-02
 -1.26371944e+00  1.50656730e-01  2.11000651e-01 -2.11142749e-03
  1.73429642e-02  2.67431065e-02 -6.83486834e-02  2.42024250e-02
  5.20412624e-03 -5.24368025e-02 -5.55904321e-02  2.24909615e-02
  1.34837732e-01  4.61816378e-02 -9.35395807e-02 -7.21071884e-02
 -8.52866769e-02 -6.69751465e-02 -4.74636741e-02 -7.50315413e-02
  7.53237261e-03  2.78376527e-02 -3.41446958e-02 -5.79334050e-02
 -3.71873602e-02 -1.12035863e-01  2.50705909e-02 -5.78852706e-02
  1.34372517e-01  1.50861293e-02 -3.01528946e-02  4.10258621e-02
 -9.67732910e-03 -4.56370153e-02 -1.12796240e-01  6.34944364e-02
  6.69909967e-03 -3.51219364e-02  8.46422929e-03 -9.01845619e-02
  2.66135065e-03 -7.27335662e-02 -1.45241603e-01 -4.90702353e-02
 -2.65349410e-02 -9.75504294e-02 -4.19219211e-02  1.10685546e-02
  8.61035287e-02  2.06585191e-02  4.93311137e-02  7.12807402e-02
 -1.00086510e-01  2.63198689e-02 -1.67035796e-02  1.25243828e-01
 -1.05668139e-02 -1.19949043e-01 -2.14458443e-02  1.60552979e-01
 -9.00433362e-02 -7.66610503e-02 -5.50686717e-02  1.10140545e-02
  1.95168465e-01 -1.84920114e-02  4.93176617e-02  1.34883001e-02
  7.02754483e-02 -3.39424312e-02 -8.85552689e-02 -7.08585307e-02
  4.05594474e-03 -1.70666635e-01  2.76947170e-02  9.22666490e-02
 -4.02171798e-02 -3.87982689e-02 -1.57628641e-01  7.38738105e-02
  1.36203943e-02  6.59089442e-03 -2.76750401e-02  6.13207519e-02
  2.38623507e-02 -2.92682089e-03 -5.13713285e-02  8.41584802e-02
 -5.21717183e-02  2.70015523e-02 -6.06019832e-02  6.67507872e-02
  9.67155546e-02  9.47949141e-02 -1.70890838e-02 -6.73476160e-02
  2.93387454e-02 -1.62215143e-01 -3.64345051e-02  1.06720850e-01
  1.13053983e-02  5.38144931e-02 -5.98892234e-02  8.59192312e-02
  9.49497223e-02 -5.38322888e-02 -6.84643313e-02 -1.06010884e-01
 -9.19809863e-02  1.01062536e-01  4.23326939e-02 -8.91820937e-02
 -9.48470924e-03 -5.99794555e-03  5.45678847e-02  1.95466697e-01
  9.99818891e-02 -8.39695185e-02 -1.06593117e-01  6.10944480e-02
  1.49142385e-01  1.10690340e-01 -3.90812680e-02  7.86308348e-02
  9.38537791e-02 -2.91924439e-02 -5.98246008e-02  1.69358682e-02
  2.10155666e-01  5.94129860e-02 -5.30707985e-02 -5.44334985e-02
 -9.95674431e-02 -1.54171541e-01 -1.36872202e-01  3.95603441e-02
 -3.21762450e-02  7.44344369e-02 -1.45444786e-03  1.98339060e-01
  2.58016914e-01 -7.31731132e-02 -2.78430469e-02 -8.77422020e-02
 -4.37610596e-02 -3.28980982e-02  1.48785040e-01 -1.25699237e-01
  1.48799390e-01 -2.67258789e-02 -1.65224969e-01  1.99796744e-02
 -1.94624401e-04 -2.79833637e-02 -1.45414320e-03  1.93717834e-02
 -3.79961953e-02 -6.66726455e-02 -2.77482066e-02  3.50829475e-02]

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}36}]:} \PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZob{}}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{Token}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{:\PYZlt{}15s\PYZcb{}}\PY{l+s+se}{\PYZbs{}t}\PY{l+s+s2}{Similarity to whole document}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n}{demo\PYZus{}post}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{15}\PY{p}{]}\PY{p}{:}
             \PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+si}{\PYZob{}i.text:\PYZlt{}15s\PYZcb{}}\PY{l+s+se}{\PYZbs{}t}\PY{l+s+s2}{\PYZob{}}\PY{l+s+s2}{demo\PYZus{}post.similarity(i):\PYZlt{}.3f\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Token          	Similarity to whole document
               	0.000
Well           	0.832
,              	0.559
everyone       	0.743
got            	0.699
up             	0.755
and            	0.714
going          	0.788
this           	0.715
morning        	0.581
.              	0.626
It             	0.840
's             	0.530
still          	0.805
raining        	0.413

    \end{Verbatim}

    \hypertarget{classification-and-regression-with-text}{%
\subsection{Classification and Regression with
Text}\label{classification-and-regression-with-text}}

Since we have vectors representing documents (we can also use the
bag-of-words representation for this, too!), we can do any classical
statistical test or modelling we want, like trying to build a model to
predict the age of a post's author from its contents, or predicting
their gender. But we won't use statistics, because we have tools better
suited to the specifics of the tasks at hand: machine learning.

Without getting too deep in the weeds, machine learning models are
essentially extensions of statistics that are based on
\emph{computational mathematics} rather than \emph{classical
mathematics}. They tend to scale better to massive datasets (i.e.~run
faster and predict better) and are often more powerful for pure
prediction, but they are designed for larger datasets and may not work
well at lower sizes.

However, ultimately, the distinction between ``machine learning'' and
``statistics'' is a false one. The real difference is
``experimentalists'' (who seek to understand \emph{causation}) and
``data miners,'' who only want to build models with good
\emph{predictive power}. (I won't get any further into this--but Leo
Breiman's 2001 paper
\href{https://projecteuclid.org/euclid.ss/1009213726}{Statistical
Modeling: The Two Cultures} is an interesting exploration of this
divide).

Python has some absoltuely top-tier machine learning libraries:
Scikit-Learn for non-neural models (e.g.~decision trees, random forests,
support vector machines), and various environments like Keras,
Tensorflow, and Pytorch for building neural networks. We'll use
non-neural models for this demo--they both run faster (as in, by a
factor of days, sometimes) and are \emph{much} easier to interpret.

We'll build six models, using two different sets of target variables and
three different sets of predictors.

Targets: * Author age (regression) * Author gender (binary
classification) * Author's astrological sign (multi-class
classification)

Predictors: * Document vectors, generated by the Doc2Vec algorithm,
which we will train ourselves. * Bag-of-Words model, \emph{without}
tf-idf weighting (it ends up being only marginally helpful for the model
performance of SVMs, but detrimental to the model interpretability)

We'll just one type of model--Support Vector Machines--for both
regression and classification. SVMs strike a nice balance between speed
(though there are faster models) and performance (though there are more
powerful models), and are always a good go-to. For binary classification
tasks, they're still among the best models around. Specifically, we'll
use SVMs with a \emph{linear kernel}--don't worry for now about the
impelementation details of that, just know that a linear kernel SVM runs
faster than other kinds of SVM, and is often plenty good for most tasks.
W

(Spoilers: our predictions will be kinda crap)

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}37}]:} \PY{c+c1}{\PYZsh{} we need a lot more imports for this than before}
         \PY{k+kn}{from} \PY{n+nn}{gensim}\PY{n+nn}{.}\PY{n+nn}{corpora} \PY{k}{import} \PY{n}{Dictionary}
         \PY{k+kn}{from} \PY{n+nn}{gensim}\PY{n+nn}{.}\PY{n+nn}{corpora}\PY{n+nn}{.}\PY{n+nn}{mmcorpus} \PY{k}{import} \PY{n}{MmCorpus}
         \PY{k+kn}{from} \PY{n+nn}{gensim}\PY{n+nn}{.}\PY{n+nn}{matutils} \PY{k}{import} \PY{n}{corpus2csc}
         \PY{k+kn}{from} \PY{n+nn}{gensim}\PY{n+nn}{.}\PY{n+nn}{models} \PY{k}{import} \PY{n}{doc2vec}
         \PY{k+kn}{from} \PY{n+nn}{gensim}\PY{n+nn}{.}\PY{n+nn}{models}\PY{n+nn}{.}\PY{n+nn}{callbacks} \PY{k}{import} \PY{n}{CallbackAny2Vec}
         \PY{k+kn}{from} \PY{n+nn}{gensim}\PY{n+nn}{.}\PY{n+nn}{models}\PY{n+nn}{.}\PY{n+nn}{phrases} \PY{k}{import} \PY{n}{Phrases}\PY{p}{,} \PY{n}{Phraser}
         \PY{k+kn}{from} \PY{n+nn}{gensim}\PY{n+nn}{.}\PY{n+nn}{models}\PY{n+nn}{.}\PY{n+nn}{tfidfmodel} \PY{k}{import} \PY{n}{TfidfModel}
         \PY{k+kn}{from} \PY{n+nn}{gensim}\PY{n+nn}{.}\PY{n+nn}{utils} \PY{k}{import} \PY{n}{simple\PYZus{}preprocess}
         \PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
         \PY{k+kn}{import} \PY{n+nn}{pandas} \PY{k}{as} \PY{n+nn}{pd}
         \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{svm} \PY{k}{import} \PY{n}{LinearSVC}\PY{p}{,} \PY{n}{LinearSVR}
         \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{metrics} \PY{k}{import} \PY{n}{f1\PYZus{}score}\PY{p}{,} \PY{n}{make\PYZus{}scorer}
         \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{model\PYZus{}selection} \PY{k}{import} \PY{n}{GridSearchCV}\PY{p}{,} \PY{n}{RandomizedSearchCV}
         \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{metrics}\PY{n+nn}{.}\PY{n+nn}{pairwise} \PY{k}{import} \PY{n}{cosine\PYZus{}similarity}
         \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{preprocessing} \PY{k}{import} \PY{n}{MaxAbsScaler}\PY{p}{,} \PY{n}{Normalizer}
         \PY{k+kn}{from} \PY{n+nn}{scipy}\PY{n+nn}{.}\PY{n+nn}{sparse} \PY{k}{import} \PY{n}{csr\PYZus{}matrix}
         \PY{k+kn}{import} \PY{n+nn}{spacy}
         \PY{k+kn}{from} \PY{n+nn}{tqdm} \PY{k}{import} \PY{n}{tqdm\PYZus{}notebook} \PY{k}{as} \PY{n}{tqdm}
         
         \PY{k+kn}{from} \PY{n+nn}{pprint} \PY{k}{import} \PY{n}{pprint}
         \PY{k+kn}{import} \PY{n+nn}{re}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}38}]:} \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Reading in target data.}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n}{targets} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}csv}\PY{p}{(}
             \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{corpus data files/Author Data.csv}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}
             \PY{n}{usecols}\PY{o}{=}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Age}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Gender}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Sign}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{,}
             \PY{n}{squeeze}\PY{o}{=}\PY{k+kc}{True}
         \PY{p}{)}
         \PY{n}{ages} \PY{o}{=} \PY{n}{targets}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Age}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{o}{.}\PY{n}{values}
         \PY{n}{genders} \PY{o}{=} \PY{n}{targets}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Gender}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{o}{.}\PY{n}{values}
         \PY{n}{signs} \PY{o}{=} \PY{n}{targets}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Sign}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{o}{.}\PY{n}{values}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Reading in target data.

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}39}]:} \PY{k+kn}{from} \PY{n+nn}{collections} \PY{k}{import} \PY{n}{Counter}
         \PY{c+c1}{\PYZsh{} Look for bad values we might need to remove;}
         \PY{c+c1}{\PYZsh{} the Counter() will tell us how many of each}
         \PY{c+c1}{\PYZsh{} value we have in our dataset.}
         \PY{n}{pprint}\PY{p}{(}\PY{n}{Counter}\PY{p}{(}\PY{n}{ages}\PY{p}{)}\PY{p}{)}
         \PY{n}{pprint}\PY{p}{(}\PY{n}{Counter}\PY{p}{(}\PY{n}{genders}\PY{p}{)}\PY{p}{)}
         \PY{n}{pprint}\PY{p}{(}\PY{n}{Counter}\PY{p}{(}\PY{n}{signs}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Counter(\{17: 2381,
         16: 2152,
         23: 2026,
         24: 1895,
         15: 1771,
         25: 1620,
         26: 1340,
         14: 1246,
         27: 1205,
         13: 690,
         33: 464,
         34: 378,
         35: 338,
         36: 288,
         37: 259,
         38: 171,
         39: 152,
         40: 145,
         41: 139,
         42: 127,
         43: 116,
         45: 103,
         44: 94,
         48: 77,
         46: 72,
         47: 71\})
Counter(\{'female': 9660, 'male': 9660\})
Counter(\{'Virgo': 1783,
         'Cancer': 1722,
         'Libra': 1700,
         'Taurus': 1645,
         'Scorpio': 1631,
         'Leo': 1619,
         'Aries': 1597,
         'Gemini': 1595,
         'Pisces': 1580,
         'Sagittarius': 1549,
         'Aquarius': 1474,
         'Capricorn': 1425\})

    \end{Verbatim}

    Fortunately, all of our categorical data is pretty well-balanged (i.e.,
there isn't too huge of a discrepancy between the number of observations
in each class). For classification tasks this is extremely important,
sometimes moreso than regression tasks.

Now, we \emph{could} get the GloVe vectors from spaCy, but this will
take a pretty long time (about 5 hours on my computer, by my
estimation). So we won't do that. But if we wanted to, the code would
look like this:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{o}{\PYZpc{}\PYZpc{}}\PY{k}{time}
        
        print(\PYZdq{}Loading spaCy model.\PYZdq{})
        \PYZsh{} Only large model has pre\PYZhy{}trained word vectors.
        \PYZsh{} Disable the parser, tagger, and named entity recognizer
        \PYZsh{} for extra speed in this step.
        nlp = spacy.load(\PYZdq{}en\PYZus{}core\PYZus{}web\PYZus{}lg\PYZdq{}, disable=[\PYZdq{}parser\PYZdq{}, \PYZdq{}tagger\PYZdq{}, \PYZdq{}ner\PYZdq{}])
        print(\PYZdq{}Loading blog posts.\PYZdq{})
        glove\PYZus{}corpus = list(pd.read\PYZus{}csv(
            \PYZdq{}corpus data files/Author Data.csv\PYZdq{},
            usecols=[\PYZdq{}Posts\PYZdq{}],
            squeeze=True
        ))
        print(\PYZdq{}Retrieving vectors.\PYZdq{})
        glove\PYZus{}corpus = np.array([
            nlp(i).vector
            for i in tqdm(glove\PYZus{}corpus, desc=\PYZdq{}spaCy vectors\PYZdq{})
        ])
        \PYZsh{} Cast to a 32\PYZhy{}bit floating point format (smaller memory/on\PYZhy{}disk size)
        \PYZsh{} and save.  This means we don\PYZsq{}t have to re\PYZhy{}run the above time\PYZhy{}
        \PYZsh{} consuming steps every time we want to do some work with this
        \PYZsh{} matrix.
        np.save(
            \PYZdq{}corpus data files/GloVe matrix.npy\PYZdq{}, 
            glove\PYZus{}corpus.astype(np.float32)
        )
\end{Verbatim}


    This might not be the most practical way to get vectors for large
amounts of text due to runtime, but the vectors you get are pretty much
guaranteed to be high-quality (assuming, of course, you're working on
data that's more or less general English, rather than being highly
domain-specific, e.g.~financial filings or court decisions). For
comparison (both of code and of downstream results), we'll also use
Doc2Vec, as implemented in Gensim, to generate our own word vectors.
Doc2Vec is one of the many vectorization/embedding approaches, but
rather than working at the word level, it works directly at the document
level. We'll use a less aggressive preprocessing, since vectorization
models are less sensitive to sparsity; this one just de-accents,
lowercases, and removes tokens by length. We'll leave all settings at
default (so, mininum length 2, maximum length 15).

We'll also use 300-dimensional vectors for our embeddings, which is a
pretty standard size for production code. Note that in general, higher
dimensional embeddings (e.g.~300 vs 100) will give better, more accurate
results, but will be sparser and take longer to run models on.

Note that we're not applying most of the usual preprocessing steps we've
already seen. We're just casting to lowercase and splitting at word
boundaries with a regular expression. We won't even bother with
phrase-finding for this. We're doing this because vectorization
algorithms--especially those based on Word2Vec, including
Doc2Vec--aren't as senstivie to sparsity as Bag of Words models, and
since different inflectional forms of a word might actually occur in
consistently different contexts that we wish to capture.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}40}]:} \PY{o}{\PYZpc{}\PYZpc{}}\PY{k}{time}
         
         splitter = re.compile(r\PYZdq{}[a\PYZhy{}z0\PYZhy{}9\PYZsq{}]\PYZdq{})
         print(\PYZdq{}Reading blog data.\PYZdq{})
         df = pd.read\PYZus{}csv(
             \PYZdq{}corpus data files/Author Data.csv\PYZdq{}, 
             usecols=[\PYZdq{}Posts\PYZdq{}], 
             squeeze=True
         ).values
         df = [
             splitter.findall(i.lower())
             for i in tqdm(df, desc=\PYZdq{}Preprocessing\PYZdq{})
         ]
         \PYZsh{} phrasing
         \PYZsh{} phrases = Phrases(tqdm(df, desc=\PYZdq{}Phrase\PYZhy{}finding\PYZdq{}), min\PYZus{}count=100)
         \PYZsh{} print(\PYZdq{}Creating Phraser() object for faster phrasing.\PYZdq{})
         \PYZsh{} phrases = Phraser(phrases)
         \PYZsh{} df = phrases[tqdm(df, desc=\PYZdq{}Applying phraser\PYZdq{})]
         df = [
             doc2vec.TaggedDocument(i, [j])
             for j,i in enumerate(df)
         ]
         
         print(\PYZdq{}Done.\PYZdq{})
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Reading blog data.

    \end{Verbatim}

    
    \begin{verbatim}
HBox(children=(IntProgress(value=0, description='Preprocessing', max=19320), HTML(value='')))
    \end{verbatim}

    
    \begin{Verbatim}[commandchars=\\\{\}]

Done.
Wall time: 2min 5s

    \end{Verbatim}

    We'll create an EpochLogger class from Gensim's CallbackAny2Vec class.
The only thing this will do for us is that, when we pass it to our
Doc2Vec model, it will print out the progress during training. This
doesn't affect the code, but like the tqdm() calls scattered throughout
this notebook, it just lets us keep track of progress.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}41}]:} \PY{c+c1}{\PYZsh{} class to let us track training progress}
         \PY{c+c1}{\PYZsh{} through Gensim\PYZsq{}s callbacks interface}
         \PY{k}{class} \PY{n+nc}{EpochLogger}\PY{p}{(}\PY{n}{CallbackAny2Vec}\PY{p}{)}\PY{p}{:}
             \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Callback to log information about training}\PY{l+s+s2}{\PYZdq{}}
             \PY{k}{def} \PY{n+nf}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{)}\PY{p}{:}
                 \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{epoch} \PY{o}{=} \PY{l+m+mi}{1}
         
             \PY{k}{def} \PY{n+nf}{on\PYZus{}epoch\PYZus{}begin}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{model}\PY{p}{)}\PY{p}{:}
                 \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Epoch \PYZsh{}}\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{ start}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{epoch}\PY{p}{)}\PY{p}{)}
         
             \PY{k}{def} \PY{n+nf}{on\PYZus{}epoch\PYZus{}end}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{model}\PY{p}{)}\PY{p}{:}
                 \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Epoch \PYZsh{}}\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{ end}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{epoch}\PY{p}{)}\PY{p}{)}
                 \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{epoch} \PY{o}{+}\PY{o}{=} \PY{l+m+mi}{1}
\end{Verbatim}


    And now, we run the Doc2Vec model and save it to file for potential
later use.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}42}]:} \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Beginning Doc2Vec.}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n}{epoch\PYZus{}logger} \PY{o}{=} \PY{n}{EpochLogger}\PY{p}{(}\PY{p}{)}
         \PY{n}{d2v} \PY{o}{=} \PY{n}{doc2vec}\PY{o}{.}\PY{n}{Doc2Vec}\PY{p}{(}
             \PY{n}{df}\PY{p}{,}
             \PY{n}{min\PYZus{}count}\PY{o}{=}\PY{l+m+mi}{5}\PY{p}{,}
             \PY{n}{epochs}\PY{o}{=}\PY{l+m+mi}{25}\PY{p}{,}
             \PY{n}{vector\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{300}\PY{p}{,}
             \PY{n}{window}\PY{o}{=}\PY{l+m+mi}{5}\PY{p}{,}
             \PY{n}{worker}\PY{o}{=}\PY{l+m+mi}{3}\PY{p}{,}
             \PY{n}{callbacks}\PY{o}{=}\PY{p}{[}\PY{n}{epoch\PYZus{}logger}\PY{p}{]}
         \PY{p}{)}
         
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Doc2Vec model trained.  Saving model to file.}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n}{d2v}\PY{o}{.}\PY{n}{save}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{model files/Doc2Vec}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Model saved.}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Beginning Doc2Vec.
Epoch \#1 start
Epoch \#1 end
Epoch \#2 start
Epoch \#2 end
Epoch \#3 start
Epoch \#3 end
Epoch \#4 start
Epoch \#4 end
Epoch \#5 start
Epoch \#5 end
Epoch \#6 start
Epoch \#6 end
Epoch \#7 start
Epoch \#7 end
Epoch \#8 start
Epoch \#8 end
Epoch \#9 start
Epoch \#9 end
Epoch \#10 start
Epoch \#10 end
Epoch \#11 start
Epoch \#11 end
Epoch \#12 start
Epoch \#12 end
Epoch \#13 start
Epoch \#13 end
Epoch \#14 start
Epoch \#14 end
Epoch \#15 start
Epoch \#15 end
Epoch \#16 start
Epoch \#16 end
Epoch \#17 start
Epoch \#17 end
Epoch \#18 start
Epoch \#18 end
Epoch \#19 start
Epoch \#19 end
Epoch \#20 start
Epoch \#20 end
Epoch \#21 start
Epoch \#21 end
Epoch \#22 start
Epoch \#22 end
Epoch \#23 start
Epoch \#23 end
Epoch \#24 start
Epoch \#24 end
Epoch \#25 start
Epoch \#25 end
Doc2Vec model trained.  Saving model to file.
Model saved.

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}43}]:} \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Loading model from file.}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n}{d2v} \PY{o}{=} \PY{n}{doc2vec}\PY{o}{.}\PY{n}{Doc2Vec}\PY{o}{.}\PY{n}{load}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{model files/Doc2Vec}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n}{d2v\PYZus{}corpus} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{n}{d2v}\PY{o}{.}\PY{n}{infer\PYZus{}vector}\PY{p}{(}\PY{n}{i}\PY{o}{.}\PY{n}{words}\PY{p}{)} \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n}{tqdm}\PY{p}{(}\PY{n}{df}\PY{p}{)}\PY{p}{]}\PY{p}{)}
         \PY{c+c1}{\PYZsh{} df takes up a good chunk of memory, and we\PYZsq{}re done with it,}
         \PY{c+c1}{\PYZsh{} so delete it to conserve RAM.}
         \PY{k}{del} \PY{n}{df}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Done.  Saving document vectors.}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n}{np}\PY{o}{.}\PY{n}{save}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{corpus data files/Doc2Vec Matrix.npy}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{d2v\PYZus{}corpus}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Done.}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Loading model from file.

    \end{Verbatim}

    
    \begin{verbatim}
HBox(children=(IntProgress(value=0, max=19320), HTML(value='')))
    \end{verbatim}

    
    \begin{Verbatim}[commandchars=\\\{\}]

Done.  Saving document vectors.
Done.

    \end{Verbatim}

    And now we re-generate our bag-of-words representation for the by-author
corpus and save it to disk. Note that we could do tf-idf scaling here,
and it might help out results a bit, and it would probably help our
performance a bit, but we won't--it will make interpreting our model
considerably more difficult. Instead, we'll just apply a standard
scaling on the sparse matrix, scaling every feature such that the
maximum absolute value is 1. (we won't center it to zero mean and unit
variance--that would destroy the sparsity and we couldn't load the
matrix into memory)

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}44}]:} \PY{o}{\PYZpc{}\PYZpc{}}\PY{k}{time}
         
         from gensim.parsing.preprocessing import preprocess\PYZus{}string
         from gensim.models.phrases import Phrases, Phraser
         
         print(\PYZdq{}Reading in posts.\PYZdq{})
         corpus = list(pd.read\PYZus{}csv(
             \PYZdq{}corpus data files/Author Data.csv\PYZdq{},
             usecols=[\PYZdq{}Posts\PYZdq{}],
             squeeze=True
         ))
         
         \PYZsh{} preprocess our corpus
         corpus = [
             preprocess\PYZus{}string(i) 
             for i in tqdm(corpus, desc=\PYZdq{}Preprocessing\PYZdq{})
         ]
         phrases = Phrases(tqdm(corpus, desc=\PYZdq{}Phrase\PYZhy{}finding\PYZdq{}), min\PYZus{}count=100)
         corpus = list(phrases[tqdm(corpus, desc=\PYZdq{}Phrasing\PYZdq{})])
         id2word = Dictionary(tqdm(corpus, desc=\PYZdq{}id2word\PYZdq{}))
         vocabsize = len(id2word)
         \PYZsh{} remove tokens with extremely high or low frequencies
         id2word.filter\PYZus{}extremes(
             no\PYZus{}above=.5,  \PYZsh{} remove tokens in \PYZgt{} 50\PYZpc{} of the documents (default)
             no\PYZus{}below=5,   \PYZsh{} remove tokens in \PYZlt{} 5 documents (default)
             keep\PYZus{}n=500000 \PYZsh{} only keep 500k tokens, max\PYZhy{}\PYZhy{}up from default 100k for good measure
         )
         \PYZsh{} Reset index spacings for better efficiency
         id2word.compactify()
         print(f\PYZdq{}Removed \PYZob{}vocabsize \PYZhy{} len(id2word)\PYZcb{} tokens based on frequency criteria.\PYZdq{})
         corpus = [
             id2word.doc2bow(i) 
             for i in tqdm(corpus, desc=\PYZdq{}BoW\PYZdq{})
         ]
         
         print(\PYZdq{}Serializing corpus.\PYZdq{})
         id2word.save(\PYZdq{}corpus data files/author\PYZus{}id2word\PYZdq{})
         MmCorpus.serialize(
             fname=\PYZdq{}corpus data files/author\PYZus{}corpus.mm\PYZdq{},
             corpus=corpus,
             id2word=id2word
         )
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Reading in posts.

    \end{Verbatim}

    
    \begin{verbatim}
HBox(children=(IntProgress(value=0, description='Preprocessing', max=19320), HTML(value='')))
    \end{verbatim}

    
    \begin{Verbatim}[commandchars=\\\{\}]


    \end{Verbatim}

    
    \begin{verbatim}
HBox(children=(IntProgress(value=0, description='Phrase-finding', max=19320), HTML(value='')))
    \end{verbatim}

    
    \begin{Verbatim}[commandchars=\\\{\}]


    \end{Verbatim}

    
    \begin{verbatim}
HBox(children=(IntProgress(value=0, description='Phrasing', max=19320), HTML(value='')))
    \end{verbatim}

    
    \begin{Verbatim}[commandchars=\\\{\}]
c:\textbackslash{}users\textbackslash{}andersonh\textbackslash{}appdata\textbackslash{}local\textbackslash{}programs\textbackslash{}python\textbackslash{}python36\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}gensim\textbackslash{}models\textbackslash{}phrases.py:486: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class
  warnings.warn("For a faster implementation, use the gensim.models.phrases.Phraser class")

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]


    \end{Verbatim}

    
    \begin{verbatim}
HBox(children=(IntProgress(value=0, description='id2word', max=19320), HTML(value='')))
    \end{verbatim}

    
    \begin{Verbatim}[commandchars=\\\{\}]

Removed 521864 tokens based on frequency criteria.

    \end{Verbatim}

    
    \begin{verbatim}
HBox(children=(IntProgress(value=0, description='BoW', max=19320), HTML(value='')))
    \end{verbatim}

    
    \begin{Verbatim}[commandchars=\\\{\}]

Serializing corpus.
Wall time: 19min 59s

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}45}]:} \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Loading bag\PYZhy{}of\PYZhy{}words (non tf\PYZhy{}idf) corpus.}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n}{corpus} \PY{o}{=} \PY{n+nb}{list}\PY{p}{(}\PY{n}{MmCorpus}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{corpus data files/author\PYZus{}corpus.mm}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{p}{)}
         \PY{n}{corpus} \PY{o}{=} \PY{n}{corpus2csc}\PY{p}{(}\PY{n}{corpus}\PY{p}{)}\PY{o}{.}\PY{n}{transpose}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{tocsr}\PY{p}{(}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Scaling bag\PYZhy{}of\PYZhy{}words features.}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n}{corpus} \PY{o}{=} \PY{n}{MaxAbsScaler}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{fit\PYZus{}transform}\PY{p}{(}\PY{n}{corpus}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Loading bag-of-words (non tf-idf) corpus.
Scaling bag-of-words features.

    \end{Verbatim}

    Now let's do the actual modeling. We'll use Support Vector Machines for
classification and regression; these are pretty powerful models and
generally pretty good choices. Others, which we won't use here because
they take longer to run, include Random Forests, Adaboost, Stochastic
Gradient Descent (which is fast, but has a \emph{lot} of parameters to
tune), and neural networks.

We will:

\begin{itemize}
\tightlist
\item
  Use a grid search to exhaustively search for good parameters (within a
  pre-defined search space). This is called \emph{(hyper)parameter
  optimization.}
\item
  Use 3-fold cross-validation to assess model performance for each
  parameter combination we try.

  \begin{itemize}
  \tightlist
  \item
    Data is split into 3 equally sized ``folds.'' (usually, 5 or 10
    folds would be used, but for time's sake, 3 will suffice here).
  \item
    Four folds are used to train the model; the fifth is used to assess
    its performance.
  \item
    All permutations of ``train-on-2, test-on-1'' are performed, and
    scores are averaged to get the overall model performance.
  \end{itemize}
\item
  Print out the best model parameters and the best score for each of our
  nine feature-target combinations.
\item
  ``Open up'' one of the models and look at the feature weights it
  learned.
\end{itemize}

Cross-fold validation is often used in place of statistical significance
testing in the machine learning world. There are a lot of reasons for
this, and the differences are pretty interesting, but for our purposes,
we only need to know two things to understand why we prefer
cross-validation to significance testing. 1. In most practical cases, ML
models are being used with an eye towards strong \emph{predictive power}
rather than strong \emph{explanatory power.} Thus, we're more interested
in assessing how well our model generalizes to new data rather than the
statistical significance of relationships we uncover. 2. Cross-fold
validation evaluates the model on data that it did not see during
training, approximating the process of feeding novel data into the
model. This is far more reliable on very large, randomly sampled
datasets.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}53}]:} \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{externals} \PY{k}{import} \PY{n}{joblib}
         
         \PY{c+c1}{\PYZsh{} Load our vector corpus from file and normalize}
         \PY{c+c1}{\PYZsh{} each observation (not feature) to have magnitude 1}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Loading and Doc2Vec corpus.}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n}{d2v\PYZus{}corpus} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{load}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{corpus data files/Doc2Vec matrix.npy}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n}{d2v\PYZus{}corpus} \PY{o}{=} \PY{n}{Normalizer}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{fit\PYZus{}transform}\PY{p}{(}\PY{n}{d2v\PYZus{}corpus}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Done.}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         
         
         \PY{c+c1}{\PYZsh{} Functions to optimize our classifiers/regressors.}
         \PY{k}{def} \PY{n+nf}{fit\PYZus{}svr}\PY{p}{(}\PY{n}{data}\PY{p}{,} \PY{n}{labels}\PY{p}{,} \PY{n}{outfile}\PY{p}{)}\PY{p}{:}
             \PY{n}{crossval} \PY{o}{=} \PY{n}{GridSearchCV}\PY{p}{(}
                 \PY{n}{estimator}\PY{o}{=}\PY{n}{LinearSVR}\PY{p}{(}\PY{p}{)}\PY{p}{,}
                 \PY{n}{param\PYZus{}grid}\PY{o}{=}\PY{p}{\PYZob{}}
                     \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{C}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:}\PY{n}{np}\PY{o}{.}\PY{n}{logspace}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{7}\PY{p}{)}\PY{p}{,}
                 \PY{p}{\PYZcb{}}\PY{p}{,}
                 \PY{n}{n\PYZus{}jobs}\PY{o}{=}\PY{l+m+mi}{3}\PY{p}{,}
                 \PY{n}{verbose}\PY{o}{=}\PY{l+m+mi}{3}\PY{p}{,}
                 \PY{n}{cv}\PY{o}{=}\PY{l+m+mi}{3}\PY{p}{,}
             \PY{p}{)}
             \PY{n}{crossval}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{data}\PY{p}{,} \PY{n}{labels}\PY{p}{)}
             \PY{n}{joblib}\PY{o}{.}\PY{n}{dump}\PY{p}{(}\PY{n}{crossval}\PY{o}{.}\PY{n}{best\PYZus{}estimator\PYZus{}}\PY{p}{,} \PY{n}{outfile}\PY{p}{)}
             
             \PY{k}{return} \PY{n}{crossval}\PY{o}{.}\PY{n}{best\PYZus{}score\PYZus{}}\PY{p}{,} \PY{n}{crossval}\PY{o}{.}\PY{n}{best\PYZus{}params\PYZus{}}
         
         \PY{k}{def} \PY{n+nf}{fit\PYZus{}svc}\PY{p}{(}\PY{n}{data}\PY{p}{,} \PY{n}{labels}\PY{p}{,} \PY{n}{outfile}\PY{p}{)}\PY{p}{:}
             \PY{n}{crossval} \PY{o}{=} \PY{n}{GridSearchCV}\PY{p}{(}
                 \PY{n}{estimator}\PY{o}{=}\PY{n}{LinearSVC}\PY{p}{(}\PY{p}{)}\PY{p}{,}
                 \PY{n}{param\PYZus{}grid}\PY{o}{=}\PY{p}{\PYZob{}}
                     \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{C}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:}\PY{n}{np}\PY{o}{.}\PY{n}{logspace}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{7}\PY{p}{)}
                 \PY{p}{\PYZcb{}}\PY{p}{,}
                 \PY{n}{n\PYZus{}jobs}\PY{o}{=}\PY{l+m+mi}{3}\PY{p}{,}
                 \PY{n}{verbose}\PY{o}{=}\PY{l+m+mi}{3}\PY{p}{,}
                 \PY{n}{cv}\PY{o}{=}\PY{l+m+mi}{3}\PY{p}{,}
                 \PY{n}{scoring}\PY{o}{=}\PY{n}{make\PYZus{}scorer}\PY{p}{(}\PY{n}{f1\PYZus{}score}\PY{p}{,} \PY{n}{average}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{macro}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
             \PY{p}{)}
             \PY{n}{crossval}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{data}\PY{p}{,} \PY{n}{labels}\PY{p}{)}
             \PY{n}{joblib}\PY{o}{.}\PY{n}{dump}\PY{p}{(}\PY{n}{crossval}\PY{o}{.}\PY{n}{best\PYZus{}estimator\PYZus{}}\PY{p}{,} \PY{n}{outfile}\PY{p}{)}
             
             \PY{k}{return} \PY{n}{crossval}\PY{o}{.}\PY{n}{best\PYZus{}score\PYZus{}}\PY{p}{,} \PY{n}{crossval}\PY{o}{.}\PY{n}{best\PYZus{}params\PYZus{}}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Loading and Doc2Vec corpus.
Done.

    \end{Verbatim}

    Now, let's optimize some models! We'll save the best-performing models
to file for each set.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}54}]:} \PY{o}{\PYZpc{}\PYZpc{}}\PY{k}{time}
         
         from sklearn.externals import joblib
         
         print(\PYZdq{}Beginning CSR\PYZhy{}Age regression fit.\PYZdq{})
         age\PYZus{}bow = fit\PYZus{}svr(corpus, ages, \PYZdq{}model files/age\PYZus{}bow.pkl\PYZdq{})
         
         print(\PYZdq{}Beginning Doc2Vec\PYZhy{}Age regression fit.\PYZdq{})
         age\PYZus{}d2v = fit\PYZus{}svr(d2v\PYZus{}corpus, ages, \PYZdq{}model files/age\PYZus{}d2v.pkl\PYZdq{})
         
         print(\PYZdq{}Beginning CSR\PYZhy{}Gender binary classification fit.\PYZdq{})
         gender\PYZus{}bow = fit\PYZus{}svc(corpus, genders, \PYZdq{}model files/gender\PYZus{}bow.pkl\PYZdq{})
         
         print(\PYZdq{}Beginning Doc2Vec\PYZhy{}Gender binary classification fit.\PYZdq{})
         gender\PYZus{}d2v = fit\PYZus{}svc(d2v\PYZus{}corpus, genders, \PYZdq{}model files/gender\PYZus{}d2v.pkl\PYZdq{})
         
         print(\PYZdq{}Beginning CSR\PYZhy{}Sign classification fit.\PYZdq{})
         sign\PYZus{}bow = fit\PYZus{}svc(corpus, signs,\PYZdq{}model files/sign\PYZus{}bow.pkl\PYZdq{})
         
         print(\PYZdq{}Beginning Doc2Vec\PYZhy{}Sign classification fit.\PYZdq{})
         sign\PYZus{}d2v = fit\PYZus{}svc(corpus, signs, \PYZdq{}model files/sign\PYZus{}d2v.pkl\PYZdq{})
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Beginning CSR-Age regression fit.
Fitting 3 folds for each of 7 candidates, totalling 21 fits

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
[Parallel(n\_jobs=3)]: Done  21 out of  21 | elapsed:  2.0min finished

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
Beginning Doc2Vec-Age regression fit.
Fitting 3 folds for each of 7 candidates, totalling 21 fits

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
[Parallel(n\_jobs=3)]: Done  21 out of  21 | elapsed:    3.3s finished

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
Beginning CSR-Gender binary classification fit.
Fitting 3 folds for each of 7 candidates, totalling 21 fits

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
[Parallel(n\_jobs=3)]: Done  21 out of  21 | elapsed:  1.9min finished

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
Beginning Doc2Vec-Gender binary classification fit.
Fitting 3 folds for each of 7 candidates, totalling 21 fits

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
[Parallel(n\_jobs=3)]: Done  21 out of  21 | elapsed:   17.9s finished

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
Beginning CSR-Sign classification fit.
Fitting 3 folds for each of 7 candidates, totalling 21 fits

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
[Parallel(n\_jobs=3)]: Done  21 out of  21 | elapsed: 20.3min finished

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
Beginning Doc2Vec-Sign classification fit.
Fitting 3 folds for each of 7 candidates, totalling 21 fits

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
[Parallel(n\_jobs=3)]: Done  21 out of  21 | elapsed: 22.2min finished

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
Wall time: 1h 35s

    \end{Verbatim}

    We can quickly compute the approximate F1 scores for randomly guessing
by randomly permuting the target classification variables and
calculating the F1 scores from treating these permutations as our
predictions.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}55}]:} \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{preprocessing} \PY{k}{import} \PY{n}{LabelEncoder}
         
         \PY{n}{genders\PYZus{}} \PY{o}{=} \PY{n}{LabelEncoder}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{fit\PYZus{}transform}\PY{p}{(}\PY{n}{genders}\PY{p}{)}
         \PY{n}{signs\PYZus{}} \PY{o}{=} \PY{n}{LabelEncoder}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{fit\PYZus{}transform}\PY{p}{(}\PY{n}{signs}\PY{p}{)}
         
         \PY{n}{genders\PYZus{}f1\PYZus{}chance} \PY{o}{=} \PY{n}{f1\PYZus{}score}\PY{p}{(}
             \PY{n}{genders\PYZus{}}\PY{p}{,} 
             \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{permutation}\PY{p}{(}\PY{n}{genders\PYZus{}}\PY{p}{)}\PY{p}{,}
             \PY{n}{average}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{macro}\PY{l+s+s2}{\PYZdq{}}
         \PY{p}{)}
         \PY{n}{signs\PYZus{}f1\PYZus{}chance} \PY{o}{=} \PY{n}{f1\PYZus{}score}\PY{p}{(}
             \PY{n}{signs\PYZus{}}\PY{p}{,}
             \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{permutation}\PY{p}{(}\PY{n}{signs\PYZus{}}\PY{p}{)}\PY{p}{,}
             \PY{n}{average}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{macro}\PY{l+s+s2}{\PYZdq{}}
         \PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}56}]:} \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{CSR\PYZhy{}Age model:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Best R\PYZca{}2 score:  }\PY{l+s+si}{\PYZob{}age\PYZus{}bow[0]\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Best parameters: }\PY{l+s+si}{\PYZob{}age\PYZus{}bow[1]\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{Doc2Vec\PYZhy{}Age model:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Best R\PYZca{}2 score:  }\PY{l+s+si}{\PYZob{}age\PYZus{}d2v[0]\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Best parameters: }\PY{l+s+si}{\PYZob{}age\PYZus{}d2v[1]\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{CSR\PYZhy{}Gender model:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Best F1 score:   }\PY{l+s+si}{\PYZob{}gender\PYZus{}bow[0]\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Chance F1 score: }\PY{l+s+si}{\PYZob{}genders\PYZus{}f1\PYZus{}chance\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Best parameters: }\PY{l+s+si}{\PYZob{}gender\PYZus{}bow[1]\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{Doc2Vec\PYZhy{}Gender model:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Best F1 score:   }\PY{l+s+si}{\PYZob{}gender\PYZus{}d2v[0]\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Chance F1 score: }\PY{l+s+si}{\PYZob{}genders\PYZus{}f1\PYZus{}chance\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Best parameters: }\PY{l+s+si}{\PYZob{}gender\PYZus{}d2v[1]\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{CSR\PYZhy{}Astrological Sign model:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Best F1 score:   }\PY{l+s+si}{\PYZob{}sign\PYZus{}bow[0]\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Chance F1 score: }\PY{l+s+si}{\PYZob{}signs\PYZus{}f1\PYZus{}chance\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Best parameters: }\PY{l+s+si}{\PYZob{}sign\PYZus{}bow[1]\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{Doc2Vec\PYZhy{}Astrological Sign model:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Best F1 score:   }\PY{l+s+si}{\PYZob{}sign\PYZus{}d2v[0]\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Chance F1 score: }\PY{l+s+si}{\PYZob{}signs\PYZus{}f1\PYZus{}chance\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Best parameters: }\PY{l+s+si}{\PYZob{}sign\PYZus{}d2v[1]\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
CSR-Age model:
Best R\^{}2 score:  0.047575101844957314
Best parameters: \{'C': 0.1\}

Doc2Vec-Age model:
Best R\^{}2 score:  0.18963770676288672
Best parameters: \{'C': 10.0\}

CSR-Gender model:
Best F1 score:   0.7567812859164255
Chance F1 score: 0.4988612836438923
Best parameters: \{'C': 0.01\}

Doc2Vec-Gender model:
Best F1 score:   0.6401506929615315
Chance F1 score: 0.4988612836438923
Best parameters: \{'C': 0.1\}

CSR-Astrological Sign model:
Best F1 score:   0.08757137308243307
Chance F1 score: 0.08286388077593226
Best parameters: \{'C': 10.0\}

Doc2Vec-Astrological Sign model:
Best F1 score:   0.08716301883213448
Chance F1 score: 0.08286388077593226
Best parameters: \{'C': 10.0\}

    \end{Verbatim}

    Now, for kicks, let's open up one of the models--the age-bow model--and
see what it learned. SVMs are linear models, so we can just look at the
coefficient weights.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}62}]:} \PY{n}{age\PYZus{}bow} \PY{o}{=} \PY{n}{joblib}\PY{o}{.}\PY{n}{load}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{model files/age\PYZus{}bow.pkl}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n}{coefs} \PY{o}{=} \PY{n}{age\PYZus{}bow}\PY{o}{.}\PY{n}{coef\PYZus{}}
         \PY{c+c1}{\PYZsh{} Zip up the coefficients with the actual text}
         \PY{c+c1}{\PYZsh{} of the tokens, for human interpretability.}
         \PY{n}{id2word} \PY{o}{=} \PY{n}{Dictionary}\PY{o}{.}\PY{n}{load}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{corpus data files/author\PYZus{}id2word}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n}{coefs} \PY{o}{=} \PY{p}{[}\PY{p}{(}\PY{n}{id2word}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{,} \PY{n}{coefs}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{)} \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{coefs}\PY{p}{)}\PY{p}{)}\PY{p}{]}
         \PY{n}{coefs} \PY{o}{=} \PY{n+nb}{sorted}\PY{p}{(}\PY{n}{coefs}\PY{p}{,} \PY{n}{key}\PY{o}{=}\PY{k}{lambda} \PY{n}{x}\PY{p}{:} \PY{n+nb}{abs}\PY{p}{(}\PY{n}{x}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}\PY{p}{,} \PY{n}{reverse}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
         \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n}{coefs}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{25}\PY{p}{]}\PY{p}{:}
             \PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+si}{\PYZob{}i[0]:\PYZlt{}25s\PYZcb{}}\PY{l+s+s2}{ }\PY{l+s+si}{\PYZob{}i[1]\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
xanga                     -1.9668928332057503
boredom                   -1.8040857581198255
bore                      -1.7574341127238464
common\_test               -1.5383788467231674
sec                       -1.452939663775986
bestest\_friend            -1.4480233805637324
saw\_shrek                 -1.3607749270471736
xanga\_com                 -1.3518653749844385
dad                       -1.318006354165074
daughter                  1.2881148284677584
skateboard                -1.2838153196574003
local                     1.2572066491487446
summer\_vacat              -1.2390704503589176
awesom                    -1.2380422349340612
sian                      -1.2335334775650209
best\_friend               -1.2067217024609447
sunburn                   -1.2061339708563275
ti                        -1.2037574041500079
recess                    -1.1926773118455363
newborn                   1.1726798398733633
prefect                   -1.1691740438884966
dislik                    -1.1669345610542374
immatur                   -1.147140623661155
song\_lyric                -1.1452647578274544
tml                       -1.1316503417267156

    \end{Verbatim}


    % Add a bibliography block to the postdoc
    
    
    
    \end{document}
