{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python and NLP for Linguists\n",
    "I'm going to try something novel: giving this talk from a Jupyter notebook so I can run code on the fly.\n",
    "\n",
    "While I was writing up this notebook, this talk turned into a broad introduction to NLP as well as Python.  Oh well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Who is this guy?\n",
    "* Henry Anderson ([henry.anderson@uta.edu](mailto:henry.anderson@uta.edu))\n",
    "* Data Scientist in the University Analytics department\n",
    "* Specialist in unstructured data (i.e., text), machine learning, and Natural Language Processing\n",
    "* First year Linguistics masters student, with interests in computational social science, digital language use, and the language of online communities and networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "* Who stands to gain the most\n",
    "* Why consider _programming,_ generally?\n",
    "* Why consider _Python,_ specifically?\n",
    "* Some demos:\n",
    "  * Custom concordance code, with massive flexibility\n",
    "  * Quick n-gram analysis\n",
    "  * Automatic dependency parsing, POS-tagging, lemmatization, tokenization, etc. (i.e., text preprocessing)\n",
    "  * Topic models\n",
    "  * Word vectors\n",
    "  * Classification and regression tasks with text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goals of this talk\n",
    "* Put some basic computational/programming tools on your radar.\n",
    "* Give a sense of what _type_ of work can be done with these tools.\n",
    "* Make you generally aware of the scope and nature of computational tools.\n",
    "* Give some _very_ basic exposure to Python.\n",
    "  * We'll walk through some very basic code examples, but we'll skim over most of them.\n",
    "\n",
    "## This talk is _not..._\n",
    "* A tutorial on Python, the dataset, or the libraries.\n",
    "  * That can come later, if people are interested.\n",
    "* A tutorial in natural language processing, text processing, or big data.\n",
    "* Really a tutorial in anything."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Who stands to gain\n",
    "* Anyone who deals with _data:_ people interested in corpus work, sociolinguistics, natural language processing, digital/online language, etc.\n",
    "* Anyone interested in _computational social science_ (CSS): i.e. general social science approaches leveraging large datasets and computational horsepower.\n",
    "  * CSS is currently exploding, and is a hugely important avenue for applied social science research.\n",
    "  * CSS is also massively interdisciplinary: programming, statistics, machine learning, AI, network analysis, linguistics, sociology, psychology, etc all combine to make CSS happen.\n",
    "* If you deal mostly with theory, or are primarily an experimentalist, you may stand to gain less from this talk.  (But you're still welcome!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What does programming offer?\n",
    "* (Quite literally) infinite control over your data processing: you're not limited by the features someone else decided to code into their program--you can change your code up to do anything you want.\n",
    "\n",
    "* Scalability and automation of your data work\n",
    "  * Work with literally millions of documents and billions of words with relative ease.\n",
    "  * Automate steps from data collection through final analysis.\n",
    "\n",
    "* Some types of analysis simply are infeasible to do by hand--network analysis, topic modeling, word embeddings, etc--and _have_ to be done computationally.\n",
    "\n",
    "* Marketable skills: even a little bit of Python, Java, or any other language can open doors in the job market.\n",
    "\n",
    "* You'll feel like a badass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What does _Python_ offer?\n",
    "* Free (as in speech, not beer.  But also as in beer), open-source, royalty-free.  No licenses to sign, no royalties to pay, and _essentially no restrictions_ on what you can and can't do with it.  (the [Python Software Foundation license](https://docs.python.org/3/license.html) is an extremely permissive BSD-type license)\n",
    "\n",
    "* Easy-to-learn language.\n",
    "  * Very user-friendly language with very friendly users.\n",
    "  * Great documentation and stupid amounts of free, high-quality learning resources.\n",
    "  * Among its core ideas:\n",
    "    * Code is read far more than it is written, so the language should be _human-readable._\n",
    "    * \"There should be one, and preferably only one, obvious way to do it.\"  I.e., the most straightforward approach is _usually_ the best.  (This results in a lot of people writing straightforward, fairly easy-to-follow code).\n",
    "  * Commonly taught as a first programming language, so there are LOTS of materials for eveyone from beginning programmers to seasoned professionals; the Python community is also very welcoming of newcomers.\n",
    "\n",
    "* General purpose language: can do (almost) everything you want to make it to.\n",
    "  * Compare to R, which is great for statistics, and a pain for a lot of other stuff.\n",
    "  * Or Matlab, which is great for being a broken, slow, difficult software environment, and isn't so good at being, well, good.\n",
    "    * (this has been your mandatory \"Matlab is bad\" comment)\n",
    "\n",
    "* Rapidly becoming _the_ language for data science, displacing even R in most applications.  (R is still dominant for raw statistics, though Python has plenty of packages that implement common statistical tests).\n",
    "  * Though, keep an eye on a different language--Julia--over the next few years.  It is truly a worthy contender, but has yet to hit version 1.0 as of this talk.\n",
    "\n",
    "* **For linguists**: a _huge_ array of language processing functionality and libraries.\n",
    "  * [spaCy](https://spacy.io), basically a Python version of Stanford's CoreNLP toolkit (lemmatization, tokenization, dependency parsing, POS tagging, and more).\n",
    "  * [Gensim](https://radimrehurek.com/gensim/), full of topic models and pretty bleeding-edge NLP tools.\n",
    "  * [Natural Language Toolkit (NLTK)](http://www.nltk.org/), a _massive_ library that's designed to teach a lot of NLP concepts (but can be used for some serious production work too).\n",
    "  * [Pandas](http://pandas.pydata.org/) for R-like dataframes, statistics, and general tabular data management.\n",
    "  * [Matplotlib](https://matplotlib.org/) (and others like [Seaborn](https://seaborn.pydata.org/), [PyGal](http://www.pygal.org/en/stable/), [Bokeh](https://bokeh.pydata.org/en/latest/), ...) for high-quality, powerful data visualization.\n",
    "  * [scikit-learn](http://scikit-learn.org/stable/index.html) for non-neural machine learning (support vector machines, random forests, and a few text features like basic preprocessing)\n",
    "    * Side note, the scikit-learn [User Guides](http://scikit-learn.org/stable/user_guide.html) are an _excellent_ technical crash course in machine learning, even if you're not too interested in Python.\n",
    "  * [Networkx](https://networkx.github.io/) for performing network analysis.\n",
    "  * [Tensorflow](https://www.tensorflow.org/)+[Keras](https://keras.io/), for quickly and easily building neural networks.\n",
    "  * [PyTorch](http://pytorch.org/), an up-and-coming (but extremely exciting) neural network library.\n",
    "  * And dozens more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some links\n",
    "\n",
    "* [Python.org](https://www.python.org/), the official Python website.\n",
    "  * [Python Standard Library reference](https://docs.python.org/3/library/index.html)\n",
    "  * [Python's official, basic tutorial](https://docs.python.org/3/tutorial/index.html).  It's not the best out there, but it's pretty good, and pretty fast.\n",
    "* [PyCharm](https://www.jetbrains.com/pycharm/), an excellent Python editor (download the community edition, not the professional edition--it's free and still has more features than most people need)\n",
    "* [Automate the Boring Stuff with Python](https://automatetheboringstuff.com/), a free (Creative Commons-licensed) eBook that introduces Python via concrete, hands-on examples and projects.\n",
    "* [r/learnpython](https://www.reddit.com/r/learnpython/), a subreddit dedicated to people learning Python and asking questions about the language.\n",
    "* [Project Euler](https://projecteuler.net/archives), a collection of math problems designed to be used as programming practice (in any language).\n",
    "* [Rosalind](http://rosalind.info/problems/locations/), another collection of practice problems, but geared at genetics and bioinformatics.\n",
    "* PyCon's YouTube channels ([2017](https://www.youtube.com/channel/UCrJhliKNQ8g0qoE_zvL8eVg), [2016](https://www.youtube.com/channel/UCwTD5zJbsQGJN75MwbykYNw), etc) have a lot of good videos, including some long-form tutorials and workshops.\n",
    "* [PyData](https://www.youtube.com/user/PyDataTV), a data science themed Python conference/convention, posts almost all of their talks to YouTube."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running this notebook\n",
    "\n",
    "To run this notebook, you will need to install the following Python libraries:\n",
    "* Jupyter (to run/view the notebook itself)\n",
    "* Gensim\n",
    "* spaCy\n",
    "  * You'll need to download two of spaCy's language models: `en_core_web_sm` and `en_core_web_lg`.  Installation instructions are [here.](https://spacy.io/models/)\n",
    "* Numpy\n",
    "* Scikit-learn (goes by sklearn when installing)\n",
    "* Matplotlib\n",
    "* Natural Language Toolkit (NLTK)\n",
    "* tqdm\n",
    "* Pandas\n",
    "\n",
    "For the first part of the talk, you'll also need to download `glen carrig.txt` from the Github repository and have it in the same folder as this notebook.\n",
    "\n",
    "For the second part of the talk, you'll need to download the [Blog Authorship Corpus](http://u.cs.biu.ac.il/~koppel/BlogCorpus.htm) and unzip its files into a folder named \"blogs\" (case-sensitive) in the same folder as this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some basic demos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, some necessary setup--we just need to have this program create some folders to save stuff into."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.isdir(\"corpus data files\"):\n",
    "    os.mkdir(\"corpus data files\")\n",
    "if not os.path.isdir(\"topic models\"):\n",
    "    os.mkdir(\"topic models\")\n",
    "if not os.path.isdir(\"model files\"):\n",
    "    os.mkdir(\"model files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concordances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concordances can be done with regular expressions and a teeny tiny bit of legwork.  (By the way: if you're working with text, you have no excuse to not learn regular expressions.  But that would be another talk all unto itself).  We'll work with the text of William Hope Hodgeson's novel [_The Boats of the \"Glen Carrig\"_](https://en.wikipedia.org/wiki/The_Boats_of_the_%22Glen_Carrig%22), a 1907 horror novel.  The text was taken [from Project Gutenberg](http://www.gutenberg.org/ebooks/10542), with the site's boilerplate text removed from the front and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import re\n",
    "\n",
    "def concordance(text, token, window=50):\n",
    "    pattern = re.compile(r\"\\b{}\\b\".format(token.strip()), re.IGNORECASE)\n",
    "    # convert all whitespaces to single space characters\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    for i in pattern.finditer(text):\n",
    "        print(\n",
    "            \"...\",\n",
    "            text[i.start() - window:i.start()].rjust(window, \" \"),\n",
    "            text[i.start():i.start() + window].ljust(window, \" \"),\n",
    "            \"...\",\n",
    "            sep=\"\"\n",
    "        )\n",
    "\n",
    "glen_carrig = open(\"glen carrig.txt\", \"r\", encoding=\"utf8\").read()\n",
    "concordance(glen_carrig, \"think\", window=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if we want to get _really_ clever, we can have our concordance function search by stemmed forms.  We'll revisit stemming in a bit more detail shortly; for now, just know that stemming is the process of determining an uninflected form of words, but it's based purely on character patterns--so each word is treated completely in isolation, with no information about parts of speech.\n",
    "\n",
    "We need to stem the original text, then search for concordances of any tokens that get stemmed to the same value as our input.  Then we run the previous concordance function on those tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import re\n",
    "from gensim.parsing.preprocessing import stem_text\n",
    "\n",
    "def stem_concordance(text, token, window=50):\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    # get a unique list of all word-like tokens using a basic regex\n",
    "    word_finder = re.compile(r\"[A-z0-9]+\", re.MULTILINE)\n",
    "    vocab_to_stem = {\n",
    "        i.lower():stem_text(i)\n",
    "        for i in set(word_finder.findall(text))\n",
    "    }\n",
    "    if token.lower().strip() not in vocab_to_stem:\n",
    "        print(\"Token is not in the vocabulary.  Please try again.\")\n",
    "        return\n",
    "    # now flip the dict to {stemmed_form:{set of unstemmed form}}\n",
    "    stem_to_vocab = {i:set() for i in vocab_to_stem.values()}\n",
    "    for i in vocab_to_stem:\n",
    "        stem_to_vocab[vocab_to_stem[i]].add(i.lower())\n",
    "    # look up other tokens that have same stem as input token\n",
    "    stemmed_token = vocab_to_stem[token]\n",
    "    possible_forms = stem_to_vocab[stemmed_token]\n",
    "    \n",
    "    # and now run the previous concordance function.\n",
    "    for i in possible_forms:\n",
    "        concordance(text, i, window=window)\n",
    "\n",
    "glen_carrig = open(\"glen carrig.txt\", \"r\", encoding=\"utf8\").read()\n",
    "stem_concordance(glen_carrig, \"think\", window=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That added less than a second to the total runtime.  Nice.\n",
    "\n",
    "Now let's do it again, but with spaCy instead of Gensim.  spaCy has built-in tokenization, lemmatization, and more that are all based on large, pre-trained machine learning models.  This will give us much better accuracy--both with tokenizing and lemmatizing--but at the cost of higher runtime.  spaCy also has multiple models to choose from--for English, there's small, medium, and large.  The bigger the model, the better its accuracy, but also the slower it runs.  But since the interface is exactly the same, we'll use the small model for speed.  (the small model is pretty darn good, anyways)\n",
    "\n",
    "We'll also revisit lemmatization in a bit more detail shortly.  The short version: it's like stemming, but it returns a valid, real word corresponding to the uninflected form of a token.  (unlike stemming, which might map \"today\" to the root form \"todai\"--lemmatization would correctly map this to \"today\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import re\n",
    "import spacy\n",
    "\n",
    "def lemma_concordance(text, token, window=50):\n",
    "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\", \"parser\", \"ner\"])\n",
    "    \n",
    "    # directly get the mapping of raw form to lemma from spaCy's \n",
    "    # tokenization/stemming\n",
    "    word_finder = re.compile(r\"[A-z0-9]+\", re.MULTILINE)\n",
    "    vocab_to_stem = {\n",
    "        i.lower_:i.lemma_\n",
    "        for i in nlp(text)\n",
    "    }\n",
    "    if token.lower().strip() not in vocab_to_stem:\n",
    "        print(\"Token is not in the vocabulary.  Please try again.\")\n",
    "        return\n",
    "    # now flip the dict to {stemmed_form:{set of unstemmed form}}\n",
    "    stem_to_vocab = {i:set() for i in vocab_to_stem.values()}\n",
    "    for i in vocab_to_stem:\n",
    "        stem_to_vocab[vocab_to_stem[i]].add(i.lower())\n",
    "    \n",
    "    # Now get the stemmed form of the input token and look up\n",
    "    # the list of possible unstemmed forms--this approximates\n",
    "    # finding other inflected forms of the same word.\n",
    "    stemmed_token = vocab_to_stem[token]\n",
    "    possible_forms = stem_to_vocab[stemmed_token]\n",
    "    \n",
    "    # and now run the previous concordance function.\n",
    "    for i in possible_forms:\n",
    "        concordance(text, i, window=window)\n",
    "\n",
    "glen_carrig = open(\"glen carrig.txt\", \"r\", encoding=\"utf8\").read()\n",
    "lemma_concordance(glen_carrig, \"think\", window=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-gram frequencies\n",
    "\n",
    "Python has a number of ways we could find n-grams.  The first is using a pre-built tool, like NLTK's ngrams() function, or a Phrases()/Phraser() combination from Gensim, which are actually used to find multi-word phrases.  Or, we could hack it together ourselves with a few lines of code.\n",
    "\n",
    "First, let's hack it together ourselves.  Then we'll print out the top most common N-grams and plot the frequencies by rank (on a logarithmic scale, naturally.  We're not monsters, after all).  We'll use spaCy's tokenization for maximum accuracy.\n",
    "\n",
    "First, we'll tokenize our document and convert everything to lowercase using spaCy.  We'll also remove punctuation and stopwords while we're at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%matplotlib inline\n",
    "\n",
    "from collections import Counter\n",
    "from pprint import pprint\n",
    "import re\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "\n",
    "# Change this number manually to change the N in the ngrams\n",
    "NGRAM_N = 2\n",
    "\n",
    "# Preprocess the document\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\", \"parser\", \"ner\"])\n",
    "doc = open(\"glen carrig.txt\", \"r\", encoding=\"utf8\").read()\n",
    "doc = re.sub(r\"\\s+\", \" \", doc)\n",
    "doc = [\n",
    "    i.lower_ \n",
    "    for i in nlp(doc)\n",
    "    if not i.is_punct\n",
    "    and not i.is_stop\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll find n-grams by explicitly coding an n-gram finding bit of Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%matplotlib inline\n",
    "\n",
    "# change the default matplotlib figure size for Jupyter's sake\n",
    "mpl.rcParams[\"figure.figsize\"] = (10,10)\n",
    "\n",
    "# Find the n-grams with some manual code\n",
    "n_grams = [\n",
    "    \"_\".join(doc[i:i+NGRAM_N]) \n",
    "    for i in range(0, len(doc) - NGRAM_N)\n",
    "]\n",
    "n_grams = Counter(n_grams)\n",
    "\n",
    "# do some prettier formatting than the default printing\n",
    "print(f\"{'NGRAM':<30s}\\tCOUNT\")\n",
    "for i in n_grams.most_common(20):\n",
    "    print(f\"{i[0]:<30s}\\t{i[1]}\")\n",
    "    \n",
    "# Now, let's just plot the counts by rank.\n",
    "counts = sorted(n_grams.values(), reverse=True)\n",
    "plt.plot(counts)\n",
    "plt.yscale(\"log\")\n",
    "plt.xscale(\"log\")\n",
    "plt.title(\"Look at this beautiful Zipf distribution!\")\n",
    "plt.xlabel(\"Rank (log)\")\n",
    "plt.ylabel(\"Count (log)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK's ngrams() function will find ngrams for us, like we just did by hand.  It'll be more readable, but function exactly the same, and run almost exactly as far--so in general, this method might be preferable for most people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%matplotlib inline\n",
    "\n",
    "from nltk import ngrams\n",
    "\n",
    "# change the default matplotlib figure size for Jupyter's sake\n",
    "mpl.rcParams[\"figure.figsize\"] = (10,10)\n",
    "\n",
    "# Find n-grams\n",
    "n_grams = ngrams(doc, NGRAM_N)\n",
    "n_grams = Counter(n_grams)\n",
    "\n",
    "# do some prettier formatting than the default printing\n",
    "print(f\"{'NGRAM':<30s}\\tCOUNT\")\n",
    "for i in n_grams.most_common(20):\n",
    "    # okay, so the format string changes a little too\n",
    "    print(f\"{'_'.join(i[0]):<30s}\\t{i[1]}\")\n",
    "    \n",
    "# Now, let's just plot the counts by rank.\n",
    "counts = sorted(n_grams.values(), reverse=True)\n",
    "plt.plot(counts)\n",
    "plt.yscale(\"log\")\n",
    "plt.xscale(\"log\")\n",
    "plt.title(\"Look at this beautiful Zipf distribution!\")\n",
    "plt.xlabel(\"Rank (log)\")\n",
    "plt.ylabel(\"Count (log)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phrase-finding/collocation analysis\n",
    "\n",
    "Ngrams are fun and all, but what if you want to find multi-word phrases that appear more often than they should by random chance alone (i.e., collocates)?  Well, as before, we could hack a bit of code together, or we could use a pre-built tool from the amazing Gensin library: the [Phrasing tools!](https://radimrehurek.com/gensim/models/phrases.html)  These tools let you scan a(n already processed) corpus of texts, and finds bigrams that are collocated more than the raw prior distributions would indicate.  Then, these tools let you transform your original corpus, replacing these bigrams with a single token.  You can repeat this process all you want to find arbitrarily long phrases.\n",
    "\n",
    "Before doing this, we should run our text through a basic preprocessing pipeline in Gensim.  We'll revisit this in a bit more detail later to talk about what it does; for now, just know that it automates a lot of the basic preprocessing steps for us, like lowercasing, removing stopwords, stemming, etc.\n",
    "\n",
    "We'll use all the default values for our phrasing models except for the threshold (to guarnatee we find at least _some_ phrases for this demo), but they'll be provided explicitly to show how much customization there is.  Note that the phraser expectes a _list of sentences_, i.e. a _list of lists of words._  We don't strictly need to make them the actual sentences; the only reason Gensim says to use sentences is to avoid collocation across sentence boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from gensim.models.phrases import Phrases\n",
    "from gensim.parsing.preprocessing import preprocess_string\n",
    "\n",
    "doc = open(\"glen carrig.txt\", \"r\", encoding=\"utf8\").read()\n",
    "# clean up line breaks and other whitespace issues\n",
    "doc = re.sub(r\"\\s+\", \" \", doc)\n",
    "doc = preprocess_string(doc)\n",
    "phrasing = Phrases(\n",
    "    [doc], # phrases() expects a list of tokenized sentences/documents\n",
    "    min_count=5,\n",
    "    threshold=10,\n",
    "    max_vocab_size=40000000,\n",
    "    delimiter=b\"_\", # this has to be a byte string--just a quirk of this model\n",
    "    progress_per=10000,\n",
    "    scoring=\"default\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can look at some of the phrases that our phrase model discovered.  This will print out the phrases in the order they're found in the text, so we might see duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "found_phrases = list(phrasing.export_phrases([doc]))\n",
    "pprint(found_phrases[:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, we can transform our original document(s), replacing all of the discovered bigrams with a single token (e.g., `[\"the\", boat\"]` --> `\"the_boat\"`).  Gensim likes to use the indexing syntax to do transformations--it's a bit weird but you get used to it.\n",
    "\n",
    "Note that we'll get a warning from Gensim (warnings are not errors--they're more of a \"heads up, something looks weird here\" sort of notice).  Gensim also has Phraser() objects, which are initialized from a Phrases() object, and are much faster at transforming a corpus.  This only really matters when you're dealing with _massive_ corpora and datasets; for our single book, we don't really need to bother, but I'll show how it would be done anyways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phraser\n",
    "\n",
    "# transform the text with the original phrases object...\n",
    "phrased_doc = phrasing[doc]\n",
    "print(phrased_doc[500:550])\n",
    "\n",
    "# ...or by creating a new Phraser() from it first.\n",
    "phraser = Phraser(phrasing)\n",
    "phrased_doc = phraser[doc]\n",
    "print('\\n\\n', phrased_doc[500:550])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(as you can see, the results of Phrases() and Phraser() are the same--Phraser() will just be _much_ faster, and use much less memory, for very large phrasing passes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some more fun demos: Natural Language Processing 101\n",
    "\n",
    "Now, let's do some more interesting demos.  These will focus on more sophisticated (but still \"entry-level\") NLP techniques and tools.\n",
    "\n",
    "We will:\n",
    "* Go from raw data to cleaned, workable text.\n",
    "* Look at some of spaCy and Gensim's text (pre)processing tools\n",
    "* Do some basic topic modeling with Latent Dirichlet Allocation\n",
    "* Do some basic classification and regression tasks with some text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting our data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous demos were very simple (even simplistic) and don't really leverage all the cool stuff Python--or programming in general--can do for you.  Let's work with a non-trivial dataset now and do some more NLP-like work.  We'll use the [Blog Authorship Corpus](http://u.cs.biu.ac.il/~koppel/BlogCorpus.htm).  Download it to the same folder as this notebook, then unzip it into a folder names \"blogs\" (case-sensitive!).\n",
    "\n",
    "Each author's blog posts are stored as a .xml file, named in the following format: \n",
    "\n",
    "`[ID number].[gender].[age].[industry of employment].[astrological sign].xml`\n",
    "\n",
    "E.g., `11253.male.26.Technology.Aquarius.xml`\n",
    "\n",
    "And they contain blog data that looks like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<Blog>\n",
    "<date>20,July,2004</date>\n",
    "<post>\n",
    "\n",
    "     \n",
    "      About to go t bed late (again) got sucked into (another) late night film. Tonight was  urlLink Maybe Baby . It was really good made me think, but not about babies. The guy screws up his marriage and it made me think about making sure, everyday, that mine is tip top. If I'm honest there are areas that we are just getting by in - so I need to resolve to sort them out now before they are a problem. In the film they both keep diaries so I thought I should blog tonight.&nbsp;   Weekend was hectic but great fun. Not that long ago k and I had to work on spending time with other people as a couple. This weekend we never ate alone, except breakfast.&nbsp; K, P and I went for a very breif trip on the river saturday durring a gap in the weather. K stripped off and went for a swim, I love her so much.&nbsp;   Tonight we went out for dinner. It was a lovely evening, the first in weeks, so we ate at the Bridge and sat outside, next to the  urlLink river .     \n",
    "     \n",
    "    \n",
    "</post>\n",
    "</Blog>```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First thing's first: we need to deal with the XML formatting.  Fortunately, Python has some excellent tools for that, e.g. `xml.etree.ElementTree`.  We'll also want to use a better data structure to represent this text.  There's an absolutely indispensible library called Pandas, which gives you R-style dataframes to work with (and Pandas is probably the single biggest reason that Python has taken over the data science world, demoting R to second-place)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "files = [i.path for i in os.scandir(\"blogs\")]\n",
    "print(files[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a first pass where we just try to parse each file, to make sure there are no problems.  (Data validations is a step you _absolutely do not skip_ if you're doing any real data work, after all!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xml.etree import ElementTree\n",
    "\n",
    "# sorted() just makes sure the files are always in the same order\n",
    "# for the demos\n",
    "for i in sorted(files):\n",
    "    try:\n",
    "        ElementTree.parse(i)\n",
    "    except Exception as e:\n",
    "        print(\"\\nEXCEPTION ON FILE:\", i)\n",
    "        print(\"EXCEPTION:\", e)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uh-oh.  Let's take a look at the file that broke and go see where the problem is.  Fortunately, the error above gave us a line _and_ a column number, so we can go to the exact location where an issue was encountered.\n",
    "\n",
    "```\n",
    "<Blog>\n",
    "[...]\n",
    "\n",
    "<date>14,January,2003</date>\n",
    "<post>\n",
    "\n",
    "\n",
    "      Hehe, just finished dinner! Yum! I'm so happy right now. I don't even know why, I just...am! I'm talking to a bunch of my friends while writing this, which is always fun. Plus I'm doing homework, PLUS I'm watching Law & Order...how massively talented am I? Well, my day was pretty kick butt. Umm, no band OR music theory...very cool, so Kristen and I sat together and did homework and discussed Winter. Next period I hung out with Chris and Kelly for a bit (Alex too for a bit, since he was in Gym) and I left eventually. They started working on music, and...I just always end up feeling left out when they do, so, I try to stay away from those two together in general.Oh well, I went and sat alone in a practice room. Darn...no good stories for the newspaper to write about me. That was a great story, no matter how angry people are about the band comment. I wish people would have read the story actually, instead of reaching paragraph two and deciding it was terrible. Well, anyway, umm...EPVM was interesting. I'm getting nervous about my final. My brother said it was difficult...my brother with the perfect ACT AND SAT scores...arg. I just...wow, I'm so afraid of that test. I'm completely going to fail. Gym...oh jeez...two words  Commando Crawl  OWWWWWWWWWWWWWW  Good lord...that was one of the most painful...oh jeez...just thinking about it. Seriously, if you ever by some chance do that for high ropes...WEAR PANTS! Well, yes...wear pants normally, but don't wear shorts, make sure they are pants, because...it's quite painful if you don't. I made it through though, so it's all good! It just hurt...a lot.   Math, boring, shock shock.  Intervening thought: Why do I always write these while talking to Alex??  Lunch...was interesting. Just hung out with Kelly and Emily some, then Chris and Alex. It was cool...not much to it. Comparative Religion was boring...just, meh, presenting projects...and finally chem...boring, shock shock, and then, I came home, and did stuff, and that is the end of my day...therefore, I shall leave, and go kill alex...I mean...wait...you know NOTHING. You have no evidence...:)\n",
    "\n",
    "</post>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Column numbers aren't displayed in this notebook, but the error is at the ampersand in `Law & Order`.  As it turns out, ampersands are special characters in XML code, and need to be escaped specially (in this case, as `&amp;`).  We could do some manual replacement of characters with the appropriate XML escapes, but that sounds like a lot of work and a lot of room for error.\n",
    "\n",
    "Fortunately, our document structure is so simple that we can just hack this together with regular expressions.  This will be fast, but **is not** how we should in general deal with problematic XML or other markup files--we'd want to use some of Python's more rudimentary tools, like the base HTML or XML parsers, and overwrite their functionality (e.g. by subclassing).\n",
    "\n",
    "We'll create a list of dictionaries (think JSONs), which we can easily pass into Pandas to make a big, beautiful, glorious Dataframe object (which we'll save to a .csv so we can re-open it directly later).  We'll work with this Dataframe for most the rest of this demo.\n",
    "\n",
    "We'll also create a second dataframe, where we only have one entry per author, and concatenate all of their posts together.  We'll use this at the end of the talk for some regression and classification tasks where we're predicting author-level variables, rather than working at the document level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import os\n",
    "import re\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# pre-compile patterns we'll use a lot--for speed\n",
    "date_finder = re.compile(r\"^<date>(.*?)</date>$\", re.MULTILINE)\n",
    "post_finder = re.compile(r\"^<post>$(.*?)^</post>$\", re.MULTILINE|re.DOTALL)\n",
    "whitespace_cleaner = re.compile(r\"\\s+\", re.MULTILINE)\n",
    "\n",
    "def process_file(infile, by_author=False):\n",
    "    # get the user metadata from the filename\n",
    "    metadata = os.path.split(infile)[-1].split(\".\")\n",
    "    text = open(infile, \"r\", encoding=\"ISO-8859-1\").read()\n",
    "    \n",
    "    # Extract date and post content from the file\n",
    "    dates = date_finder.findall(text)\n",
    "    posts = post_finder.findall(text)\n",
    "    \n",
    "    # assert check will crash our program if it fails--\n",
    "    # this make sure our approach works.\n",
    "    assert len(dates) == len(posts)\n",
    "    if by_author == False:\n",
    "        blog_data = [\n",
    "            {\n",
    "                 \"Author ID\" : metadata[0],\n",
    "                 \"Gender\"    : metadata[1],\n",
    "                 \"Age\"       : int(metadata[2]),\n",
    "                 \"Industry\"  : metadata[3],\n",
    "                 \"Sign\"      : metadata[4],\n",
    "                 \"Date\"      : dates[i],\n",
    "                 \"Post\"      : whitespace_cleaner.sub(\" \", posts[i])\n",
    "            }\n",
    "            for i in range(len(dates))\n",
    "        ]\n",
    "    elif by_author == True:\n",
    "        posts = \" \".join(\n",
    "            whitespace_cleaner.sub(\" \", i)\n",
    "            for i in posts\n",
    "        )\n",
    "        blog_data = {\n",
    "             \"Author ID\" : metadata[0],\n",
    "             \"Gender\"    : metadata[1],\n",
    "             \"Age\"       : int(metadata[2]),\n",
    "             \"Industry\"  : metadata[3],\n",
    "             \"Sign\"      : metadata[4],\n",
    "             \"Posts\"     : posts\n",
    "        }\n",
    "    \n",
    "    return blog_data\n",
    "\n",
    "blog_dataframe = pd.concat(\n",
    "    pd.DataFrame(process_file(i, by_author=False))\n",
    "    for i in tqdm(files, desc=\"Generating Dataframe\")\n",
    ")\n",
    "print(\"Casting Date column to datetime format.\")\n",
    "print(\"Saving blog dataframe to Blog Data.csv.\")\n",
    "blog_dataframe.to_csv(\"corpus data files/Blog Data.csv\", index=False)\n",
    "print(blog_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, the by-author dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_dataframe = pd.DataFrame([\n",
    "    process_file(i, by_author=True) \n",
    "    for i in tqdm(files, desc=\"Generating Dataframe\")\n",
    "])\n",
    "print(\"Saving author dataframe to Author Data.csv.\")\n",
    "author_dataframe.to_csv(\"corpus data files/Author Data.csv\", index=False)\n",
    "print(author_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of entries in these dataframes.  And the saved CSV is 765MB (!!!) on my machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = sum(len(i.split()) for i in blog_dataframe[\"Post\"])\n",
    "num_authors = blog_dataframe['Author ID'].nunique()\n",
    "print(f\"Number of posts:             {blog_dataframe.shape[0]:,}\")\n",
    "print(f\"Number of authors:           {num_authors:,}\")\n",
    "print(f\"Approximate number of words: {num_words:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's look at some of the ways we can process this text with various libraries.  We'll use the very first blog post as an working example to show what some of these processes do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_post = list(blog_dataframe[\"Post\"])[0]\n",
    "print(demo_post)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before we continue: some basic NLP ideas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The big challenge: Sparsity\n",
    "\n",
    "NLP is fundamentally a branch of machine learning (ML) that focuses just on language data.  As such, it inherits a lot of ideas and concerns from ML.  One of the biggest ones is _sparsity_.  Sparsity is the phenomenon of having a lot of features with \"null\" values for a lot of your observations (this usually manifests as \"missing data\" or \"zero\").\n",
    "\n",
    "In language, sparsity is everywhere, since _most utterances don't use most features._  So if we want to categorize any non-trivial corpus of language data, we'll need a _lot_ of features, but most of them won't appear in most of our utterances.\n",
    "\n",
    "Sparsity is bad.  It makes models have less data to work with for finding patterns, and thus, models will tend to overfit--they'll fit very well to the _current data,_ but will generalize poorly to new data.  Thus, almost all of the work in NLP is actually geared at finding ways to reduce sparsity in language data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data at scale\n",
    "\n",
    "All of the models of language used in the NLP community are essentially data-driven.  Rules-based models are all but completely dead.\n",
    "\n",
    "This means that models love data.  More data is better than less, and will make better models, since they're ultimately statistically-based models.\n",
    "\n",
    "But more data means more computing time.  So, we strive to build models that balance computational efficiency against power and accuracy.  As it turns out, one of the best ways to do this is to get a basic, but working, model of language, and just train it on a massive dataset.  Your results may not be perfectly accurate for every single document, but they should be pretty accurate for the whole dataset, and generally accurate for most documents, if you do it right.\n",
    "\n",
    "Bear in mind: computational approaches care about balancing precision and recall, but they often come at the expense of the other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A good enough model of language\\*: Just count the words!\n",
    "\\* _At least, for many basic purposes._\n",
    "\n",
    "One of the ways to deal with sparsity is to just ignore certain classes of features and focus on a richer subset.  As it turns out, we can do a surprisingly large amount by ignoring every aspect of language except for _what lexical units are used_ and _how often they are used_.  I.e., ignore syntax, ignore pragmatics, ignore even semantics--just count words.\n",
    "\n",
    "It's a brutally simplistic model of language.  But for many tasks, it works, and it is useful.  Consider: if my task is to find out \"what are people talking about,\" e.g. tracking discussions on Twitter at a large scale, looking at the words used is probably going to be my fastest way to get to that.\n",
    "\n",
    "Equally important, this is a _computationally_ simple, straightforward, and fast approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducing sparsity through preprocessing\n",
    "\n",
    "One of the most important ways to reduce sparsity is through preprocessing your data. We want to \"condense\" the data down to a still meaningful representation that irons out all the noise.  E.g., we often want everything to be in the same casee, because computers think that \"WORD\" (in uppercase) is different from \"word\", \"Word\", \"WoRd\", etc, and does not see any similarities between these tokens unless we explicitly tell it.\n",
    "\n",
    "When we're doing a word-counting type analysis, remember that we're trying to get at a dense representation of the _content_ of our data.  Some common preprocessing steps to that end are:\n",
    "* Convert to lowercase\n",
    "* (Sometimes, but not always) De-accent characters, e.g. convert \"Ã¢\" to \"a\".\n",
    "* Tokenize, i.e. split a doucment into a list of tokens (words, punctuation, etc)\n",
    "* Remove tokens:\n",
    "  * _Stopwords_ (generally, _function_ words), e.g. \"the\", \"a\", \"to\", etc.\n",
    "  * Words with very low frequencies (contain very little information, and are not useful)\n",
    "  * Words with extremely high _document-wise_ frequencies (these don't help us discriminate between documents in our corpus)\n",
    "* Stem, or lemmatize, the text (not always--depends on the analysis!)\n",
    "  * Stemming is significantly faster, but lemmatization can be more accurate.\n",
    "  * But for downstream tasks and analysis, both stemming and lemmatization tend to perform about the same.\n",
    "* Find multi-word phrases\n",
    "  * This might add features, but those features may be more meaningful than individual word counts.\n",
    "\n",
    "In almost all cases, these steps are followed by generating a _vectorized_ representation of the text--e.g. Bag-of-Words, Word2Vec, Doc2Vec, GloVe, etc.  These vector representations can then be used for any quantitative, statistical, or machine learning analyses, e.g. regressions and classifications.\n",
    "\n",
    "For other analyses (rather than just counting words), we might be interested in preserving more sophisticated, perhaps structural, features of the text:\n",
    "* Identifying (named) entities\n",
    "* Identifying noun chunks (more or less NPs/DPs)\n",
    "* Syntactic parsing (dependency and constituent parsing are most common)\n",
    "* Part-of-speech tagging\n",
    "\n",
    "As for the actual analyses we might do, they are many:\n",
    "* Topic modeling\n",
    "* Sentiment analysis\n",
    "* Document scoring/classification (e.g., author identification)\n",
    "\n",
    "The rest of this notebook is a whirlwind tour through some of these capabilities in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "We can use the two excellent libraries we've already seen: spaCy and Gensim.\n",
    "\n",
    "Gensim is a much _faster_ library for preprocessing, since it only operates based on raw string patterns and is designed to be fast and scale to massive datasets.  There is the assumption that any error we induce through the very simplistic approaches will be balanced out by the amount of data we work with--generally a good, and correct, assumption.  Gensim does no syntactic parsing, POS tagging, or other such structure-related analysis, but it's not designed for that.\n",
    "\n",
    "spaCy is the much _more accurate_ library, since it parses text based on large, powerful pre-trained models (think Stanford's CoreNLP toolkit--it's very much analogous to that).  While still very fast, spaCy is painfully slow compared to Gensim.  But, it has a far more robust tokenizer, it can do part-of-speech tagging, lemmatization, dependency parsing, entity and noun chunk identification, and it even has pre-trained word vectors (and can easily compute vectors for strings of words or entire documents)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gensim\n",
    "\n",
    "Let's first look at Gensim's preprocessing.  There's one function--`gensim.parsing.preprocessing.preprocess_string()`--which encompasses almost all the basic functions we need: lowercasing, de-accenting, tokenizing, stemming (with the Porter stemmer algorithm), stopword removal, removal of numbers, and removal of very short words (which are generally noise to use).\n",
    "\n",
    "We can also use the Phrases()/Phraser() objects we saw before to find multi-word phrases with ease, though given the size of our demo post, we won't see any show up.  We probably want to do this _after_ we run the main preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phrases\n",
    "from gensim.parsing.preprocessing import preprocess_string\n",
    "\n",
    "processed = preprocess_string(demo_post)\n",
    "phrases = Phrases(processed)\n",
    "processed = list(phrases[processed])\n",
    "\n",
    "print(\"Original post:\")\n",
    "print(demo_post)\n",
    "\n",
    "print(\"\\nAfter Gensim preprocessing:\")\n",
    "print(\" \".join(processed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the Porter Stemmer finds the uninflected forms of words.  It bases its processing _purely_ on the _letters of a word_.  This makes it fast, but it doesn't always give real words or the most correct forms, e.g. `\"morning\"` --> `\"morn\"`, and `\"okay\"` --> `\"okai\"`.  But, for most text mining or NLP tasks, this is actually not that big of an issue.  This only _really_ matters, in any practical sense, for human interpretation--which, admittedly, is a non-trivial concern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spaCy\n",
    "\n",
    "spaCy requires us to load models in, as we saw earlier when doing stem-based concordancing.  As before, we'll use their small English model, but we would just change the model name in `spacy.load()` if we wanted a different model. The small model will generally be a bit faster, which is all we need for this demo's purposes.\n",
    "\n",
    "As before, applying the spaCy pipeline is easy, though we do need to manually filter our stopwords and punctuation with some explicit checks.  And as with the Gensim examples, we'll run the Phrases() model on our post, though as before we won't see any changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from gensim.models.phrases import Phrases\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "processed = [\n",
    "    i.lemma_\n",
    "    for i in nlp(demo_post)\n",
    "    if i.is_stop == False\n",
    "    and i.is_punct == False\n",
    "]\n",
    "phrases = Phrases(processed)\n",
    "processed = list(phrases[processed])\n",
    "\n",
    "print(\"Original post:\")\n",
    "print(demo_post)\n",
    "\n",
    "print(\"\\nAfter spaCy preprocessing:\")\n",
    "print(\" \".join(processed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the processed text is much more _human-readable_ with this approach (this is due to the use of a lemmatizer, rather than a stemmer).  While nice for reporting and inspecting results, the extra overhead in runtime (not evident in this small example) might make this an unreasonable proposition for large datasets if time is an issue.  And, as mentioned earlier, if you're doing an _automated analysis_ of your text later, there isn't always a big difference, if any, in how well stemming versus lemmatization performs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Detour through Linguistic Analysis with spaCy\n",
    "\n",
    "spaCy's language models have a LOT of functionality.  Let's look at just some of the most easily accessible ones.\n",
    "\n",
    "First, we've already seen spaCy's ability to do lemmatization, stopword tagging, and punctuation tagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "print(f\"{'Token':<15s}\\t{'Lemma':<15s}\\t{'Is stopword?':<15s}\\t Is punctuation?\")\n",
    "for i in nlp(demo_post)[:15]:\n",
    "    token = i.text\n",
    "    lemma = i.lemma_\n",
    "    is_stop = str(i.is_stop)\n",
    "    is_punct = str(i.is_punct)\n",
    "    print(f\"{token:<15s}\\t{lemma:<15s}\\t{is_stop:<15s}\\t{is_punct}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, we can also do part-of-speech tagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{'TOKEN':<15s}\\t{'COARSE POS':<17s}\\t{'FINE POS'}\")\n",
    "for i in nlp(demo_post)[:15]:\n",
    "    token = i.text\n",
    "    coarse = i.pos_\n",
    "    fine = i.tag_\n",
    "    print(f\"{token:<15s}\\t{coarse:<17s}\\t{fine}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entity recognition..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "ents = nlp(demo_post).ents\n",
    "pprint(list(ents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noun chunk identification..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "noun_chunks = nlp(demo_post).noun_chunks\n",
    "pprint(list(noun_chunks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, as we might suspect from the above information, spaCy also does dependency parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(f\"{'TOKEN':<15s}\\t{'HEAD':<15s}\\t{'DEPENDENCY RELATION':<20s}\\t{'CHILDREN'}\")\n",
    "for i in nlp(demo_post)[:25]:\n",
    "    token = i.text\n",
    "    head = i.head.text\n",
    "    dep = i.dep_\n",
    "    children = \", \".join(c.text for c in i.children if not c.is_punct)\n",
    "    print(f\"{token:<15s}\\t{head:<15s}\\t{dep:<20s}\\t{children}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the built-in disiplaCy tool to generate a visualization of the dependency parse (though only of the first sentence, for space's sake):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "displacy.render(\n",
    "    nlp(\"Well, everyone got up and going this morning.\"), \n",
    "    style=\"dep\",\n",
    "    jupyter=True # to make this render correctly in the Jupyter notebook\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's more that spaCy can do, and there are other models and libraries available for doing this sort of automated parsing and annotation of text (e.g., there are interfaces to Stanford's CoreNLP suite), but spaCy is always a good bet since it's fast (for the amount of work it does), pretty accurate, easy to use, and flexible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling\n",
    "\n",
    "Topic Modeling refers to a wide variety of algorithms that are used to explore and discover \"topics\" within a corpus.  \"Topic\" is being used with a very specific meaning here--a topic is a _statistical distribution of words_.  You might already be familiar with Latent Semantic Analysis (LSA; sometimes called Latent Semantic Indexing, or LSI), which is an older model for this sort of analysis.\n",
    "\n",
    "Most modern algorithms are based on [Latent Dirichlet Allocation (LDA)](http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf), which uses word co-occurrences within documents to determine the topics.  LDA has given rise to a number of subsequent topic models: \n",
    "* Author-Topic models, which are LDA with metadata (usually, but not always, the author of a piece)\n",
    "* Dynamic topic models, which include time metadata in the modeling process\n",
    "* Hierarchical Dirichlet Process, an extension of LDA that is _nonparametric_ with regards to the number of topics (but can give less clear results in some cases).\n",
    "\n",
    "All of these models are implemented in Genim.  Due to the size of our corpus and the fact that this is a live demo, we'll use Gensim's speedier preprocessing tools to work with our data and prepate it for topic modeling.  And we'll only show LDA, since the others do the same basic thing and look basically the same in code.\n",
    "\n",
    "Gensim requires that the corpus be in a _bag of words_ format for topic modeling, so we'll need to put our documents in that format first.  Fortunately this requires little code: just do our preprocessing, then use some pre-built tools from Gensim to do the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.corpora.mmcorpus import MmCorpus\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.parsing.preprocessing import preprocess_string\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "# preprocess our corpus\n",
    "corpus = [\n",
    "    preprocess_string(i) \n",
    "    for i in tqdm(blog_dataframe[\"Post\"], desc=\"Preprocessing\")\n",
    "]\n",
    "phrases = Phraser(Phrases(tqdm(corpus, desc=\"Phrase-finding\"), min_count=100))\n",
    "corpus = list(phrases[tqdm(corpus, desc=\"Phrasing\")])\n",
    "id2word = Dictionary(tqdm(corpus, desc=\"id2word\"))\n",
    "vocabsize = len(id2word)\n",
    "# remove tokens with extremely high or low frequencies\n",
    "id2word.filter_extremes(\n",
    "    no_above=.5,  # remove tokens in > 50% of the documents (default)\n",
    "    no_below=5,   # remove tokens in < 5 documents (default)\n",
    "    keep_n=500000 # only keep 500k tokens, max--up from default 100k for good measure\n",
    ")\n",
    "# Reset index spacings for better efficiency\n",
    "id2word.compactify()\n",
    "print(f\"Removed {vocabsize - len(id2word)} tokens based on frequency criteria.\")\n",
    "corpus = [\n",
    "    id2word.doc2bow(i) \n",
    "    for i in tqdm(corpus, desc=\"BoW\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Around this time we should also notice that we're using a _huge_ amount of RAM.  So much, in fact, that the computer might be close to running out.  (It turns out that it's almost entirely from the dataframe still in memory, but let's just pretend for the moment that it's our actual corpus).  How do we deal with this?  Simple: _streaming._  Gensim is built around the concept of working with data one chunk at a time--so we can save our data to a file, then read one \"chunk\" of that file at a time!  When dealing with a few hundred, or a few thousand, documents this isn't needed.  But when dealing with _millions_ of documents or more, it's absolutely required, unless you want to spend thousands of dollars on extremely high-end computing hardware or cloud computing time/space.\n",
    "\n",
    "Our corpus doesn't actually require this.  (The cell below will show it's only using a few megabytes--perfectly reasonable).  But we'll do it anyways, just for demonstration's sake, since this same idea and approach would scale up nicely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys import getsizeof\n",
    "\n",
    "# getsizeof returns bytes--convert to megabytes\n",
    "# nb: factors of 1024, not 1000\n",
    "id2word_size = getsizeof(id2word) / 1024\n",
    "corpus_size = getsizeof(corpus) / 1048576\n",
    "blog_size = getsizeof(blog_dataframe) / 1048576\n",
    "\n",
    "print(f\"id2word dictionary size in memory: {id2word_size:.2f}KB\")\n",
    "print(f\"Gensim corpus size in memory: {corpus_size:.2f}MB\")\n",
    "print(f\"Blog Dataframe dictionary size in memory: {blog_size:.2f}MB\")\n",
    "\n",
    "# Delete that massive blog dataframe first--we already saved\n",
    "# it to file.\n",
    "del blog_dataframe\n",
    "\n",
    "print(\"Saving id2word dictionary.\")\n",
    "id2word.save(\"corpus data files/id2word\")\n",
    "print(\"Serializing bag-of-words corpus.\")\n",
    "MmCorpus.serialize(\n",
    "    fname=\"corpus data files/bow_corpus.mm\",\n",
    "    corpus=corpus,\n",
    "    id2word=id2word\n",
    ")\n",
    "print(\"Corpus and id2word dict saved.\")\n",
    "\n",
    "# we can reload the id2word and corpus data from the files\n",
    "# we just saved, too.\n",
    "del corpus\n",
    "del id2word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When re-reload the corpus, we'll see that it's _much_ smaller in memory.  This is because, using Gensim's tools, we're only looking at the size of the _thing that accesses the data,_ but which does not currently store any of the data--it's all in a file on disk.  \n",
    "\n",
    "Accessing data from disk will be slow, though.  Even the fastest SSDs are still at least 10x slower than even slow RAM.  So our runtime will be limited by _disk access speed_.  BUT, disk space is _significantly_ cheaper than RAM space!  At the time of writing, 16 gigabytes of RAM costs about \\$200*.  Meanwhile, $200 can get you between 7 and 10 _terabytes_ of hard drive space (less if you want SSDs, though).  So while reading corpora off disk is very slow, it lets us work with _much_ larger datasets.\n",
    "\n",
    "We can re-load the corpus and dictionary we just saved.  The dictionary didn't take up much memory at all, but saving a copy was a good idea anyways.  The corpus also didn't take much space.\n",
    "\n",
    "\\*_RAM prices are currently (as of early 2018) very highly inflated, though; normally this much RAM should only cost about $100.  But the point still stands: RAM is expensive, hard drives are dirt cheap._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading id2word dictionary.\")\n",
    "id2word = Dictionary.load(\"corpus data files/id2word\")\n",
    "print(\"Loading bag-of-words corpus.\")\n",
    "corpus = MmCorpus(\"corpus data files/bow_corpus.mm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aside: spaCy bag-of-words pipeline\n",
    "\n",
    "We won't run this during this demo, but just for comparison, here's a spaCy preprocessing pipeline that does the same thing.  The only thing that changes is the first pass through the corpus (the first `corpus = [...]` bit)--the bag-of-words steps are as before.  The change is shown below--all the other code would be the same.\n",
    "\n",
    "```\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"tagger\", \"ner\"])\n",
    "corpus = [\n",
    "    [\n",
    "        i.lemma_\n",
    "        for i in nlp(j)\n",
    "        if i.is_stop == False\n",
    "        and i.is_punct == False\n",
    "    ]\n",
    "    for j in tqdm(texts, desc=\"Preprocessing\")\n",
    "]\n",
    "```\n",
    "\n",
    "When I ran this on my computer, it took about an hour and a half to run (~200 documents per second), compared to the Gensim pipeline which took about 11 minutes (~1000 documents per second).  Of course, if you remove the `disable` line, you'll get much higher-quality results, but it will run about 10x slower (~20 documents per second on my computer, nearly 12 hours total runtime)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back to topic models\n",
    "\n",
    "Now, let's run some of these topic models.  We won't bother tweaking any of the default settings (except for chunksize, which should give us a bit more speed), meaning each one will search for 100 topics.  This is the most important parameter in the models, by far--and sadly, the only really good way to find a good value is to run it at a range of different topic numbers and see what gives you useful results.  You _can_ look at the _coherence_ of each topic (calculated by Gensum automatically) and use that to evaluate your model, but the be-all-end-all is the human interpretability and the usefulness of your topics.\n",
    "\n",
    "These models will take a while.  So we can skip down to the next cell and just load the models back up from disk.\n",
    "\n",
    "We also need to do a _[term frequency-inverse document frequency](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)_ transform on the corpus.  This is a weighting scheme that converts the raw word counts into values that _decrease_ the weight of a word based on how many documents it appears in (more documents --> proves less informatio about any individual document, so decrease the weight), and _increases_ it based on how often it appears _within the current document_ (more occurrences --> more important to the document)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "from gensim.corpora.mmcorpus import MmCorpus\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "from gensim.models.atmodel import AuthorTopicModel\n",
    "from gensim.models.hdpmodel import HdpModel\n",
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = Dictionary.load(\"corpus data files/id2word\")\n",
    "corpus = MmCorpus(\"corpus data files/bow_corpus.mm\")\n",
    "tfidf_model = TfidfModel(tqdm(corpus, desc=\"TFIDF Fitting\"))\n",
    "# TfidfModel returns a generator.  We want it as a list to \n",
    "# re-use it for all the models.\n",
    "corpus = list(tfidf_model[tqdm(corpus, desc=\"TFIDF Transforming\")])\n",
    "print(\"Serializing tf-idf corpus.\")\n",
    "MmCorpus.serialize(\n",
    "    fname=\"corpus data files/tfidf_corpus.mm\",\n",
    "    corpus=corpus,\n",
    "    id2word=id2word\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gensim has a multi-threaded LDA implementation that can take advantage of multi-core processors for significant speedups; we'll use that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# corpus = MmCorpus.load(\"corpus data files/tfidf_corpus.mm\")\n",
    "print(\"Running LDA model.\")\n",
    "lda = LdaMulticore(corpus, workers=3, id2word=id2word, chunksize=50000)\n",
    "print(\"Saving LDA model to file.\")\n",
    "lda.save(\"topic models/LDA.model\")\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some of the LDA outputs and see if we can interpret them.  We'll look at only the top ten highest-likelihood topics, sorted by decreasing likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "lda = LdaMulticore.load(\"topic models/LDA.model\")\n",
    "pprint(lda.top_topics(corpus, topn=10)[:10])\n",
    "\n",
    "# delete the model to conserve RAM space.\n",
    "# We've already saved it to disk, so it can be reloaded.\n",
    "del lda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings\n",
    "\n",
    "Word embeddings have absolutely taken over the entire field of NLP, starting with Mikolov et al's 2013 paper on [Word2Vec](https://arxiv.org/abs/1301.3781).  The basic idea of these embeddings:\n",
    "* Some notion of \"meaning\" is recoverable from the _contexts_ that a word occurs in\n",
    "* It is possible to generate a _vector_ representation of a word such that _words appearing in similar contexts have similar vectors_ (or, more intuitively, \"are close to each other\").\n",
    "\n",
    "Word vectors are used for almost every kind of task: document scoring and classification, sentiment analysis, document and word clustering, machine translation (though less commmonly), and more.  Plus, they've been demonstrated to excel at a number of lexical similarity and analogy tasks.\n",
    "\n",
    "Gensim has an impelementation of Word2Vec that we can use on our corpus.  spaCy, meanwhile, comes pre-bundled with word vectors computed using Stanford's GloVe algorithm (which works differently under the hood, but in terms of how well the vectors actually perform in any application, is basically identical).\n",
    "\n",
    "### Brief aside: Poincare Embeddings\n",
    "In May 2017, Maximilian Nickel and Douwe Kiela published [an extremely exciting paper](https://arxiv.org/pdf/1705.08039) on embeddings performed in _hyperbolic space_ rather than _Euclidean space_ (all prior models were in Euclidean space).  These embeddings--while not yet well-suited to extracting word similarities and meanings from large bodies of unlabeled text--show an extreme aptitude for embedding _graph_- and _network_-like data, e.g. WordNet, and can capture various relations like hypernymy/hyponymy and synonymy quite well.  Keep an eye on \"Poincare Embeddings\"--lots of interesting things are sure to happen there soon.\n",
    "\n",
    "### spaCy word vectors\n",
    "We'll work with spaCy's pre-trained vectors just for time's sake--training word2vec models in Gensim is surprisingly fast, but we'll be doing basically the same thing downstream.  First, as always, we need to load our data and our model.  We'll just use a single blog post, and we'll turn off a bunch of the spaCy parsing stuff for speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# just use the very first post for demos\n",
    "demo_post = pd.read_csv(\n",
    "    \"corpus data files/Blog Data.csv\",\n",
    "    usecols=[\"Post\"],\n",
    "    nrows=1,\n",
    "    squeeze=True\n",
    ")[0]\n",
    "demo_post = nlp(demo_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(demo_post[1])\n",
    "print(demo_post[1].vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{'Token':<15s}\\tSimilarity to '{demo_post[1].text}'\")\n",
    "\n",
    "for i in demo_post[:15]:\n",
    "    # try/except because some token comparisons throw errors on spaCy's end\n",
    "    try:\n",
    "        print(f\"{i.text:<15s}\\t{demo_post[1].similarity(i):.3f}\")\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word vectors can also be extended to computer vectors for entire documents.  Usually, this is done by simply adding or averaging the vectors for each individual word in the document.  (Incidentally, this exact same approach lets you get a vector for any arbitrary bit of text!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(demo_post.vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{'Token':<15s}\\tSimilarity to whole document\")\n",
    "for i in demo_post[:15]:\n",
    "    print(f\"{i.text:<15s}\\t{demo_post.similarity(i):<.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification and Regression with Text\n",
    "Since we have vectors representing documents (we can also use the bag-of-words representation for this, too!), we can do any classical statistical test or modelling we want, like trying to build a model to predict the age of a post's author from its contents, or predicting their gender.  But we won't use statistics, because we have tools better suited to the specifics of the tasks at hand: machine learning.\n",
    "\n",
    "Without getting too deep in the weeds, machine learning models are essentially extensions of statistics that are based on _computational mathematics_ rather than _classical mathematics_.  They tend to scale better to massive datasets (i.e. run faster and predict better) and are often more powerful for pure prediction, but they are designed for larger datasets and may not work well at lower sizes.\n",
    "\n",
    "However, ultimately, the distinction between \"machine learning\" and \"statistics\" is a false one.  The real difference is \"experimentalists\" (who seek to understand _causation_) and \"data miners,\" who only want to build models with good _predictive power_.  (I won't get any further into this--but Leo Breiman's 2001 paper [Statistical Modeling: The Two Cultures](https://projecteuclid.org/euclid.ss/1009213726) is an interesting exploration of this divide).\n",
    "\n",
    "Python has some absoltuely top-tier machine learning libraries: Scikit-Learn for non-neural models (e.g. decision trees, random forests, support vector machines), and various environments like Keras, Tensorflow, and Pytorch for building neural networks.  We'll use non-neural models for this demo--they both run faster (as in, by a factor of days, sometimes) and are _much_ easier to interpret.\n",
    "\n",
    "We'll build four models, using two different sets of target variables and two different sets of predictors.\n",
    "\n",
    "Targets: \n",
    "* Author age (regression)\n",
    "* Author gender (binary classification)\n",
    "* Author industry, where provided (multi-class classification)\n",
    "\n",
    "Predictors:\n",
    "* Document vectors, generated by spaCy\n",
    "* Bag-of-Words model, _without_ tf-idf weighting (it ends up being only marginally helpful for the model performance of SVMs, but detrimental to the model interpretability)\n",
    "\n",
    "We'll just one type of model--Support Vector Machines--for both regression and classification.  SVMs strike a nice balance between speed (though there are faster models) and performance (though there are more powerful models), and are always a good go-to.  For binary classification tasks, they're still among the best models around.  Specifically, we'll use SVMs with a _linear kernel_--don't worry for now about the impelementation details of that, just know that a linear kernel SVM runs faster than other kinds, and is often plenty good for most tasks.\n",
    "W\n",
    "\n",
    "(Spoilers: our predictions will be kinda crap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need a lot more imports for this than before\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.corpora.mmcorpus import MmCorpus\n",
    "from gensim.matutils import corpus2csc\n",
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.svm import LinearSVC, LinearSVR\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import MaxAbsScaler, Normalizer\n",
    "from scipy.sparse import csr_matrix\n",
    "import spacy\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading in target data.\n"
     ]
    }
   ],
   "source": [
    "print(\"Reading in target data.\")\n",
    "targets = pd.read_csv(\n",
    "    \"corpus data files/Author Data.csv\",\n",
    "    usecols=[\"Age\", \"Gender\", \"Industry\"],\n",
    "    squeeze=True\n",
    ")\n",
    "ages = targets[\"Age\"].values\n",
    "genders = targets[\"Gender\"].values\n",
    "industries = targets[\"Industry\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({17: 2381,\n",
      "         16: 2152,\n",
      "         23: 2026,\n",
      "         24: 1895,\n",
      "         15: 1771,\n",
      "         25: 1620,\n",
      "         26: 1340,\n",
      "         14: 1246,\n",
      "         27: 1205,\n",
      "         13: 690,\n",
      "         33: 464,\n",
      "         34: 378,\n",
      "         35: 338,\n",
      "         36: 288,\n",
      "         37: 259,\n",
      "         38: 171,\n",
      "         39: 152,\n",
      "         40: 145,\n",
      "         41: 139,\n",
      "         42: 127,\n",
      "         43: 116,\n",
      "         45: 103,\n",
      "         44: 94,\n",
      "         48: 77,\n",
      "         46: 72,\n",
      "         47: 71})\n",
      "Counter({'female': 9660, 'male': 9660})\n",
      "Counter({'indUnk': 6827,\n",
      "         'Student': 5120,\n",
      "         'Education': 980,\n",
      "         'Technology': 943,\n",
      "         'Arts': 721,\n",
      "         'Communications-Media': 479,\n",
      "         'Internet': 397,\n",
      "         'Non-Profit': 372,\n",
      "         'Engineering': 312,\n",
      "         'Government': 236,\n",
      "         'Law': 197,\n",
      "         'Consulting': 191,\n",
      "         'Science': 184,\n",
      "         'Marketing': 180,\n",
      "         'BusinessServices': 163,\n",
      "         'Publishing': 150,\n",
      "         'Advertising': 145,\n",
      "         'Religion': 139,\n",
      "         'Telecommunications': 119,\n",
      "         'Military': 116,\n",
      "         'Banking': 112,\n",
      "         'Accounting': 105,\n",
      "         'Fashion': 98,\n",
      "         'HumanResources': 94,\n",
      "         'Tourism': 94,\n",
      "         'Transportation': 91,\n",
      "         'Sports-Recreation': 90,\n",
      "         'Manufacturing': 87,\n",
      "         'Architecture': 69,\n",
      "         'Chemicals': 62,\n",
      "         'Biotech': 57,\n",
      "         'LawEnforcement-Security': 57,\n",
      "         'RealEstate': 55,\n",
      "         'Museums-Libraries': 55,\n",
      "         'Construction': 55,\n",
      "         'Automotive': 54,\n",
      "         'Agriculture': 36,\n",
      "         'InvestmentBanking': 33,\n",
      "         'Environment': 28,\n",
      "         'Maritime': 17})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "# Look for bad values we might need to remove;\n",
    "# the Counter() will tell us how many of each\n",
    "# value we have in our dataset.\n",
    "pprint(Counter(ages))\n",
    "pprint(Counter(genders))\n",
    "pprint(Counter(industries))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll want to remove the `'indUnk'` industry entries, since these are basically just missing data for us, they consist of a large portion of our total dataset, and we cannot reasonably impute any values.  We'll do this later, when we get to the actual model-building stage.\n",
    "\n",
    "Now, we'll get the GloVe vectors from spaCy.  This will take a pretty long time (about 5 hours on my computer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "print(\"Loading spaCy model.\")\n",
    "# Only large model has pre-trained word vectors.\n",
    "# Disable the parser, tagger, and named entity recognizer\n",
    "# for extra speed in this step.\n",
    "nlp = spacy.load(\"en_core_web_lg\", disable=[\"parser\", \"tagger\", \"ner\"])\n",
    "print(\"Loading blog posts.\")\n",
    "vector_corpus = list(pd.read_csv(\n",
    "    \"corpus data files/Author Data.csv\",\n",
    "    usecols=[\"Posts\"],\n",
    "    squeeze=True\n",
    "))\n",
    "print(\"Retrieving vectors.\")\n",
    "vector_corpus = np.array([\n",
    "    nlp(i).vector\n",
    "    for i in tqdm(vector_corpus, desc=\"spaCy vectors\")\n",
    "])\n",
    "print(vector_corpus)\n",
    "print(vector_corpus.shape)\n",
    "# Cast to a 32-bit floating point format (smaller memory/on-disk size)\n",
    "# and save.  This means we don't have to re-run the above time-\n",
    "# consuming steps every time we want to do some work with this\n",
    "# matrix.\n",
    "np.save(\n",
    "    \"corpus data files/GloVe matrix.npy\", \n",
    "    vector_corpus.astype(np.float32)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Haha, no, just kidding.  That took almost 5 hours to run on my machine, so let's not do that.  We'll use Doc2Vec, as implemented in Gensim, to generate our own word vectors.  Doc2Vec is one of the many vectorization/embedding approaches, but rather than working at the word level, it works directly at the document level.  We'll use a less aggressive preprocessing, since vectorization models are less sensitive to sparsity; this one just de-accents, lowercases, and removes tokens by length.  We'll leave all settings at default (so, mininum length 2, maximum length 15).\n",
    "\n",
    "We'll also only use 100-dimensional vectors for our embeddings, just for speed's sake (and not at all because I meant to do 300 dimensions but forgot to set that option, and only realized half an hour ago).  Note that in general, higher dimensional embeddings (e.g. 300 vs 100) will give better, more accurate results, but will be sparser and take longer to run models on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from gensim.models.callback import CallbackAny2vec\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import doc2vec\n",
    "from gensim.models.phrases import Phrases\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# class to let us track training progress\n",
    "# through Gensim's callbacks interface\n",
    "class EpochLogger(CallbackAny2Vec):\n",
    "    \"Callback to log information about training\"\n",
    "    def __init__(self):\n",
    "        self.epoch = 0\n",
    "\n",
    "    def on_epoch_begin(self, model):\n",
    "        print(\"Epoch #{} start\".format(self.epoch))\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        print(\"Epoch #{} end\".format(self.epoch))\n",
    "        self.epoch += 1\n",
    "\n",
    "df = pd.read_csv(\n",
    "    \"corpus data files/Author Data.csv\", \n",
    "    usecols=[\"Posts\"], \n",
    "    squeeze=True\n",
    ").values\n",
    "\n",
    "# gensim preprocessing\n",
    "df = [\n",
    "    simple_preprocess(i)\n",
    "    for i in tqdm(df, desc=\"Preprocessing\")\n",
    "]\n",
    "# phrasing\n",
    "phrases = Phrases(tqdm(df, desc=\"Phrase-finding\"), min_count=100)\n",
    "df = phrases[tqdm(df, desc=\"Applying phraser\")]\n",
    "\n",
    "df = [\n",
    "    doc2vec.TaggedDocument(i, [j])\n",
    "    for j,i in enumerate(df)\n",
    "]\n",
    "\n",
    "epoch_logger = EpochLogger()\n",
    "d2v = doc2vec.Doc2Vec(\n",
    "    df,\n",
    "    min_count=5,\n",
    "    epochs=15,\n",
    "    vector_size=100,  # increase this number for better accuracy\n",
    "    window=5,\n",
    "    worker=3,\n",
    "    callback=[epoch_logger]\n",
    ")\n",
    "\n",
    "vectors = np.array([d2v.infer_vector(i.words) for i in tqdm(df)])\n",
    "np.save(\"corpus data files/Doc2Vec Matrix.npy\", vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we re-generate our bag-of-words corpus and save it to disk.  Note that we could do tf-idf scaling here, and it might help results a bit, and it would probably help our performance a bit, but we won't--it will make interpreting our model considerably more difficult.  Instead, we'll just apply a standard scaling on the sparse matrix, scaling every feature such that the maximum absolute value is 1.  (we won't center it to zero mean and unit variance--that would destroy the sparsity and we couldn't load the matrix into memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading in posts.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79c5f0793601407a88dc3912000c2242",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Preprocessing', max=19320), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "455529e39ca34e8a8245bfe38ae180b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Phrase-finding', max=19320), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b2c518b4d8946cab6b0ecf541106f8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Phrasing', max=19320), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\andersonh\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\gensim\\models\\phrases.py:486: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9471baf9f2ee4961b7e40d000563cb23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='id2word', max=19320), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Removed 521864 tokens based on frequency criteria.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35cf3e91ef024dcaaeaa5b84162c2ad4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='BoW', max=19320), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Serializing corpus.\n",
      "Wall time: 21min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from gensim.parsing.preprocessing import preprocess_string\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "\n",
    "print(\"Reading in posts.\")\n",
    "corpus = list(pd.read_csv(\n",
    "    \"corpus data files/Author Data.csv\",\n",
    "    usecols=[\"Posts\"],\n",
    "    squeeze=True\n",
    "))\n",
    "\n",
    "# preprocess our corpus\n",
    "corpus = [\n",
    "    preprocess_string(i) \n",
    "    for i in tqdm(corpus, desc=\"Preprocessing\")\n",
    "]\n",
    "phrases = Phrases(tqdm(corpus, desc=\"Phrase-finding\"), min_count=100)\n",
    "corpus = list(phrases[tqdm(corpus, desc=\"Phrasing\")])\n",
    "id2word = Dictionary(tqdm(corpus, desc=\"id2word\"))\n",
    "vocabsize = len(id2word)\n",
    "# remove tokens with extremely high or low frequencies\n",
    "id2word.filter_extremes(\n",
    "    no_above=.5,  # remove tokens in > 50% of the documents (default)\n",
    "    no_below=5,   # remove tokens in < 5 documents (default)\n",
    "    keep_n=500000 # only keep 500k tokens, max--up from default 100k for good measure\n",
    ")\n",
    "# Reset index spacings for better efficiency\n",
    "id2word.compactify()\n",
    "print(f\"Removed {vocabsize - len(id2word)} tokens based on frequency criteria.\")\n",
    "corpus = [\n",
    "    id2word.doc2bow(i) \n",
    "    for i in tqdm(corpus, desc=\"BoW\")\n",
    "]\n",
    "\n",
    "print(\"Serializing corpus.\")\n",
    "id2word.save(\"corpus data files/author_id2word\")\n",
    "MmCorpus.serialize(\n",
    "    fname=\"corpus data files/author_corpus.mm\",\n",
    "    corpus=corpus,\n",
    "    id2word=id2word\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading bag-of-words (non tf-idf) corpus.\n",
      "Scaling bag-of-words features.\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading bag-of-words (non tf-idf) corpus.\")\n",
    "corpus = list(MmCorpus(\"corpus data files/author_corpus.mm\"))\n",
    "corpus = corpus2csc(corpus).transpose().tocsr()\n",
    "print(\"Scaling bag-of-words features.\")\n",
    "corpus = MaxAbsScaler().fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do the actual modeling.  We'll use Support Vector Machines for classification and regression; these are pretty powerful models and generally pretty good choices.  Others, which we won't use here because they take longer to run, include Random Forests, Adaboost, Stochastic Gradient Descent (which is fast, but has a _lot_ of parameters to tune), and neural networks.\n",
    "\n",
    "We will:\n",
    "\n",
    "* Use a grid search to exhaustively search for good parameters (within a pre-defined search space).  This is called _(hyper)parameter optimization._\n",
    "* Use 3-fold cross-validation to assess model performance for each parameter combination we try.\n",
    "  * Data is split into 3 equally sized \"folds.\"  (usually, 5 or 10 folds would be used, but for time's sake, 3 will suffice here).\n",
    "  * Four folds are used to train the model; the fifth is used to assess its performance.\n",
    "  * All permutations of \"train-on-2, test-on-1\" are performed, and scores are averaged to get the overall model performance.\n",
    "* Print out the best model parameters and the best score for each task.\n",
    "* Open up one of the models and look inside it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and Doc2Vec corpus.\n",
      "(19320, 100)\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# For remove \"indUnk\" industries later\n",
    "tmp = industries != \"indUnk\"\n",
    "\n",
    "# Load our vector corpus from file and normalize\n",
    "# each observation (not feature) to have magnitude 1\n",
    "print(\"Loading and Doc2Vec corpus.\")\n",
    "vector_corpus = np.load(\"corpus data files/Doc2Vec matrix.npy\")\n",
    "vector_corpus = Normalizer().fit_transform(vector_corpus)\n",
    "print(vector_corpus.shape)\n",
    "print(\"Done.\")\n",
    "\n",
    "# Create the scikit-learn optimizers that will do cross-validaton\n",
    "# scoring and tune our model parameters\n",
    "svr_optimizer = GridSearchCV(\n",
    "    estimator=LinearSVR(),\n",
    "    param_grid={\n",
    "        \"C\":np.logspace(-5, 1, 7),\n",
    "    },\n",
    "    n_jobs=3,\n",
    "    verbose=3,\n",
    "    cv=3,\n",
    "    error_score=0,\n",
    ")\n",
    "\n",
    "svc_optimizer = GridSearchCV(\n",
    "    estimator=LinearSVC(),\n",
    "    param_grid={\n",
    "        \"C\":np.logspace(-5, 1, 7)\n",
    "    },\n",
    "    n_jobs=3,\n",
    "    verbose=3,\n",
    "    cv=3,\n",
    "    error_score=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's optimize some models!  We'll save the best-performing models to file for each set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning CSR-Age regression fit.\n",
      "Fitting 3 folds for each of 7 candidates, totalling 21 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done  21 out of  21 | elapsed:  1.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning GloVe-Age regression fit.\n",
      "Fitting 3 folds for each of 7 candidates, totalling 21 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done  21 out of  21 | elapsed:    1.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning CSR-Astrological Sign classification fit.\n",
      "Fitting 3 folds for each of 7 candidates, totalling 21 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done  21 out of  21 | elapsed:  1.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning GloVe-Astrological Sign classification fit.\n",
      "Fitting 3 folds for each of 7 candidates, totalling 21 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done  21 out of  21 | elapsed:    6.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning CSR-Industry classification fit.\n",
      "Fitting 3 folds for each of 7 candidates, totalling 21 fits\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "print(\"Beginning CSR-Age regression fit.\")\n",
    "best_age_bow = svr_optimizer.fit(\n",
    "    corpus,\n",
    "    ages\n",
    ")\n",
    "joblib.dump(\n",
    "    best_age_bow.best_estimator_,\n",
    "    \"model files/age_bow.pkl\"\n",
    ")\n",
    "\n",
    "print(\"Beginning GloVe-Age regression fit.\")\n",
    "best_age_glove = svr_optimizer.fit(\n",
    "    vector_corpus,\n",
    "    ages\n",
    ")\n",
    "joblib.dump(\n",
    "    best_age_glove.best_estimator_,\n",
    "    \"model files/age_glove.pkl\"\n",
    ")\n",
    "\n",
    "print(\"Beginning CSR-Astrological Sign classification fit.\")\n",
    "best_sign_bow = svc_optimizer.fit(\n",
    "    corpus,\n",
    "    genders\n",
    ")\n",
    "joblib.dump(\n",
    "    best_sign_bow.best_estimator_,\n",
    "    \"model files/sign_bow.pkl\"\n",
    ")\n",
    "\n",
    "print(\"Beginning GloVe-Astrological Sign classification fit.\")\n",
    "best_sign_glove = svc_optimizer.fit(\n",
    "    vector_corpus,\n",
    "    genders\n",
    ")\n",
    "joblib.dump(\n",
    "    best_sign_glove.best_estimator_,\n",
    "    \"model files/sign_glove.pkl\"\n",
    ")\n",
    "\n",
    "print(\"Beginning CSR-Industry classification fit.\")\n",
    "best_industry_bow = svc_optimizer.fit(\n",
    "    corpus[tmp],\n",
    "    industries[tmp]\n",
    ")\n",
    "joblib.dump(\n",
    "    best_industry_bow.best_estimator_,\n",
    "    \"model files/industry_bow.pkl\"\n",
    ")\n",
    "\n",
    "print(\"Beginning GloVe-Industry Sign classification fit.\")\n",
    "best_industry_glove = svc_optimizer.fit(\n",
    "    vector_corpus[tmp],\n",
    "    industries[tmp]\n",
    ")\n",
    "joblib.dump(\n",
    "    best_industry_glove.best_estimator_,\n",
    "    \"model files/industry_glove.pkl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can quickly compute the approximate F1 scores for randomly guessing by randomly permuting the target classification variables and calculating the F1 scores then."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genders_f1_chance = f1_score(\n",
    "    genders, \n",
    "    np.random.permutation(genders)\n",
    ")\n",
    "industries_f1_chance = f1_score(\n",
    "    industries, \n",
    "    np.random.permutation(industries)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"CSR-Age model:\")\n",
    "print(f\"Best R^2 score:  {best_age_bow.best_score_}\")\n",
    "print(f\"Best parameters: {best_age_bow.best_params_}\")\n",
    "\n",
    "print(\"\\nGloVe-Age model:\")\n",
    "print(f\"Best R^2 score:  {best_age_glove.best_score_}\")\n",
    "print(f\"Best parameters: {best_age_glove.best_params_}\")\n",
    "\n",
    "print(\"\\nCSR-Gender model:\")\n",
    "print(f\"Best F1 score:   {best_sign_bow.best_score_}\")\n",
    "print(f\"Chance F1 score: {genders_f1_change}\")\n",
    "print(f\"Best parameters: {best_sign_bow.best_params_}\")\n",
    "\n",
    "print(\"\\nGloVe-Gender model:\")\n",
    "print(f\"Best F1 score:   {best_sign_glove.best_score_}\")\n",
    "print(f\"Chance F1 score: {genders_f1_change}\")\n",
    "print(f\"Best parameters: {best_sign_glove.best_params_}\")\n",
    "\n",
    "print(\"\\nCSR-Industry model:\")\n",
    "print(f\"Best F1 score:   {best_industry_bow.best_score_}\")\n",
    "print(f\"Chance F1 score: {industries_f1_change}\")\n",
    "print(f\"Best parameters: {best_industry_bow.best_params_}\")\n",
    "\n",
    "print(\"\\nGloVe-Industry model:\")\n",
    "print(f\"Best F1 score:   {best_industry_glove.best_score_}\")\n",
    "print(f\"Chance F1 score: {industries_f1_change}\")\n",
    "print(f\"Best parameters: {best_industry_glove.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, for kicks, let's open up one of the models--the age-bow model--and see what it learned.  SVMs are linear models, so we can just look at the coefficient weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = best_age_bow.best_estimator_\n",
    "coefs = model.coef_\n",
    "# Zip up the coefficients with the actual text\n",
    "# of the tokens, for human interpretability.\n",
    "id2word = Dictionary.load(\"corpus data files/author_id2word\")\n",
    "coefs = [(id2word[i], coefs[i]) for i in range(len(coefs))]\n",
    "coefs = sorted(coefs, key=lambda x: abs(x[1]), reverse=True)\n",
    "for i in coefs[:25]:\n",
    "    print(f\"{i[0]:<25s} {i[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
