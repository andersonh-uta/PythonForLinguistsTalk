{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python for Linguists\n",
    "I'm going to try something novel: giving this talk from a Jupyter notebook so I can run code on the fly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Who is this guy?\n",
    "* Henry Anderson ([henry.anderson@uta.edu](mailto:henry.anderson@uta.edu))\n",
    "* Data Scientist in the University Analytics department\n",
    "* Specialist in unstructured data (i.e., text), machine learning, and Natural Language Processing\n",
    "* First year masters student, with interests in computational social science, digital language use, and the language of online communities and networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In this talk...\n",
    "* Who stands to gain the most\n",
    "* Why consider _programming,_ generally?\n",
    "* Why consider _Python,_ specifically?\n",
    "* Some demos:\n",
    "  * Custom concordance code, with massive flexibility\n",
    "  * Some very cool NLP tools:\n",
    "    * Topic models\n",
    "    * Word vectors\n",
    "    * Word vector embeddings with t-SNE and plotting with Matplotlib\n",
    "    * Automatic dependency parsing, POS-tagging, lemmatization, tokenization, etc. (i.e., text preprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Who this talk is for\n",
    "* Anyone who deals with _data:_ people interested in corpus work, sociolinguistics, natural language processing, digital/online language, etc.\n",
    "* Anyone interested in _computational social science_ (CSS): i.e. general social science approaches leveraging large datasets and computational horsepower.\n",
    "  * CSS is currently exploding, and is a hugely important avenue for applied social science research.\n",
    "  * CSS is also massively interdisciplinary: programming, statistics, machine learning, AI, network analysis, linguistics, sociology, psychology, etc all combine to make CSS happen.\n",
    "* If you deal mostly with theory, or are primarily an experimentalist, you probably stand to gain less from this talk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What does programming offer?\n",
    "* (Quite literally) infinite control over your data processing: you're not limited by the features someone else decided to code into their program--you can change your code up to do anything you want.\n",
    "* Scalability and automation of your data work\n",
    "  * Work with literally millions of documents and billions of words with relative ease.\n",
    "  * Automate steps from data collection through final analysis.\n",
    "* Marketable skills: even a little bit of Python, Java, or any other language can open doors in the job market.\n",
    "* You'll feel like a badass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What does _Python_ offer?\n",
    "* Free (as in speech, not beer.  But also as in beer), open-source, royalty-free.  No licenses to sign, no royalties to pay, and _essentially no restrictions_ on what you can and can't do with it.  (the [Python Software Foundation license](https://docs.python.org/3/license.html) is an extremely permissive BSD-type license)\n",
    "\n",
    "* Huge userbase that's big into Open Source and Free Software--so it's easy to find help or sample code.\n",
    "\n",
    "* Rapidly becoming _the_ language for data science, displacing even R in most applications.  (R is still dominant for raw statistics, though Python has plenty of packages that implement common statistical tests).\n",
    "  * Though, keep an eye on a different language--Julia--over the next few years.  It is truly a worthy contender, but has yet to hit version 1.0 as of this talk.\n",
    "\n",
    "* Easy-to-learn language.\n",
    "  * Great documentation and stupid amounts of free, high-quality learning resources.\n",
    "  * Among its core ideas:\n",
    "    * Code is read far more than it is written, so the language should be _human-readable._\n",
    "    * \"There should be one, and preferably only one, obvious way to do it.\"  I.e., the most straightforward approach is _usually_ the best.  (This results in a lot of people writing straightforward, fairly easy-to-follow code).\n",
    "  * Commonly taught as a first programming language, so there are LOTS of materials for eveyone from beginning programmers to seasoned professionals; the Python community is also very welcoming of newcomers.\n",
    "\n",
    "* General purpose language: can do (almost) everything you want to make it to.\n",
    "  * Compare to R, which is great for statistics, and a pain for a lot of other stuff.\n",
    "  * Or Matlab, which is great for being a broken, slow, difficult software environment, and isn't so good at being, well, good.\n",
    "    * (this has been your mandatory \"Matlab is bad\" comment)\n",
    "\n",
    "* **For linguists**: a _huge_ array of language processing functionality and libraries.\n",
    "  * [spaCy](https://spacy.io), basically a Python version of Stanford's CoreNLP toolkit (lemmatization, tokenization, dependency parsing, POS tagging, and more).\n",
    "  * [Gensim](https://radimrehurek.com/gensim/), full of topic models and pretty bleeding-edge NLP tools.\n",
    "  * [Natural Language Toolkit (NLTK)](http://www.nltk.org/), a _massive_ library that's designed to teach a lot of NLP concepts (but can be used for some serious production work too).\n",
    "  * [Tensorflow](https://www.tensorflow.org/)+[Keras](https://keras.io/), for quickly and easily building neural networks.\n",
    "  * [PyTorch](http://pytorch.org/), an up-and-coming (but extremely exciting) neural network library.\n",
    "  * [Pandas](http://pandas.pydata.org/) for R-like dataframes, statistics, and general tabular data management.\n",
    "  * [Matplotlib](https://matplotlib.org/) (and others like [Seaborn](https://seaborn.pydata.org/), [PyGal](http://www.pygal.org/en/stable/), [Bokeh](https://bokeh.pydata.org/en/latest/), ...) for high-quality, powerful data visualization.\n",
    "  * [scikit-learn](http://scikit-learn.org/stable/index.html) for non-neural machine learning (support vector machines, random forests, and a few text features like basic preprocessing)\n",
    "    * Side note, the scikit-learn [User Guides](http://scikit-learn.org/stable/user_guide.html) are an _excellent_ technical crash course in machine learning, even if you're not too interested in Python.\n",
    "  * [Networkx](https://networkx.github.io/) for performing network analysis.\n",
    "  * And dozens more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some basic demos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concordances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concordances can be done with regular expressions and a teeny tiny bit of legwork.  (By the way: if you're working with text, you have no excuse to not learn regular expressions.  But that would be another talk all unto itself).  We'll work with the text of William Hope Hodgeson's novel _[The Boats of the \"Glen Carrig\"](https://en.wikipedia.org/wiki/The_Boats_of_the_%22Glen_Carrig%22)_, a 1907 horror novel.  The text was taken [from Project Gutenberg)](http://www.gutenberg.org/ebooks/10542), with the site's boilerplate text removed from the front and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import re\n",
    "\n",
    "def concordance(text, token, window=50):\n",
    "    pattern = re.compile(r\"\\b{}\\b\".format(token.strip()), re.IGNORECASE)\n",
    "    # convert all whitespaces to single space characters\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    for i in pattern.finditer(text):\n",
    "        print(\n",
    "            \"...\",\n",
    "            text[i.start() - 50:i.start()].rjust(50, \" \"),\n",
    "            text[i.start():i.start() + 50].ljust(50, \" \"),\n",
    "            \"...\",\n",
    "            sep=\"\"\n",
    "        )\n",
    "\n",
    "glen_carrig = open(\"glen carrig.txt\", \"r\", encoding=\"utf8\").read()\n",
    "concordance(glen_carrig, \"think\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if we want to get _really_ clever, we can have our concordance function search by stemmed forms.  We'll revisit stemming in a bit more detail shortly; for now, just know that stemming is the process of determining an uninflected form of words, but it's based purely on character patterns--so each word is treated completely in isolation, with no information about parts of speech.\n",
    "\n",
    "We need to stem the original text, then search for concordances of any tokens that get stemmed to the same value as our input.  Then we run the previous concordance function on those tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import re\n",
    "from gensim.parsing.preprocessing import stem_text\n",
    "\n",
    "def stem_concordance(text, token, window=50):\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    # get a unique list of all word-like tokens using a basic regex\n",
    "    word_finder = re.compile(r\"[A-z0-9]+\", re.MULTILINE)\n",
    "    vocab_to_stem = {\n",
    "        i.lower():stem_text(i)\n",
    "        for i in set(word_finder.findall(text))\n",
    "    }\n",
    "    if token.lower().strip() not in vocab_to_stem:\n",
    "        print(\"Token is not in the vocabulary.  Please try again.\")\n",
    "        return\n",
    "    # now flip the dict to {stemmed_form:{set of unstemmed form}}\n",
    "    stem_to_vocab = {i:set() for i in vocab_to_stem.values()}\n",
    "    for i in vocab_to_stem:\n",
    "        stem_to_vocab[vocab_to_stem[i]].add(i.lower())\n",
    "    # look up other tokens that have same stem as input token\n",
    "    stemmed_token = vocab_to_stem[token]\n",
    "    possible_forms = stem_to_vocab[stemmed_token]\n",
    "    # and now run the previous concordance function.\n",
    "    for i in possible_forms:\n",
    "        concordance(text, i, window=window)\n",
    "\n",
    "glen_carrig = open(\"glen carrig.txt\", \"r\", encoding=\"utf8\").read()\n",
    "stem_concordance(glen_carrig, \"think\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That only added about 0.2 seconds to the total runtime.  Nice.\n",
    "\n",
    "Now let's do it again, but with spaCy instead of Gensim.  spaCy has built-in tokenization, lemmatization, and more that are all based on large, pre-trained machine learning models.  This will give us much better accuracy--both with tokenizing and lemmatizing--but at the cost of _significantly_ higher runtime.  spaCy also has multiple models to choose from--for English, there's small, medium, and large.  The bigger the model, the better its accuracy, but also the slower it runs.  But since the interface is exactly the same, we'll use the small model for speed.\n",
    "\n",
    "We'll also revisit lemmatization in a bit more detail shortly.  The short version: it's like stemming, but it returns a valid, real word corresponding to the uninflected form of a token.  (unlike stemming, which might map \"today\" to the root form \"todai\"--lemmatization would correctly map this to \"today\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import re\n",
    "import spacy\n",
    "\n",
    "def lemma_concordance(text, token, window=50):\n",
    "    print(\"Loading spaCy model...\")\n",
    "    # change to \"en_core_web_md\" for medium model,\n",
    "    # or \"en_core_web_lg\" for large--no other code here needs changing\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    print(\"Loaded.\")\n",
    "    \n",
    "    # directly get the mapping of raw form to lemma from spaCy's \n",
    "    # tokenization/stemming\n",
    "    word_finder = re.compile(r\"[A-z0-9]+\", re.MULTILINE)\n",
    "    vocab_to_stem = {\n",
    "        i.lower_:i.lemma_\n",
    "        for i in nlp(text)\n",
    "    }\n",
    "    if token.lower().strip() not in vocab_to_stem:\n",
    "        print(\"Token is not in the vocabulary.  Please try again.\")\n",
    "        return\n",
    "    # now flip the dict to {stemmed_form:{set of unstemmed form}}\n",
    "    stem_to_vocab = {i:set() for i in vocab_to_stem.values()}\n",
    "    for i in vocab_to_stem:\n",
    "        stem_to_vocab[vocab_to_stem[i]].add(i.lower())\n",
    "    \n",
    "    # Now get the stemmed form of the input token and look up\n",
    "    # the list of possible unstemmed forms--this approximates\n",
    "    # finding other inflected forms of the same word.\n",
    "    stemmed_token = vocab_to_stem[token]\n",
    "    possible_forms = stem_to_vocab[stemmed_token]\n",
    "    \n",
    "    # and now run the previous concordance function.\n",
    "    for i in possible_forms:\n",
    "        concordance(text, i, window=window)\n",
    "\n",
    "glen_carrig = open(\"glen carrig.txt\", \"r\", encoding=\"utf8\").read()\n",
    "lemma_concordance(glen_carrig, \"think\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-gram frequencies\n",
    "\n",
    "Python has a number of ways we could find n-grams.  The first is using a pre-built tool, like NLTK's ngrams() function, or a Phrases()/Phraser() combination from Gensim, which are actually used to find multi-word phrases.  Or, we could hack it together ourselves with a few lines of code.\n",
    "\n",
    "First, let's hack it together ourselves.  Then we'll print out the top most common N-grams and plot the frequencies by rank (on a logarithmic scale, naturally.  We're not monsters, after all).  We'll use spaCy's tokenization for maximum accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from collections import Counter\n",
    "from pprint import pprint\n",
    "import re\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "\n",
    "# Change this number manually to change the N in the ngrams\n",
    "NGRAM_N = 2\n",
    "\n",
    "# change the default matplotlib figure size for Jupyter's sake\n",
    "mpl.rcParams[\"figure.figsize\"] = (10,10)\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = open(\"glen carrig.txt\", \"r\", encoding=\"utf8\").read()\n",
    "# clean up line breaks and other whitespace issues\n",
    "doc = re.sub(r\"\\s+\", \" \", doc)\n",
    "doc = nlp(doc)\n",
    "# replace tokens with lowercase form\n",
    "doc = [i.lower_ for i in doc]\n",
    "# find n-grams--represent them as plain old strings\n",
    "ngrams = [\n",
    "    \"_\".join(doc[i:i+NGRAM_N]) \n",
    "    for i in range(0, len(doc) - NGRAM_N)\n",
    "]\n",
    "ngrams = Counter(ngrams)\n",
    "# do some prettier formatting than the default printing\n",
    "print(f\"{'NGRAM':<30s}\\tCOUNT\")\n",
    "for i in ngrams.most_common(20):\n",
    "    print(f\"{i[0]:<30s}\\t{i[1]}\")\n",
    "    \n",
    "# Now, let's just plot the counts by rank.\n",
    "counts = sorted(ngrams.values(), reverse=True)\n",
    "plt.plot(counts)\n",
    "plt.yscale(\"log\")\n",
    "plt.xscale(\"log\")\n",
    "plt.title(\"Look at this beautiful Zipf distribution!\")\n",
    "plt.xlabel(\"Rank (log)\")\n",
    "plt.ylabel(\"Count (log)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK's ngrams() function will find ngrams for us, like we just did by hand:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from collections import Counter\n",
    "from pprint import pprint\n",
    "import re\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk import ngrams\n",
    "import spacy\n",
    "\n",
    "# Change this number manually to change the N in the ngrams\n",
    "NGRAM_N = 2\n",
    "\n",
    "# change the default matplotlib figure size for Jupyter's sake\n",
    "mpl.rcParams[\"figure.figsize\"] = (10,10)\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = open(\"glen carrig.txt\", \"r\", encoding=\"utf8\").read()\n",
    "# clean up line breaks and other whitespace issues\n",
    "doc = re.sub(r\"\\s+\", \" \", doc)\n",
    "doc = nlp(doc)\n",
    "# replace tokens with lowercase form\n",
    "doc = [i.lower_ for i in doc]\n",
    "# This ie the only line that changed with NLTK imported\n",
    "ngrams = ngrams(doc, NGRAM_N)\n",
    "ngrams = Counter(ngrams)\n",
    "# do some prettier formatting than the default printing\n",
    "print(f\"{'NGRAM':<30s}\\tCOUNT\")\n",
    "for i in ngrams.most_common(20):\n",
    "    # okay, so the format string changes a little too\n",
    "    print(f\"{'_'.join(i[0]):<30s}\\t{i[1]}\")\n",
    "    \n",
    "# Now, let's just plot the counts by rank.\n",
    "counts = sorted(ngrams.values(), reverse=True)\n",
    "plt.plot(counts)\n",
    "plt.yscale(\"log\")\n",
    "plt.xscale(\"log\")\n",
    "plt.title(\"Look at this beautiful Zipf distribution!\")\n",
    "plt.xlabel(\"Rank (log)\")\n",
    "plt.ylabel(\"Count (log)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phrase-finding/collocation analysis\n",
    "\n",
    "Ngrams are fun and all, but what if you want to find multi-word phrases that appear more often than they should by random chance alone (i.e., collocates)?  Well, as before, we could hack a bit of code together, or we could use a pre-built tool from the amazing Gensin library: the [Phrasing tools!](https://radimrehurek.com/gensim/models/phrases.html)  These tools let you scan a(n already processed) corpus of texts, and finds bigrams that are collocated more than the raw prior distributions would indicate.  Then, these tools let you transform your original corpus, replacing these bigrams with a single token.  You can repeat this process all you want to find arbitrarily long phrases.\n",
    "\n",
    "Before doing this, we should run our text through a basic preprocessing pipeline in Gensim.  We'll revisit this in a bit more detail later to talk about what it does; for now, just know that it automates a lot of the basic preprocessing steps for us, like lowercasing, removing stopwords, stemming, etc.\n",
    "\n",
    "We'll use all the default values for our phrasing models except for the threshold (to guarnatee we find at least _some_ phrases for this demo), but they'll be provided explicitly to show how much customization there is.  Note that the phraser expectes a _list of sentences_, i.e. a _list of lists of words._  We don't strictly need to make them the actual sentences; the only reason Gensim says to use sentences is to avoid collocation across sentence boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from gensim.models.phrases import Phrases\n",
    "from gensim.parsing.preprocessing import preprocess_string\n",
    "\n",
    "doc = open(\"glen carrig.txt\", \"r\", encoding=\"utf8\").read()\n",
    "# clean up line breaks and other whitespace issues\n",
    "doc = re.sub(r\"\\s+\", \" \", doc)\n",
    "doc = preprocess_string(doc)\n",
    "phrasing = Phrases(\n",
    "    [doc],\n",
    "    min_count=5,\n",
    "    threshold=10,\n",
    "    max_vocab_size=40000000,\n",
    "    delimiter=b\"_\", # this has to be a byte string--just a quirk of this model\n",
    "    progress_per=10000,\n",
    "    scoring=\"default\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can look at some of the phrases that our phrase model discovered:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "found_phrases = list(phrasing.export_phrases([doc]))\n",
    "pprint(found_phrases[:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, we can transform our original document(s), replacing all of the discovered bigrams with a single token (e.g., `[\"the\", boat\"]` --> `\"the_boat\"`).  Gensim likes to use the indexing syntax to do transformations--it's a bit weird but you get used to it.\n",
    "\n",
    "Note that we'll get a warning from Gensim (warnings are not errors--they're more of a \"heads up, something looks weird here\" sort of notice).  Gensim also has Phraser() objects, which are initialized from a Phrases() object, and are much faster at transforming a corpus.  This only really matters when you're dealing with _massive_ corpora and datasets; for our single book, we don't really need to bother, but I'll show how it would be done anyways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phraser\n",
    "\n",
    "# transform the text with the original phrases object...\n",
    "phrased_doc = phrasing[doc]\n",
    "print(phrased_doc[500:550])\n",
    "\n",
    "# ...or by creating a new Phraser() from it first.\n",
    "phraser = Phraser(phrasing)\n",
    "phrased_doc = phraser[doc]\n",
    "print('\\n\\n', phrased_doc[500:550])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(as you can see, the results of Phrases() and Phraser() are the same--Phraser() will just be _much_ faster, and use much less memory, for very large phrasing passes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some more fun demos: Natural Language Processing 101"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting our data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous demos were very simple (even simplistic) and don't really leverage all the cool stuff Python--or programming in general--can do for you.  Let's work with a non-trivial dataset now and do some more NLP-like work.  We'll use the [Blog Authorship Corpus](http://u.cs.biu.ac.il/~koppel/BlogCorpus.htm).  Download it to the same folder as this notebook, then unzip it into a folder names \"blogs\" (case-sensitive!).\n",
    "\n",
    "Each author's blog posts are stored as a .xml file, named in the following format: \n",
    "\n",
    "`[ID number].[gender].[age].[industry of employment].[astrological sign].xml`\n",
    "\n",
    "And they contain blog data that looks like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```<Blog>\n",
    "\n",
    "<date>14,May,2004</date>\n",
    "<post>\n",
    "\n",
    " \n",
    "      why  i feel really empty and disappointed today.. i hate the teachers. i hate the stress i have to accept. i hate i'am so weak... i hate no one understand.\n",
    "     \n",
    "\n",
    "    \n",
    "</post>\n",
    "</Blog>```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First thing's first: we need to deal with the XML formatting.  Fortunately, Python has some excellent tools for that, e.g. `xml.etree.ElementTree`.  We'll also want to use a better data structure to represent this text.  There's an absolutely indispensible library called Pandas, which gives you R-style dataframes to work with (and Pandas is probably the single biggest reason that Python has taken over the data science world, demoting R to second-place)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['blogs\\\\1000331.female.37.indUnk.Leo.xml',\n",
       " 'blogs\\\\1000866.female.17.Student.Libra.xml',\n",
       " 'blogs\\\\1004904.male.23.Arts.Capricorn.xml',\n",
       " 'blogs\\\\1005076.female.25.Arts.Cancer.xml',\n",
       " 'blogs\\\\1005545.male.25.Engineering.Sagittarius.xml']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "files = [i.path for i in os.scandir(\"blogs\")]\n",
    "files[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a first pass where we just try to parse each file, to make sure there are no problems.  (Data validations is a step you _absolutely do not skip_ if you're doing any real data work, after all!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EXCEPTION ON FILE: blogs\\1000866.female.17.Student.Libra.xml\n",
      "EXCEPTION: not well-formed (invalid token): line 103, column 225\n"
     ]
    }
   ],
   "source": [
    "from xml.etree import ElementTree\n",
    "\n",
    "for i in files:\n",
    "    try:\n",
    "        ElementTree.parse(i)\n",
    "    except Exception as e:\n",
    "        print(\"\\nEXCEPTION ON FILE:\", i)\n",
    "        print(\"EXCEPTION:\", e)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uh-oh.  Let's take a look at the file that broke and go see where the problem is.  Fortunately, the error above gave us a line _and_ a column number, so we can go to the exact location where an issue was encountered.\n",
    "\n",
    "```\n",
    "<Blog>\n",
    "[...]\n",
    "\n",
    "<date>14,January,2003</date>\n",
    "<post>\n",
    "\n",
    "\n",
    "      Hehe, just finished dinner! Yum! I'm so happy right now. I don't even know why, I just...am! I'm talking to a bunch of my friends while writing this, which is always fun. Plus I'm doing homework, PLUS I'm watching Law & Order...how massively talented am I? Well, my day was pretty kick butt. Umm, no band OR music theory...very cool, so Kristen and I sat together and did homework and discussed Winter. Next period I hung out with Chris and Kelly for a bit (Alex too for a bit, since he was in Gym) and I left eventually. They started working on music, and...I just always end up feeling left out when they do, so, I try to stay away from those two together in general.Oh well, I went and sat alone in a practice room. Darn...no good stories for the newspaper to write about me. That was a great story, no matter how angry people are about the band comment. I wish people would have read the story actually, instead of reaching paragraph two and deciding it was terrible. Well, anyway, umm...EPVM was interesting. I'm getting nervous about my final. My brother said it was difficult...my brother with the perfect ACT AND SAT scores...arg. I just...wow, I'm so afraid of that test. I'm completely going to fail. Gym...oh jeez...two words  Commando Crawl  OWWWWWWWWWWWWWW  Good lord...that was one of the most painful...oh jeez...just thinking about it. Seriously, if you ever by some chance do that for high ropes...WEAR PANTS! Well, yes...wear pants normally, but don't wear shorts, make sure they are pants, because...it's quite painful if you don't. I made it through though, so it's all good! It just hurt...a lot.   Math, boring, shock shock.  Intervening thought: Why do I always write these while talking to Alex??  Lunch...was interesting. Just hung out with Kelly and Emily some, then Chris and Alex. It was cool...not much to it. Comparative Religion was boring...just, meh, presenting projects...and finally chem...boring, shock shock, and then, I came home, and did stuff, and that is the end of my day...therefore, I shall leave, and go kill alex...I mean...wait...you know NOTHING. You have no evidence...:)\n",
    "\n",
    "</post>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Column numbers aren't displayed in this notebook, but the error is at the ampersand in `Law & Order`.  As it turns out, ampersands are special characters in XML code, and need to be escaped specially.  We could do some manual replacement of characters with the appropriate XML escapes, but that sounds like a lot of work and a lot of room for error.  So instead, let's just use an HTML parser, which I've already tested and can verify does in fact work.\n",
    "\n",
    "Fortunately, our document structure is so simple that we can just hack this together with regular expressions.  This will be fast, but **is not** how we should in general deal with problematic XML or other markup files--we'd want to use some of Python's more rudimentary tools, like the base HTML or XML parsers, and overwrite their functionality (e.g. by subclassing).\n",
    "\n",
    "We'll create a list of dictionaries (think JSONs), which we can easily pass into Pandas to make a big, beautiful, glorious Dataframe object (which we'll save to a .csv so we can re-open it directly later).  We'll work from this Dataframe for the rest of this demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a72527b271641989ef20607c19bb6a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Generating Dataframe from blog posts...', max=19320), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Casting Date column to datetime format...\n",
      "Saving blog dataframe to Blog Data.csv...\n",
      "Wall time: 2min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import os\n",
    "import re\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# pre-compile patterns we'll use a lot--for speed\n",
    "date_finder = re.compile(r\"^<date>(.*?)</date>$\", re.MULTILINE)\n",
    "post_finder = re.compile(r\"^<post>$(.*?)^</post>$\", re.MULTILINE|re.DOTALL)\n",
    "whitespace_cleaner = re.compile(r\"\\s+\", re.MULTILINE)\n",
    "\n",
    "def process_file(infile):\n",
    "    # get the user metadata from the filename\n",
    "    metadata = os.path.split(infile)[-1].split(\".\")\n",
    "    text = open(infile, \"r\", encoding=\"ISO-8859-1\").read()\n",
    "    dates = date_finder.findall(text)\n",
    "    posts = post_finder.findall(text)\n",
    "    # assert check will crash our program if it fails--use to make sure our approach works!\n",
    "    assert len(dates) == len(posts)\n",
    "    blog_data = [\n",
    "        {\n",
    "             \"Author ID\" : metadata[0],\n",
    "             \"Gender\"    : metadata[1],\n",
    "             \"Age\"       : int(metadata[2]),\n",
    "             \"Industry\"  : metadata[3],\n",
    "             \"Sign\"      : metadata[4],\n",
    "             \"Date\"      : dates[i],\n",
    "             # all whitespaces to a single space.\n",
    "             \"Post\"      : whitespace_cleaner.sub(\" \", posts[i])\n",
    "        }\n",
    "        for i in range(len(dates))\n",
    "    ]\n",
    "    \n",
    "    return blog_data\n",
    "\n",
    "blog_dataframe = pd.concat(\n",
    "    pd.DataFrame(process_file(i))\n",
    "    for i in tqdm(files, desc=\"Generating Dataframe from blog posts...\")\n",
    ")\n",
    "print(\"Casting Date column to datetime format...\")\n",
    "print(\"Saving blog dataframe to Blog Data.csv...\")\n",
    "blog_dataframe.to_csv(\"Blog Data.csv\", index=False)\n",
    "blog_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of entries in this dataframe.  And the dataframe itself is 765MB on my machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of posts:             681,288\n",
      "Number of authors:           19,320\n",
      "Approximate number of words: 136,854,709\n"
     ]
    }
   ],
   "source": [
    "num_words = sum(len(i.split()) for i in blog_dataframe[\"Post\"])\n",
    "num_authors = blog_dataframe['Author ID'].nunique()\n",
    "print(f\"Number of posts:             {blog_dataframe.shape[0]:,}\")\n",
    "print(f\"Number of authors:           {num_authors:,}\")\n",
    "print(f\"Approximate number of words: {num_words:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's look at some of the ways we can process this text with various libraries.  We'll use the very first blog post as an working example to show what some of these processes do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Well, everyone got up and going this morning. It's still raining, but that's okay with me. Sort of suits my mood. I could easily have stayed home in bed with my book and the cats. This has been a lot of rain though! People have wet basements, there are lakes where there should be golf courses and fields, everything is green, green, green. But, it is supposed to be 26 degrees by Friday, so we'll be dealing with mosquitos next week. I heard Winnipeg described as an \"Old Testament\" city on urlLink CBC Radio One last week and it sort of rings true. Floods, infestations, etc., etc.. \n"
     ]
    }
   ],
   "source": [
    "demo_post = list(blog_dataframe[\"Post\"])[0]\n",
    "print(demo_post)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For any NLP or data mining tasks with language, we want to reduce the complexity and dimensionality of the text.  This in turn reduces the _sparsity_ of our data when we get to whatever modeling work we want to do with it, which means our models will be less likely to overfit to our data.\n",
    "\n",
    "Most text mining is primarily centered around the _content_ of the text, moreso than it _structure_ or _form_.  As such, most preprocessing pipelines will focus on preserving the semantic content over other elements of the text.  Some common preprocessing steps:\n",
    "* Convert to lowercase\n",
    "* (Sometimes, but not always) Remove accented characters\n",
    "* Tokenize, i.e. split a doucment into a list of tokens (words, punctuation, etc)\n",
    "* Remove tokens:\n",
    "  * _Stopwords_ (generally, _function_ words), e.g. \"the\", \"a\", \"to\", etc.\n",
    "  * Words with very low frequencies (contain very little information, and are not useful)\n",
    "  * Words with extremely high _document-wise_ frequencies (these don't help us discriminate between documents in our corpus)\n",
    "* Stem, or lemmatize, the text (not always--depends on the analysis!)\n",
    "  * Stemming is significantly faster, but lemmatization can be more accurate.\n",
    "  * But for downstream tasks and analysis, both stemming and lemmatization tend to perform about the same.\n",
    "* Generate a _vectorized_ representation of the text--e.g. Bag-of-Words, Word2Vec, Doc2Vec, GloVe, etc.\n",
    "\n",
    "For other analyses, though, we might be interested in analyzing the structure elements of the text:\n",
    "* Identifying (named) entities\n",
    "* Identifying noun chunks (more or less NPs/DPs)\n",
    "* Syntactic parsing (dependency and constituent parsing are most common)\n",
    "* Part-of-speech tagging\n",
    "\n",
    "As for the actual analyses we might do, they are many:\n",
    "* Topic modeling\n",
    "* Sentiment analysis\n",
    "* Document scoring/classification (e.g., author identification)\n",
    "\n",
    "The rest of this notebook is a whirlwind tour through some of these capabilities in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "We can use the two excellent libraries we've already seen: spaCy and Gensim.\n",
    "\n",
    "Gensim is a much _faster_ library for preprocessing, since it only operates based on raw string patterns and is designed to be fast and scale to massive datasets.  There is the assumption that any error we induce through the very simplistic approaches will be balanced out by the amount of data we work with--generally a good, and correct, assumption.  Gensim does no syntactic parsing, POS tagging, or other such structure-related analysis, but it's not designed for that.\n",
    "\n",
    "spaCy is the much _more accurate_ library, since it parses text based on large, powerful pre-trained models (think Stanford's CoreNLP toolkit--it's very much analogous to that).  While still very fast, spaCy is painfully slow compared to Gensim.  But, it has a far more robust tokenizer, it can do part-of-speech tagging, lemmatization, dependency parsing, entity and noun chunk identification, and it even has pre-trained word vectors (and can easily compute vectors for strings of words or entire documents).\n",
    "\n",
    "Let's first look at Gensim's preprocessing.  There's one function--`gensim.parsing.preprocessing.preprocess_string()`--which encompasses all the basic functions we need: lowercasing, de-accenting, tokenizing, stemming (with the Porter stemmer), stopword removal, removal of numbers, and removal of very short words (which are generally noise to use)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original post:\n",
      " Well, everyone got up and going this morning. It's still raining, but that's okay with me. Sort of suits my mood. I could easily have stayed home in bed with my book and the cats. This has been a lot of rain though! People have wet basements, there are lakes where there should be golf courses and fields, everything is green, green, green. But, it is supposed to be 26 degrees by Friday, so we'll be dealing with mosquitos next week. I heard Winnipeg described as an \"Old Testament\" city on urlLink CBC Radio One last week and it sort of rings true. Floods, infestations, etc., etc.. \n",
      "\n",
      "After Gensim preprocessing:\n",
      "got go morn rain okai sort suit mood easili stai home bed book cat lot rain peopl wet basement lake golf cours field green green green suppos degre fridai deal mosquito week heard winnipeg describ old testament citi urllink cbc radio week sort ring true flood infest\n"
     ]
    }
   ],
   "source": [
    "from gensim.parsing.preprocessing import preprocess_string\n",
    "\n",
    "print(\"Original post:\")\n",
    "print(demo_post)\n",
    "\n",
    "print(\"\\nAfter Gensim preprocessing:\")\n",
    "print(\" \".join(preprocess_string(demo_post)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the Porter Stemmer finds the uninflected forms of words.  It bases its processing _purely_ on the _letters of a word_.  This makes it fast, but it doesn't always give real words or the most correct forms, e.g. `\"morning\"` --> `\"morn\"`, and `\"okay\"` --> `\"okai\"`.  But, for most text mining or NLP tasks, this is actually not that big of an issue, and later processing steps would filter out any weird edge cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spaCy\n",
    "\n",
    "spaCy requires us to load models in, as we saw earlier when doing stem-based concordancing.  As before, we'll use their small English model, but we would just change the model name in `spacy.load()` if we wanted a different model. The small model will generally be a bit faster, which is all we need for this demo's purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original post:\n",
      " Well, everyone got up and going this morning. It's still raining, but that's okay with me. Sort of suits my mood. I could easily have stayed home in bed with my book and the cats. This has been a lot of rain though! People have wet basements, there are lakes where there should be golf courses and fields, everything is green, green, green. But, it is supposed to be 26 degrees by Friday, so we'll be dealing with mosquitos next week. I heard Winnipeg described as an \"Old Testament\" city on urlLink CBC Radio One last week and it sort of rings true. Floods, infestations, etc., etc.. \n",
      "\n",
      "After spaCy preprocessing:\n",
      "  well get go morning -PRON- be rain be okay sort suit mood -PRON- easily stay home bed book cat this lot rain people wet basement lake golf course field green green green but suppose 26 degree friday will deal mosquito week -PRON- hear winnipeg describe old testament city urllink cbc radio one week sort ring true flood infestation etc etc\n",
      "Wall time: 796 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "processed_post = \" \".join(\n",
    "    i.lemma_\n",
    "    for i in nlp(demo_post)\n",
    "    if i.is_stop == False\n",
    "    and i.is_punct == False\n",
    ")\n",
    "\n",
    "print(\"Original post:\")\n",
    "print(demo_post)\n",
    "\n",
    "print(\"\\nAfter spaCy preprocessing:\")\n",
    "print(processed_post)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the processed text is much more _human-readable_ with this approach (this is due to the use of a lemmatizer, rather than a stemmer).  While nice for reporting and inspecting results, the extra overhead in runtime (not evident in this small example) might make this an unreasonable proposition for large datasets if time is an issue.  And, as mentioned earlier, if you're doing an _automated analysis_ of your text later, there isn't always a big difference, if any, in how well stemming versus lemmatization performs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linguistic Analysis with spaCy\n",
    "\n",
    "spaCy's language models have a LOT of functionality.  Let's look at just some of the most easily accessible ones.\n",
    "\n",
    "First, we've already seen spaCy's ability to do lemmatization, stopword tagging, and punctuation tagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token          \tLemma          \tIs stopword?   \t Is punctuation?\n",
      "               \t               \tFalse          \tFalse\n",
      "Well           \twell           \tFalse          \tFalse\n",
      ",              \t,              \tFalse          \tTrue\n",
      "everyone       \teveryone       \tTrue           \tFalse\n",
      "got            \tget            \tFalse          \tFalse\n",
      "up             \tup             \tTrue           \tFalse\n",
      "and            \tand            \tTrue           \tFalse\n",
      "going          \tgo             \tFalse          \tFalse\n",
      "this           \tthis           \tTrue           \tFalse\n",
      "morning        \tmorning        \tFalse          \tFalse\n",
      ".              \t.              \tFalse          \tTrue\n",
      "It             \t-PRON-         \tFalse          \tFalse\n",
      "'s             \tbe             \tFalse          \tFalse\n",
      "still          \tstill          \tTrue           \tFalse\n",
      "raining        \train           \tFalse          \tFalse\n",
      ",              \t,              \tFalse          \tTrue\n",
      "but            \tbut            \tTrue           \tFalse\n",
      "that           \tthat           \tTrue           \tFalse\n",
      "'s             \tbe             \tFalse          \tFalse\n",
      "okay           \tokay           \tFalse          \tFalse\n",
      "with           \twith           \tTrue           \tFalse\n",
      "me             \t-PRON-         \tTrue           \tFalse\n",
      ".              \t.              \tFalse          \tTrue\n",
      "Sort           \tsort           \tFalse          \tFalse\n",
      "of             \tof             \tTrue           \tFalse\n",
      "suits          \tsuit           \tFalse          \tFalse\n",
      "my             \t-PRON-         \tTrue           \tFalse\n",
      "mood           \tmood           \tFalse          \tFalse\n",
      ".              \t.              \tFalse          \tTrue\n",
      "I              \t-PRON-         \tFalse          \tFalse\n",
      "could          \tcould          \tTrue           \tFalse\n",
      "easily         \teasily         \tFalse          \tFalse\n",
      "have           \thave           \tTrue           \tFalse\n",
      "stayed         \tstay           \tFalse          \tFalse\n",
      "home           \thome           \tFalse          \tFalse\n",
      "in             \tin             \tTrue           \tFalse\n",
      "bed            \tbed            \tFalse          \tFalse\n",
      "with           \twith           \tTrue           \tFalse\n",
      "my             \t-PRON-         \tTrue           \tFalse\n",
      "book           \tbook           \tFalse          \tFalse\n",
      "and            \tand            \tTrue           \tFalse\n",
      "the            \tthe            \tTrue           \tFalse\n",
      "cats           \tcat            \tFalse          \tFalse\n",
      ".              \t.              \tFalse          \tTrue\n",
      "This           \tthis           \tFalse          \tFalse\n",
      "has            \thave           \tTrue           \tFalse\n",
      "been           \tbe             \tTrue           \tFalse\n",
      "a              \ta              \tTrue           \tFalse\n",
      "lot            \tlot            \tFalse          \tFalse\n",
      "of             \tof             \tTrue           \tFalse\n",
      "rain           \train           \tFalse          \tFalse\n",
      "though         \tthough         \tTrue           \tFalse\n",
      "!              \t!              \tFalse          \tTrue\n",
      "People         \tpeople         \tFalse          \tFalse\n",
      "have           \thave           \tTrue           \tFalse\n",
      "wet            \twet            \tFalse          \tFalse\n",
      "basements      \tbasement       \tFalse          \tFalse\n",
      ",              \t,              \tFalse          \tTrue\n",
      "there          \tthere          \tTrue           \tFalse\n",
      "are            \tbe             \tTrue           \tFalse\n",
      "lakes          \tlake           \tFalse          \tFalse\n",
      "where          \twhere          \tTrue           \tFalse\n",
      "there          \tthere          \tTrue           \tFalse\n",
      "should         \tshould         \tTrue           \tFalse\n",
      "be             \tbe             \tTrue           \tFalse\n",
      "golf           \tgolf           \tFalse          \tFalse\n",
      "courses        \tcourse         \tFalse          \tFalse\n",
      "and            \tand            \tTrue           \tFalse\n",
      "fields         \tfield          \tFalse          \tFalse\n",
      ",              \t,              \tFalse          \tTrue\n",
      "everything     \teverything     \tTrue           \tFalse\n",
      "is             \tbe             \tTrue           \tFalse\n",
      "green          \tgreen          \tFalse          \tFalse\n",
      ",              \t,              \tFalse          \tTrue\n",
      "green          \tgreen          \tFalse          \tFalse\n",
      ",              \t,              \tFalse          \tTrue\n",
      "green          \tgreen          \tFalse          \tFalse\n",
      ".              \t.              \tFalse          \tTrue\n",
      "But            \tbut            \tFalse          \tFalse\n",
      ",              \t,              \tFalse          \tTrue\n",
      "it             \t-PRON-         \tTrue           \tFalse\n",
      "is             \tbe             \tTrue           \tFalse\n",
      "supposed       \tsuppose        \tFalse          \tFalse\n",
      "to             \tto             \tTrue           \tFalse\n",
      "be             \tbe             \tTrue           \tFalse\n",
      "26             \t26             \tFalse          \tFalse\n",
      "degrees        \tdegree         \tFalse          \tFalse\n",
      "by             \tby             \tTrue           \tFalse\n",
      "Friday         \tfriday         \tFalse          \tFalse\n",
      ",              \t,              \tFalse          \tTrue\n",
      "so             \tso             \tTrue           \tFalse\n",
      "we             \t-PRON-         \tTrue           \tFalse\n",
      "'ll            \twill           \tFalse          \tFalse\n",
      "be             \tbe             \tTrue           \tFalse\n",
      "dealing        \tdeal           \tFalse          \tFalse\n",
      "with           \twith           \tTrue           \tFalse\n",
      "mosquitos      \tmosquito       \tFalse          \tFalse\n",
      "next           \tnext           \tTrue           \tFalse\n",
      "week           \tweek           \tFalse          \tFalse\n",
      ".              \t.              \tFalse          \tTrue\n",
      "I              \t-PRON-         \tFalse          \tFalse\n",
      "heard          \thear           \tFalse          \tFalse\n",
      "Winnipeg       \twinnipeg       \tFalse          \tFalse\n",
      "described      \tdescribe       \tFalse          \tFalse\n",
      "as             \tas             \tTrue           \tFalse\n",
      "an             \tan             \tTrue           \tFalse\n",
      "\"              \t\"              \tFalse          \tTrue\n",
      "Old            \told            \tFalse          \tFalse\n",
      "Testament      \ttestament      \tFalse          \tFalse\n",
      "\"              \t\"              \tFalse          \tTrue\n",
      "city           \tcity           \tFalse          \tFalse\n",
      "on             \ton             \tTrue           \tFalse\n",
      "urlLink        \turllink        \tFalse          \tFalse\n",
      "CBC            \tcbc            \tFalse          \tFalse\n",
      "Radio          \tradio          \tFalse          \tFalse\n",
      "One            \tone            \tFalse          \tFalse\n",
      "last           \tlast           \tTrue           \tFalse\n",
      "week           \tweek           \tFalse          \tFalse\n",
      "and            \tand            \tTrue           \tFalse\n",
      "it             \t-PRON-         \tTrue           \tFalse\n",
      "sort           \tsort           \tFalse          \tFalse\n",
      "of             \tof             \tTrue           \tFalse\n",
      "rings          \tring           \tFalse          \tFalse\n",
      "true           \ttrue           \tFalse          \tFalse\n",
      ".              \t.              \tFalse          \tTrue\n",
      "Floods         \tflood          \tFalse          \tFalse\n",
      ",              \t,              \tFalse          \tTrue\n",
      "infestations   \tinfestation    \tFalse          \tFalse\n",
      ",              \t,              \tFalse          \tTrue\n",
      "etc            \tetc            \tFalse          \tFalse\n",
      ".              \t.              \tFalse          \tTrue\n",
      ",              \t,              \tFalse          \tTrue\n",
      "etc            \tetc            \tFalse          \tFalse\n",
      "..             \t..             \tFalse          \tTrue\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "print(f\"{'Token':<15s}\\t{'Lemma':<15s}\\t{'Is stopword?':<15s}\\t Is punctuation?\")\n",
    "for i in nlp(demo_post):\n",
    "    token = i.text\n",
    "    lemma = i.lemma_\n",
    "    is_stop = str(i.is_stop)\n",
    "    is_punct = str(i.is_punct)\n",
    "    print(f\"{token:<15s}\\t{lemma:<15s}\\t{is_stop:<15s}\\t{is_punct}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, we can also do part-of-speech tagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token          \tCoarse-grained POS\tFine-grained POS\n",
      "               \tSPACE            \t\n",
      "Well           \tINTJ             \tUH\n",
      ",              \tPUNCT            \t,\n",
      "everyone       \tNOUN             \tNN\n",
      "got            \tVERB             \tVBD\n",
      "up             \tPART             \tRP\n",
      "and            \tCCONJ            \tCC\n",
      "going          \tVERB             \tVBG\n",
      "this           \tDET              \tDT\n",
      "morning        \tNOUN             \tNN\n",
      ".              \tPUNCT            \t.\n",
      "It             \tPRON             \tPRP\n",
      "'s             \tVERB             \tVBZ\n",
      "still          \tADV              \tRB\n",
      "raining        \tVERB             \tVBG\n",
      ",              \tPUNCT            \t,\n",
      "but            \tCCONJ            \tCC\n",
      "that           \tDET              \tDT\n",
      "'s             \tVERB             \tVBZ\n",
      "okay           \tADJ              \tJJ\n",
      "with           \tADP              \tIN\n",
      "me             \tPRON             \tPRP\n",
      ".              \tPUNCT            \t.\n",
      "Sort           \tADV              \tRB\n",
      "of             \tADV              \tRB\n",
      "suits          \tNOUN             \tNNS\n",
      "my             \tADJ              \tPRP$\n",
      "mood           \tNOUN             \tNN\n",
      ".              \tPUNCT            \t.\n",
      "I              \tPRON             \tPRP\n",
      "could          \tVERB             \tMD\n",
      "easily         \tADV              \tRB\n",
      "have           \tVERB             \tVB\n",
      "stayed         \tVERB             \tVBN\n",
      "home           \tADV              \tRB\n",
      "in             \tADP              \tIN\n",
      "bed            \tNOUN             \tNN\n",
      "with           \tADP              \tIN\n",
      "my             \tADJ              \tPRP$\n",
      "book           \tNOUN             \tNN\n",
      "and            \tCCONJ            \tCC\n",
      "the            \tDET              \tDT\n",
      "cats           \tNOUN             \tNNS\n",
      ".              \tPUNCT            \t.\n",
      "This           \tDET              \tDT\n",
      "has            \tVERB             \tVBZ\n",
      "been           \tVERB             \tVBN\n",
      "a              \tDET              \tDT\n",
      "lot            \tNOUN             \tNN\n",
      "of             \tADP              \tIN\n",
      "rain           \tNOUN             \tNN\n",
      "though         \tADV              \tRB\n",
      "!              \tPUNCT            \t.\n",
      "People         \tNOUN             \tNNS\n",
      "have           \tVERB             \tVBP\n",
      "wet            \tADJ              \tJJ\n",
      "basements      \tNOUN             \tNNS\n",
      ",              \tPUNCT            \t,\n",
      "there          \tADV              \tEX\n",
      "are            \tVERB             \tVBP\n",
      "lakes          \tNOUN             \tNNS\n",
      "where          \tADV              \tWRB\n",
      "there          \tADV              \tEX\n",
      "should         \tVERB             \tMD\n",
      "be             \tVERB             \tVB\n",
      "golf           \tNOUN             \tNN\n",
      "courses        \tNOUN             \tNNS\n",
      "and            \tCCONJ            \tCC\n",
      "fields         \tNOUN             \tNNS\n",
      ",              \tPUNCT            \t,\n",
      "everything     \tNOUN             \tNN\n",
      "is             \tVERB             \tVBZ\n",
      "green          \tADJ              \tJJ\n",
      ",              \tPUNCT            \t,\n",
      "green          \tADJ              \tJJ\n",
      ",              \tPUNCT            \t,\n",
      "green          \tADJ              \tJJ\n",
      ".              \tPUNCT            \t.\n",
      "But            \tCCONJ            \tCC\n",
      ",              \tPUNCT            \t,\n",
      "it             \tPRON             \tPRP\n",
      "is             \tVERB             \tVBZ\n",
      "supposed       \tVERB             \tVBN\n",
      "to             \tPART             \tTO\n",
      "be             \tVERB             \tVB\n",
      "26             \tNUM              \tCD\n",
      "degrees        \tNOUN             \tNNS\n",
      "by             \tADP              \tIN\n",
      "Friday         \tPROPN            \tNNP\n",
      ",              \tPUNCT            \t,\n",
      "so             \tADV              \tRB\n",
      "we             \tPRON             \tPRP\n",
      "'ll            \tVERB             \tMD\n",
      "be             \tVERB             \tVB\n",
      "dealing        \tVERB             \tVBG\n",
      "with           \tADP              \tIN\n",
      "mosquitos      \tNOUN             \tNNS\n",
      "next           \tADJ              \tJJ\n",
      "week           \tNOUN             \tNN\n",
      ".              \tPUNCT            \t.\n",
      "I              \tPRON             \tPRP\n",
      "heard          \tVERB             \tVBD\n",
      "Winnipeg       \tPROPN            \tNNP\n",
      "described      \tVERB             \tVBD\n",
      "as             \tADP              \tIN\n",
      "an             \tDET              \tDT\n",
      "\"              \tPUNCT            \t``\n",
      "Old            \tPROPN            \tNNP\n",
      "Testament      \tPROPN            \tNNP\n",
      "\"              \tPUNCT            \t''\n",
      "city           \tNOUN             \tNN\n",
      "on             \tADP              \tIN\n",
      "urlLink        \tVERB             \tVB\n",
      "CBC            \tPROPN            \tNNP\n",
      "Radio          \tNOUN             \tNN\n",
      "One            \tNUM              \tCD\n",
      "last           \tADJ              \tJJ\n",
      "week           \tNOUN             \tNN\n",
      "and            \tCCONJ            \tCC\n",
      "it             \tPRON             \tPRP\n",
      "sort           \tADV              \tRB\n",
      "of             \tADV              \tRB\n",
      "rings          \tNOUN             \tNNS\n",
      "true           \tADJ              \tJJ\n",
      ".              \tPUNCT            \t.\n",
      "Floods         \tNOUN             \tNNS\n",
      ",              \tPUNCT            \t,\n",
      "infestations   \tNOUN             \tNNS\n",
      ",              \tPUNCT            \t,\n",
      "etc            \tX                \tFW\n",
      ".              \tPUNCT            \t.\n",
      ",              \tPUNCT            \t,\n",
      "etc            \tX                \tFW\n",
      "..             \tPUNCT            \t.\n"
     ]
    }
   ],
   "source": [
    "print(f\"{'TOKEN':<15s}\\t{'COARSE POS':<17s}\\t{'FINE POS'}\")\n",
    "for i in nlp(demo_post):\n",
    "    token = i.text\n",
    "    coarse = i.pos_\n",
    "    fine = i.tag_\n",
    "    print(f\"{token:<15s}\\t{coarse:<17s}\\t{fine}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entity recognition..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[this morning,\n",
      " 26 degrees,\n",
      " Friday,\n",
      " next week,\n",
      " Winnipeg,\n",
      " an \"Old Testament\",\n",
      " CBC Radio,\n",
      " One last week,\n",
      " Floods]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "ents = nlp(demo_post).ents\n",
    "pprint(list(ents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noun chunk identification..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[everyone,\n",
      " It,\n",
      " me,\n",
      " Sort of suits,\n",
      " my mood,\n",
      " I,\n",
      " bed,\n",
      " my book,\n",
      " the cats,\n",
      " a lot,\n",
      " rain,\n",
      " People,\n",
      " wet basements,\n",
      " lakes,\n",
      " golf courses,\n",
      " fields,\n",
      " everything,\n",
      " it,\n",
      " 26 degrees,\n",
      " Friday,\n",
      " we,\n",
      " mosquitos,\n",
      " I,\n",
      " Winnipeg,\n",
      " an \"Old Testament\" city,\n",
      " it,\n",
      " Floods,\n",
      " infestations]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "noun_chunks = nlp(demo_post).noun_chunks\n",
    "pprint(list(noun_chunks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, as we might suspect from the above information, spaCy also does dependency parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOKEN          \tHEAD           \tDEPENDENCY RELATION\n",
      "               \tWell           \t               \n",
      "Well           \tgot            \tintj           \n",
      ",              \tgot            \tpunct          \n",
      "everyone       \tgot            \tnsubj          \n",
      "got            \tgot            \tROOT           \n",
      "up             \tgot            \tprt            \n",
      "and            \tgot            \tcc             \n",
      "going          \tgot            \tconj           \n",
      "this           \tmorning        \tdet            \n",
      "morning        \tgoing          \tnpadvmod       \n",
      ".              \tgot            \tpunct          \n",
      "It             \training        \tnsubj          \n",
      "'s             \training        \taux            \n",
      "still          \training        \tadvmod         \n",
      "raining        \training        \tROOT           \n",
      ",              \training        \tpunct          \n",
      "but            \training        \tcc             \n",
      "that           \t's             \tnsubj          \n",
      "'s             \training        \tconj           \n",
      "okay           \t's             \tacomp          \n",
      "with           \t's             \tprep           \n",
      "me             \twith           \tpobj           \n",
      ".              \t's             \tpunct          \n",
      "Sort           \tof             \tadvmod         \n",
      "of             \tsuits          \tadvmod         \n",
      "suits          \tsuits          \tROOT           \n",
      "my             \tmood           \tposs           \n",
      "mood           \tsuits          \tdobj           \n",
      ".              \tsuits          \tpunct          \n",
      "I              \tstayed         \tnsubj          \n",
      "could          \tstayed         \taux            \n",
      "easily         \tstayed         \tadvmod         \n",
      "have           \tstayed         \taux            \n",
      "stayed         \tstayed         \tROOT           \n",
      "home           \tstayed         \tadvmod         \n",
      "in             \tstayed         \tprep           \n",
      "bed            \tin             \tpobj           \n",
      "with           \tstayed         \tprep           \n",
      "my             \tbook           \tposs           \n",
      "book           \twith           \tpobj           \n",
      "and            \tbook           \tcc             \n",
      "the            \tcats           \tdet            \n",
      "cats           \tbook           \tconj           \n",
      ".              \tstayed         \tpunct          \n",
      "This           \tbeen           \tnsubj          \n",
      "has            \tbeen           \taux            \n",
      "been           \tbeen           \tROOT           \n",
      "a              \tlot            \tdet            \n",
      "lot            \tbeen           \tattr           \n",
      "of             \tlot            \tprep           \n",
      "rain           \tof             \tpobj           \n",
      "though         \tbeen           \tadvmod         \n",
      "!              \tbeen           \tpunct          \n",
      "People         \thave           \tnsubj          \n",
      "have           \tare            \tccomp          \n",
      "wet            \tbasements      \tamod           \n",
      "basements      \thave           \tdobj           \n",
      ",              \tare            \tpunct          \n",
      "there          \tare            \texpl           \n",
      "are            \tis             \tccomp          \n",
      "lakes          \tare            \tattr           \n",
      "where          \tbe             \tadvmod         \n",
      "there          \tbe             \texpl           \n",
      "should         \tbe             \taux            \n",
      "be             \tlakes          \trelcl          \n",
      "golf           \tcourses        \tcompound       \n",
      "courses        \tbe             \tattr           \n",
      "and            \tcourses        \tcc             \n",
      "fields         \tcourses        \tconj           \n",
      ",              \tis             \tpunct          \n",
      "everything     \tis             \tnsubj          \n",
      "is             \tis             \tROOT           \n",
      "green          \tgreen          \tamod           \n",
      ",              \tgreen          \tpunct          \n",
      "green          \tgreen          \tconj           \n",
      ",              \tgreen          \tpunct          \n",
      "green          \tis             \tacomp          \n",
      ".              \tis             \tpunct          \n",
      "But            \tsupposed       \tcc             \n",
      ",              \tsupposed       \tpunct          \n",
      "it             \tsupposed       \tnsubjpass      \n",
      "is             \tsupposed       \tauxpass        \n",
      "supposed       \tsupposed       \tROOT           \n",
      "to             \tbe             \taux            \n",
      "be             \tsupposed       \txcomp          \n",
      "26             \tdegrees        \tnummod         \n",
      "degrees        \tbe             \tattr           \n",
      "by             \tbe             \tprep           \n",
      "Friday         \tby             \tpobj           \n",
      ",              \tsupposed       \tpunct          \n",
      "so             \tdealing        \tadvmod         \n",
      "we             \tdealing        \tnsubj          \n",
      "'ll            \tdealing        \taux            \n",
      "be             \tdealing        \taux            \n",
      "dealing        \tsupposed       \tconj           \n",
      "with           \tdealing        \tprep           \n",
      "mosquitos      \twith           \tpobj           \n",
      "next           \tweek           \tamod           \n",
      "week           \tdealing        \tnpadvmod       \n",
      ".              \tsupposed       \tpunct          \n",
      "I              \theard          \tnsubj          \n",
      "heard          \theard          \tROOT           \n",
      "Winnipeg       \tdescribed      \tnsubj          \n",
      "described      \theard          \tccomp          \n",
      "as             \tdescribed      \tprep           \n",
      "an             \tcity           \tdet            \n",
      "\"              \tcity           \tpunct          \n",
      "Old            \tTestament      \tnmod           \n",
      "Testament      \tcity           \tnmod           \n",
      "\"              \tcity           \tpunct          \n",
      "city           \tas             \tpobj           \n",
      "on             \tcity           \tprep           \n",
      "urlLink        \tRadio          \tcompound       \n",
      "CBC            \tRadio          \tcompound       \n",
      "Radio          \tOne            \tcompound       \n",
      "One            \ton             \tpobj           \n",
      "last           \tweek           \tamod           \n",
      "week           \tdescribed      \tnpadvmod       \n",
      "and            \theard          \tcc             \n",
      "it             \trings          \tnsubj          \n",
      "sort           \tof             \tadvmod         \n",
      "of             \trings          \tadvmod         \n",
      "rings          \theard          \tconj           \n",
      "true           \trings          \tamod           \n",
      ".              \trings          \tpunct          \n",
      "Floods         \tFloods         \tROOT           \n",
      ",              \tFloods         \tpunct          \n",
      "infestations   \tFloods         \tconj           \n",
      ",              \tinfestations   \tpunct          \n",
      "etc            \tinfestations   \tconj           \n",
      ".              \t,              \tpunct          \n",
      ",              \tetc            \tpunct          \n",
      "etc            \tFloods         \tconj           \n",
      "..             \tetc            \tpunct          \n"
     ]
    }
   ],
   "source": [
    "print(f\"{'TOKEN':<15s}\\t{'HEAD':<15s}\\t{'DEPENDENCY RELATION'}\")\n",
    "for i in nlp(demo_post):\n",
    "    token = i.text\n",
    "    head = i.head.text\n",
    "    dep = i.dep_\n",
    "    print(f\"{token:<15s}\\t{head:<15s}\\t{dep:<15s}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the built-in disiplaCy tool to generate a visualization of the dependency parse (though only of the first sentence, for space's sake):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"0\" class=\"displacy\" width=\"1450\" height=\"399.5\" style=\"max-width: none; height: 399.5px; color: #000000; background: #ffffff; font-family: Arial\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Well,</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">INTJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">everyone</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">got</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">up</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">PART</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">and</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">CCONJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">going</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">this</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">morning.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-0\" stroke-width=\"2px\" d=\"M70,264.5 C70,89.5 395.0,89.5 395.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-0\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">intj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,266.5 L62,254.5 78,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-1\" stroke-width=\"2px\" d=\"M245,264.5 C245,177.0 390.0,177.0 390.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-1\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,266.5 L237,254.5 253,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-2\" stroke-width=\"2px\" d=\"M420,264.5 C420,177.0 565.0,177.0 565.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-2\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">prt</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M565.0,266.5 L573.0,254.5 557.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-3\" stroke-width=\"2px\" d=\"M420,264.5 C420,89.5 745.0,89.5 745.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-3\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">cc</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M745.0,266.5 L753.0,254.5 737.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-4\" stroke-width=\"2px\" d=\"M420,264.5 C420,2.0 925.0,2.0 925.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-4\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">conj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M925.0,266.5 L933.0,254.5 917.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-5\" stroke-width=\"2px\" d=\"M1120,264.5 C1120,177.0 1265.0,177.0 1265.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-5\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1120,266.5 L1112,254.5 1128,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-6\" stroke-width=\"2px\" d=\"M945,264.5 C945,89.5 1270.0,89.5 1270.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-6\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">npadvmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1270.0,266.5 L1278.0,254.5 1262.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "displacy.render(\n",
    "    nlp(\"Well, everyone got up and going this morning.\"), \n",
    "    style=\"dep\",\n",
    "    jupyter=True # to make this render correctly in the Jupyter notebook\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's more that spaCy can do, and there are other models and libraries available for doing this sort of automated parsing and annotation of text (e.g., there are interfaces to Stanford's CoreNLP suite), but spaCy is always a good bet since it's fast (for the amount of work it does), pretty accurate, easy to use, and flexible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling\n",
    "\n",
    "Topic Modeling refers to a wide variety of algorithms that are used to explore and discover \"topics\" within a corpus.  \"Topic\" is being used with a very specific meaning here--a topic is a _statistical distribution of words_.  You might already be familiar with Latent Semantic Analysis (LSA; sometimes called Latent Semantic Indexing, or LSI), which is an older model for this sort of analysis.\n",
    "\n",
    "Most modern algorithms are based on [Latent Dirichlet Allocation (LDA)](http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf), which uses word co-occurrences within documents to determine the topics.  LDA has given rise to a number of subsequent topic models: \n",
    "* Author-Topic models, which are LDA with metadata (usually, but not always, the author of a piece)\n",
    "* Dynamic topic models, which include time metadata in the modeling process\n",
    "* Hierarchical Dirichlet Process, an extension of LDA that is _nonparametric_ with regards to the number of topics (but can give less clear results in some cases).\n",
    "\n",
    "All of these models are implemented in Genim.  Due to the size of our corpus and the fact that this is a live demo, we'll use Gensim's speedier preprocessing tools to work with our data and prepate it for topic modeling.\n",
    "\n",
    "Gensim requires that the corpus be in a _bag of words_ format for topic modeling, so we'll need to quickly do the conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f148c606b460493c859790965afa1a99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Preprocessing', max=681288), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9f94ede31a040ab929e0b0658c13cb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Creating Gensim dictionary', max=681288), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Removed 547479 tokens based on frequency criteria.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51f5e77573114ad196f4e9fe19eb6ecc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Creating BoW', max=681288), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Wall time: 14min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.parsing.preprocessing import preprocess_string\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "# preprocess our corpus\n",
    "corpus = [\n",
    "    preprocess_string(i) \n",
    "    for i in tqdm(blog_dataframe[\"Post\"], desc=\"Preprocessing\")\n",
    "]\n",
    "id2word = Dictionary(tqdm(corpus, desc=\"Creating Gensim dictionary\"))\n",
    "# remove tokens with extremely high or low frequencies\n",
    "vocabsize = len(id2word)\n",
    "id2word.filter_extremes(\n",
    "    no_above=.5, # remove tokens in > 50% of the documents\n",
    "    no_below=10  # remove tokens in < 10 documents\n",
    ")\n",
    "# Reset index spacings for better efficiency\n",
    "id2word.compactify()\n",
    "print(f\"Removed {vocabsize - len(id2word)} tokens based on frequency criteria.\")\n",
    "corpus = [\n",
    "    id2word.doc2bow(i) \n",
    "    for i in tqdm(corpus, desc=\"Creating BoW\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We absolutely won't run this during this demo, but just for comparison, here's a spaCy preprocessing pipeline that does the same thing.  The only thing that changes is the first pass through the corpus--the bag-of-words steps are as before.\n",
    "\n",
    "When I ran this next cell on my computer it processed about 20 documents per second, compared to 900-1000 documents per second with the Gensim pipeline above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6135e4812044fd7b22e0e729f55afdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Preprocessing', max=681288), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-95-a01caf13a5f6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[1;32mand\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_punct\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     ]\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblog_dataframe\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Post\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Preprocessing\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m ]\n\u001b[0;32m     17\u001b[0m corpus_spacy = [\n",
      "\u001b[1;32m<ipython-input-95-a01caf13a5f6>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[1;32mand\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_punct\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     ]\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblog_dataframe\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Post\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Preprocessing\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m ]\n\u001b[0;32m     17\u001b[0m corpus_spacy = [\n",
      "\u001b[1;32mc:\\users\\andersonh\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\spacy\\language.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, text, disable)\u001b[0m\n\u001b[0;32m    339\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdisable\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    340\u001b[0m                 \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 341\u001b[1;33m             \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mproc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    342\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mnn_parser.pyx\u001b[0m in \u001b[0;36mspacy.syntax.nn_parser.Parser.__call__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mnn_parser.pyx\u001b[0m in \u001b[0;36mspacy.syntax.nn_parser.Parser.parse_batch\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mnn_parser.pyx\u001b[0m in \u001b[0;36mspacy.syntax.nn_parser.Parser.get_batch_model\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mc:\\users\\andersonh\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\thinc\\api.py\u001b[0m in \u001b[0;36mbegin_update\u001b[1;34m(self, X, drop)\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minc_layer_grad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbegin_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minc_layer_grad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mcontinue_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msgd\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\andersonh\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\thinc\\api.py\u001b[0m in \u001b[0;36mbegin_update\u001b[1;34m(seqs_in, drop)\u001b[0m\n\u001b[0;32m    278\u001b[0m         \u001b[0mlengths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mseqs_in\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    279\u001b[0m         X, bp_layer = layer.begin_update(layer.ops.flatten(seqs_in, pad=pad),\n\u001b[1;32m--> 280\u001b[1;33m                                          drop=drop)\n\u001b[0m\u001b[0;32m    281\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbp_layer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    282\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpad\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\andersonh\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\thinc\\api.py\u001b[0m in \u001b[0;36mbegin_update\u001b[1;34m(self, X, drop)\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minc_layer_grad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbegin_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minc_layer_grad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mcontinue_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msgd\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\andersonh\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\thinc\\neural\\_classes\\resnet.py\u001b[0m in \u001b[0;36mbegin_update\u001b[1;34m(self, X, drop)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mbegin_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbp_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbegin_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mresidual_bwd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msgd\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\andersonh\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\thinc\\api.py\u001b[0m in \u001b[0;36mbegin_update\u001b[1;34m(self, X, drop)\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minc_layer_grad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbegin_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minc_layer_grad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mcontinue_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msgd\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\andersonh\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\thinc\\neural\\_classes\\layernorm.py\u001b[0m in \u001b[0;36mbegin_update\u001b[1;34m(self, X, drop)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mbegin_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbackprop_child\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchild\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbegin_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[0mN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_moments\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\andersonh\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\thinc\\neural\\_classes\\maxout.py\u001b[0m in \u001b[0;36mbegin_update\u001b[1;34m(self, X__bi, drop)\u001b[0m\n\u001b[0;32m     67\u001b[0m         \u001b[0mW\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnO\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnP\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnI\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m         \u001b[0mdrop\u001b[0m \u001b[1;33m*=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop_factor\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m         \u001b[0moutput__boc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_dot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX__bi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m         \u001b[0moutput__boc\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnO\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnP\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[0moutput__boc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput__boc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput__boc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnO\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnP\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%time\n",
    "\n",
    "from gensim.corpora import Dictionary\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "corpus_spacy = [\n",
    "    [\n",
    "        i.lemma_\n",
    "        for i in nlp(j)\n",
    "        if i.is_stop == False\n",
    "        and i.is_punct == False\n",
    "    ]\n",
    "    for j in tqdm(blog_dataframe[\"Post\"], desc=\"Preprocessing\")\n",
    "]\n",
    "corpus_spacy = [\n",
    "    [\n",
    "        i.lemma_\n",
    "        for i in j\n",
    "        if i.is_stop == False\n",
    "        and i.is_punct == False\n",
    "    ]\n",
    "    for j in tqdm(nlp.pipe(blog_dataframe[\"Post\"], n_threads=3), desc=\"Preprocessing\")\n",
    "]\n",
    "\n",
    "id2word_spacy = Dictionary(tqdm(corpus_spacy, desc=\"Creating Gensim dictionary\"))\n",
    "# remove tokens with extremely high or low frequencies\n",
    "vocabsize = len(id2word)\n",
    "id2word_spacy.filter_extremes(\n",
    "    no_above=.5, # remove tokens in > 50% of the documents\n",
    "    no_below=10  # remove tokens in < 10 documents\n",
    ")\n",
    "# Reset index spacings for better efficiency\n",
    "id2word_spacy.compactify()\n",
    "print(f\"Removed {vocabsize - len(id2word)} tokens based on frequency criteria.\")\n",
    "corpus_spacy = [\n",
    "    id2word_spacy.doc2bow(i) \n",
    "    for i in tqdm(corpus_spacy, desc=\"Creating BoW\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's run some of these topic models.  We won't bother tweaking any of the default settings (except for chunksize, which should give us a bit more speed), meaning each one will search for 100 topics.  This is the most important parameter in the models, by far--and sadly, the only really good way to find a good value is to run it at a range of different topic numbers and see what gives you useful results.  You _can_ look at the _coherence_ of each topic (calculated by Gensum automatically) and use that to evaluate your model, but the be-all-end-all is the human interpretability and the usefulness of your topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running multicore LDA...\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "from gensim.models.atmodel import AuthorTopicModel\n",
    "from gensim.models.hdpmodel import HdpModel\n",
    "\n",
    "# we need a mapping of document IDs to a list of authors who wrote them,\n",
    "# for the author-topic model.  This is pretty easy.\n",
    "doc2author = dict(\n",
    "    (i, [j])\n",
    "    for i,j in enumerate(blog_dataframe[\"Author ID\"])\n",
    ")\n",
    "\n",
    "print(\"Running multicore LDA...\")\n",
    "lda = LdaMulticore(corpus, workers=3, id2word=id2word, chunksize=50000)\n",
    "print(\"Saving LDA model...\")\n",
    "lda.save(\"LDA.model\")\n",
    "print(\"Done.\")\n",
    "# print(\"Running author-topic model...\")\n",
    "# atmodel = AuthorTopicModel(corpus, id2word=id2word, doc2author=doc2author, chunksize=50000)\n",
    "# print(\"Saving Author-Topic model...\")\n",
    "# atmodel.save(\"AuthorTopic.model\")\n",
    "# print(\"Done.\")\n",
    "# # HDP is nonparametric--don't need to specify topic number\n",
    "# print(\"Running HDP model...\")\n",
    "# hdp = HdpModel(corpus, chunksize=50000)\n",
    "# print(\"Saving HDP model...\")\n",
    "# hdp.save(\"HDP.model\")\n",
    "# print(\"Done\".)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some of the LDA outputs and see if we can interpret them.  We'll look at only the top ten highest-likelihood topics, sorted by decreasing likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lda.top_topics(corpus, topn=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could do the same with the author-topic model, but what's more interesting is to pick a user or two and see what topics _they_ tend to write about.  Then we can print just those topics out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "user_id = blog_data[\"Author ID\"][0]\n",
    "for i in atmodel[user_id]:\n",
    "    topic_num = i[0]\n",
    "    topic_prob = i[1]\n",
    "    print(f\"Topic #{topic_num} with weight {topic_prob} for author {user_id}\")\n",
    "    pprint(atmodel.show_topic(topic_num, topn=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, let's look at HDP's output, again sorted by the most likely topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hdp.top_topics(corpus, topn=10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
