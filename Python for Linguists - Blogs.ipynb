{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python for Linguists\n",
    "I'm going to try something novel: giving this talk from a Jupyter notebook so I can run code on the fly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Who is this guy?\n",
    "* Henry Anderson ([henry.anderson@uta.edu](mailto:henry.anderson@uta.edu))\n",
    "* Data Scientist in the University Analytics department\n",
    "* Specialist in unstructured data (i.e., text), machine learning, and Natural Language Processing\n",
    "* First year masters student, with interests in computational social science, digital language use, and the language of online communities and networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intended Contents\n",
    "* Who stands to gain the most\n",
    "* Why consider _programming,_ generally?\n",
    "* Why consider _Python,_ specifically?\n",
    "* Some demos:\n",
    "  * Custom concordance code, with massive flexibility\n",
    "  * Quick n-gram analysis\n",
    "  * Some very cool NLP tools:\n",
    "    * Automatic dependency parsing, POS-tagging, lemmatization, tokenization, etc. (i.e., text preprocessing)\n",
    "    * Topic models\n",
    "    * Word vectors\n",
    "    * Document prediction and classification    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goals of this talk\n",
    "* Put some basic computational/programming tools on your radar.\n",
    "* Give a sense of what _type_ of work can be done with these tools.\n",
    "* Make you generally aware of the scope and nature of computational tools.\n",
    "* Give some _basic_ exposure to Python.\n",
    "\n",
    "## This talk is _not..._\n",
    "* A tutorial on Python, the dataset, or the lirbaries.\n",
    "  * That can come later, if people are interested.\n",
    "* A tutorial in the tools that "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Who this talk is for\n",
    "* Anyone who deals with _data:_ people interested in corpus work, sociolinguistics, natural language processing, digital/online language, etc.\n",
    "* Anyone interested in _computational social science_ (CSS): i.e. general social science approaches leveraging large datasets and computational horsepower.\n",
    "  * CSS is currently exploding, and is a hugely important avenue for applied social science research.\n",
    "  * CSS is also massively interdisciplinary: programming, statistics, machine learning, AI, network analysis, linguistics, sociology, psychology, etc all combine to make CSS happen.\n",
    "* If you deal mostly with theory, or are primarily an experimentalist, you probably stand to gain less from this talk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What does programming offer?\n",
    "* (Quite literally) infinite control over your data processing: you're not limited by the features someone else decided to code into their program--you can change your code up to do anything you want.\n",
    "* Scalability and automation of your data work\n",
    "  * Work with literally millions of documents and billions of words with relative ease.\n",
    "  * Automate steps from data collection through final analysis.\n",
    "* Marketable skills: even a little bit of Python, Java, or any other language can open doors in the job market.\n",
    "* You'll feel like a badass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What does _Python_ offer?\n",
    "* Free (as in speech, not beer.  But also as in beer), open-source, royalty-free.  No licenses to sign, no royalties to pay, and _essentially no restrictions_ on what you can and can't do with it.  (the [Python Software Foundation license](https://docs.python.org/3/license.html) is an extremely permissive BSD-type license)\n",
    "\n",
    "* Huge userbase that's big into Open Source and Free Software--so it's easy to find help or sample code.\n",
    "\n",
    "* Rapidly becoming _the_ language for data science, displacing even R in most applications.  (R is still dominant for raw statistics, though Python has plenty of packages that implement common statistical tests).\n",
    "  * Though, keep an eye on a different language--Julia--over the next few years.  It is truly a worthy contender, but has yet to hit version 1.0 as of this talk.\n",
    "\n",
    "* Easy-to-learn language.\n",
    "  * Great documentation and stupid amounts of free, high-quality learning resources.\n",
    "  * Among its core ideas:\n",
    "    * Code is read far more than it is written, so the language should be _human-readable._\n",
    "    * \"There should be one, and preferably only one, obvious way to do it.\"  I.e., the most straightforward approach is _usually_ the best.  (This results in a lot of people writing straightforward, fairly easy-to-follow code).\n",
    "  * Commonly taught as a first programming language, so there are LOTS of materials for eveyone from beginning programmers to seasoned professionals; the Python community is also very welcoming of newcomers.\n",
    "\n",
    "* General purpose language: can do (almost) everything you want to make it to.\n",
    "  * Compare to R, which is great for statistics, and a pain for a lot of other stuff.\n",
    "  * Or Matlab, which is great for being a broken, slow, difficult software environment, and isn't so good at being, well, good.\n",
    "    * (this has been your mandatory \"Matlab is bad\" comment)\n",
    "\n",
    "* **For linguists**: a _huge_ array of language processing functionality and libraries.\n",
    "  * [spaCy](https://spacy.io), basically a Python version of Stanford's CoreNLP toolkit (lemmatization, tokenization, dependency parsing, POS tagging, and more).\n",
    "  * [Gensim](https://radimrehurek.com/gensim/), full of topic models and pretty bleeding-edge NLP tools.\n",
    "  * [Natural Language Toolkit (NLTK)](http://www.nltk.org/), a _massive_ library that's designed to teach a lot of NLP concepts (but can be used for some serious production work too).\n",
    "  * [Tensorflow](https://www.tensorflow.org/)+[Keras](https://keras.io/), for quickly and easily building neural networks.\n",
    "  * [PyTorch](http://pytorch.org/), an up-and-coming (but extremely exciting) neural network library.\n",
    "  * [Pandas](http://pandas.pydata.org/) for R-like dataframes, statistics, and general tabular data management.\n",
    "  * [Matplotlib](https://matplotlib.org/) (and others like [Seaborn](https://seaborn.pydata.org/), [PyGal](http://www.pygal.org/en/stable/), [Bokeh](https://bokeh.pydata.org/en/latest/), ...) for high-quality, powerful data visualization.\n",
    "  * [scikit-learn](http://scikit-learn.org/stable/index.html) for non-neural machine learning (support vector machines, random forests, and a few text features like basic preprocessing)\n",
    "    * Side note, the scikit-learn [User Guides](http://scikit-learn.org/stable/user_guide.html) are an _excellent_ technical crash course in machine learning, even if you're not too interested in Python.\n",
    "  * [Networkx](https://networkx.github.io/) for performing network analysis.\n",
    "  * And dozens more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some basic demos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concordances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concordances can be done with regular expressions and a teeny tiny bit of legwork.  (By the way: if you're working with text, you have no excuse to not learn regular expressions.  But that would be another talk all unto itself).  We'll work with the text of William Hope Hodgeson's novel _[The Boats of the \"Glen Carrig\"](https://en.wikipedia.org/wiki/The_Boats_of_the_%22Glen_Carrig%22)_, a 1907 horror novel.  The text was taken [from Project Gutenberg)](http://www.gutenberg.org/ebooks/10542), with the site's boilerplate text removed from the front and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...as being extinct in that land. For, indeed, now I think of it, I can remember that the very mud from...\n",
      "...hance of another, if, indeed, we had ever need to think more of such. And then, in the middle part o...\n",
      "... at hand. Of the places on my throat he seemed to think but little, suggesting that I had been bitte...\n",
      "...nd of living creature; so that I knew not what to think, being near to doubting if I had heard aught...\n",
      "...e sounds of things crawling in the valley. Yet, I think the silences tried us the more. And so at la...\n",
      "...t desire to be together, and further than this, I think with truth I may say, we were all fierce to ...\n",
      "... under one's feet, after so long upon the sand. I think, even thus early, I had some notion of the b...\n",
      ".... Now when they saw me thus armed, they seemed to think that I intended a jest, and some of them lau...\n",
      "...failed, and by so much that it seemed hopeless to think of success; but, for all that it appeared us...\n",
      "... these human slugs bred in me; nor, could I, do I think I would; for were I successful, then would o...\n",
      "...ld fly; for it seemed so big and unwieldy. Now, I think that Jessop gathered something of our though...\n",
      "...ed not long; for we were both of us young, and, I think, even thus early we attracted one the other;...\n",
      "...wer, assuring her that no decent-minded man could think the worse of her; but that I, for my part, t...\n",
      "... heard that dozen years, and this little thing, I think, brought back more clearly to me than aught ...\n",
      "...delight of this new thing; for I had not dared to think upon that which already my heart had made bo...\n",
      "...ut it down here. Of our love one for the other, I think yet, and ponder how that mighty man, the bo'...\n",
      "Wall time: 93.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import re\n",
    "\n",
    "def concordance(text, token, window=50):\n",
    "    pattern = re.compile(r\"\\b{}\\b\".format(token.strip()), re.IGNORECASE)\n",
    "    # convert all whitespaces to single space characters\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    for i in pattern.finditer(text):\n",
    "        print(\n",
    "            \"...\",\n",
    "            text[i.start() - 50:i.start()].rjust(50, \" \"),\n",
    "            text[i.start():i.start() + 50].ljust(50, \" \"),\n",
    "            \"...\",\n",
    "            sep=\"\"\n",
    "        )\n",
    "\n",
    "glen_carrig = open(\"glen carrig.txt\", \"r\", encoding=\"utf8\").read()\n",
    "concordance(glen_carrig, \"think\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if we want to get _really_ clever, we can have our concordance function search by stemmed forms.  We'll revisit stemming in a bit more detail shortly; for now, just know that stemming is the process of determining an uninflected form of words, but it's based purely on character patterns--so each word is treated completely in isolation, with no information about parts of speech.\n",
    "\n",
    "We need to stem the original text, then search for concordances of any tokens that get stemmed to the same value as our input.  Then we run the previous concordance function on those tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...as being extinct in that land. For, indeed, now I think of it, I can remember that the very mud from...\n",
      "...hance of another, if, indeed, we had ever need to think more of such. And then, in the middle part o...\n",
      "... at hand. Of the places on my throat he seemed to think but little, suggesting that I had been bitte...\n",
      "...nd of living creature; so that I knew not what to think, being near to doubting if I had heard aught...\n",
      "...e sounds of things crawling in the valley. Yet, I think the silences tried us the more. And so at la...\n",
      "...t desire to be together, and further than this, I think with truth I may say, we were all fierce to ...\n",
      "... under one's feet, after so long upon the sand. I think, even thus early, I had some notion of the b...\n",
      ".... Now when they saw me thus armed, they seemed to think that I intended a jest, and some of them lau...\n",
      "...failed, and by so much that it seemed hopeless to think of success; but, for all that it appeared us...\n",
      "... these human slugs bred in me; nor, could I, do I think I would; for were I successful, then would o...\n",
      "...ld fly; for it seemed so big and unwieldy. Now, I think that Jessop gathered something of our though...\n",
      "...ed not long; for we were both of us young, and, I think, even thus early we attracted one the other;...\n",
      "...wer, assuring her that no decent-minded man could think the worse of her; but that I, for my part, t...\n",
      "... heard that dozen years, and this little thing, I think, brought back more clearly to me than aught ...\n",
      "...delight of this new thing; for I had not dared to think upon that which already my heart had made bo...\n",
      "...ut it down here. Of our love one for the other, I think yet, and ponder how that mighty man, the bo'...\n",
      "...eorge call out, the bo'sun bade him keep silence, thinking it was but a piece of boyish restlessness...\n",
      "...d anon the former slithering sounds. And at that, thinking a host of evil things to be upon us, I cr...\n",
      "...e complete his examination, we turned to descend, thinking that this would be the bo'sun's intention...\n",
      "... of work. Now, though I spent much of my watch in thinking over the details of my prodigious weapon,...\n",
      "... same from which the men had caught their fish--, thinking that, if Tompkins had fallen from above, ...\n",
      "...s not correct; for she proceeded to explain that, thinking they were cut off from the world for the ...\n",
      "Wall time: 234 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import re\n",
    "from gensim.parsing.preprocessing import stem_text\n",
    "\n",
    "def stem_concordance(text, token, window=50):\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    # get a unique list of all word-like tokens using a basic regex\n",
    "    word_finder = re.compile(r\"[A-z0-9]+\", re.MULTILINE)\n",
    "    vocab_to_stem = {\n",
    "        i.lower():stem_text(i)\n",
    "        for i in set(word_finder.findall(text))\n",
    "    }\n",
    "    if token.lower().strip() not in vocab_to_stem:\n",
    "        print(\"Token is not in the vocabulary.  Please try again.\")\n",
    "        return\n",
    "    # now flip the dict to {stemmed_form:{set of unstemmed form}}\n",
    "    stem_to_vocab = {i:set() for i in vocab_to_stem.values()}\n",
    "    for i in vocab_to_stem:\n",
    "        stem_to_vocab[vocab_to_stem[i]].add(i.lower())\n",
    "    # look up other tokens that have same stem as input token\n",
    "    stemmed_token = vocab_to_stem[token]\n",
    "    possible_forms = stem_to_vocab[stemmed_token]\n",
    "    # and now run the previous concordance function.\n",
    "    for i in possible_forms:\n",
    "        concordance(text, i, window=window)\n",
    "\n",
    "glen_carrig = open(\"glen carrig.txt\", \"r\", encoding=\"utf8\").read()\n",
    "stem_concordance(glen_carrig, \"think\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That only added about 0.2 seconds to the total runtime.  Nice.\n",
    "\n",
    "Now let's do it again, but with spaCy instead of Gensim.  spaCy has built-in tokenization, lemmatization, and more that are all based on large, pre-trained machine learning models.  This will give us much better accuracy--both with tokenizing and lemmatizing--but at the cost of _significantly_ higher runtime.  spaCy also has multiple models to choose from--for English, there's small, medium, and large.  The bigger the model, the better its accuracy, but also the slower it runs.  But since the interface is exactly the same, we'll use the small model for speed.\n",
    "\n",
    "We'll also revisit lemmatization in a bit more detail shortly.  The short version: it's like stemming, but it returns a valid, real word corresponding to the uninflected form of a token.  (unlike stemming, which might map \"today\" to the root form \"todai\"--lemmatization would correctly map this to \"today\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading spaCy model...\n",
      "Loaded.\n",
      "...always with me, as I went hither and thither, the thought that IT--for that is how I designated it i...\n",
      "...ow, upon our discovery of the spring, it might be thought that we should set up a shout to those upo...\n",
      "... but an excrescence upon the trunk. With a sudden thought that it would make me a curio, I reached u...\n",
      "...Now, in the haste of our leaving, he had given no thought to take them with him; yet a portion of on...\n",
      "... I would make mention here, how that I had little thought all this while for the peril of the other ...\n",
      "...ull of stagnation, having no tides; but I had not thought to come upon such an one in my wanderings;...\n",
      "...ge ago. At this suggestion, I grew full of solemn thought; for it seemed to me that we had come upon...\n",
      "...ud a cry that we were like for the moment to have thought he had seen a second demon. Yet when we ma...\n",
      "...torm ere she was caught by the weed; and then the thought came to me of the end of those who had bui...\n",
      "...at the bite of a mosquito will make; but I had no thought to blame any mosquito. Now the stumbling o...\n",
      "...oke the fungi, or uprooted them. At least, so the thought came to me. And so we made an end of our s...\n",
      "...t would take a greater while than hitherto he had thought needful. Having concluded his examination ...\n",
      "...ftly into the shadows further ahead: but I had no thought for these when I perceived that by which t...\n",
      "...pt as on the weed side. Then, having by this time thought a little upon the matter, I put it straigh...\n",
      "... not brought it to his liking. But it must not be thought that he did naught but work at the boat; f...\n",
      "... they were pursued by an enormous crab. Now I had thought the crab we had tried to capture before co...\n",
      "...ear, to place the full breakers--which we had not thought needful to carry to the new camp on accoun...\n",
      "...d the darkening horizon, and there would come the thought to me of the terror of men whose vessels h...\n",
      "...ly weed-world. And then, even as I fell upon this thought, the bo'sun clapped me upon the shoulder, ...\n",
      "..., indeed, had we been in cooler minds, we had not thought strange, seeing that she was all so shut i...\n",
      "..., we had been occupied too sincerely to have much thought to watch the hulk, which, indeed, from tha...\n",
      "...en busied; for they were become so excited at the thought of fellow creatures almost within hail, th...\n",
      "...d be able to shoot the rope to us, and at this we thought more upon his saying; for if they had such...\n",
      "...le curs; though I am happy to tell that we had no thought at this juncture but for those who were no...\n",
      "... had come no nearer to a method of rescue. Then a thought came to me (waked perchance by the mention...\n",
      "... thing which brought me a short spell of disquiet thought. It was in this wise:--I had come to that ...\n",
      "...very strange thing in such a sight, and indeed, I thought nothing more of it than to wonder what sor...\n",
      "...possessed of two tails, and further, I could have thought I perceived a flicker as of tentacles just...\n",
      "...ilst he himself tested it at all such parts as he thought in any way doubtful, and so, presently, al...\n",
      "...t of all that desolation, and then, suddenly, the thought came to me that the screaming was from the...\n",
      "...th the water. And now I was made to understand my thought of the previous night, that I had seen the...\n",
      "... I came to the fire the first, and then, a sudden thought coming to me, I thrust the point of my cut...\n",
      "...all destroyed; but it remained hid. Then a sudden thought came into my brain, and I shouted at the t...\n",
      "...production. Presently, as a result of some little thought, he brought out from the tent the long pie...\n",
      "...eople in the hulk before the evening. And, at the thought of this, we experienced a very pleasurable...\n",
      "...and put my hand upon the lesser rope; for that he thought they in the ship were anxious to haul it i...\n",
      "...he fear that it might break it, and then a second thought that something might be climbing up to us ...\n",
      "... I did, greatly excited within myself at this new thought, as, indeed, was the bo'sun himself and th...\n",
      "...mate, who was the only officer remaining to them, thought there might be good chance to heave the ve...\n",
      "...ce. Now, having tautened the rope so much as they thought proper, they left it to have its due effec...\n",
      "...ty feet above the surface, and, at that, a sudden thought came to me which sent me hastily to the bo...\n",
      "... think the worse of her; but that I, for my part, thought rather the better, seeing that I liked the...\n",
      "...horror, and the dread which had beset them at the thought that they should all of them come to their...\n",
      "...er, and had been in such interest since, that the thought of food had escaped me; for I had seen non...\n",
      "...een very much happier to have stayed, the which I thought, for a moment, had not been displeasing to...\n",
      "...nd watched; for it was very terrible, this sudden thought of failure (though it were but temporary) ...\n",
      "...nd placed fresh chafing gear about it, so that he thought it would be so safe as ever to heave upon;...\n",
      "...appy it was for all of us in the ship that he had thought to go at that moment; for the light of the...\n",
      "...as being extinct in that land. For, indeed, now I think of it, I can remember that the very mud from...\n",
      "...hance of another, if, indeed, we had ever need to think more of such. And then, in the middle part o...\n",
      "... at hand. Of the places on my throat he seemed to think but little, suggesting that I had been bitte...\n",
      "...nd of living creature; so that I knew not what to think, being near to doubting if I had heard aught...\n",
      "...e sounds of things crawling in the valley. Yet, I think the silences tried us the more. And so at la...\n",
      "...t desire to be together, and further than this, I think with truth I may say, we were all fierce to ...\n",
      "... under one's feet, after so long upon the sand. I think, even thus early, I had some notion of the b...\n",
      ".... Now when they saw me thus armed, they seemed to think that I intended a jest, and some of them lau...\n",
      "...failed, and by so much that it seemed hopeless to think of success; but, for all that it appeared us...\n",
      "... these human slugs bred in me; nor, could I, do I think I would; for were I successful, then would o...\n",
      "...ld fly; for it seemed so big and unwieldy. Now, I think that Jessop gathered something of our though...\n",
      "...ed not long; for we were both of us young, and, I think, even thus early we attracted one the other;...\n",
      "...wer, assuring her that no decent-minded man could think the worse of her; but that I, for my part, t...\n",
      "... heard that dozen years, and this little thing, I think, brought back more clearly to me than aught ...\n",
      "...delight of this new thing; for I had not dared to think upon that which already my heart had made bo...\n",
      "...ut it down here. Of our love one for the other, I think yet, and ponder how that mighty man, the bo'...\n",
      "...eorge call out, the bo'sun bade him keep silence, thinking it was but a piece of boyish restlessness...\n",
      "...d anon the former slithering sounds. And at that, thinking a host of evil things to be upon us, I cr...\n",
      "...e complete his examination, we turned to descend, thinking that this would be the bo'sun's intention...\n",
      "... of work. Now, though I spent much of my watch in thinking over the details of my prodigious weapon,...\n",
      "... same from which the men had caught their fish--, thinking that, if Tompkins had fallen from above, ...\n",
      "...s not correct; for she proceeded to explain that, thinking they were cut off from the world for the ...\n",
      "Wall time: 15.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import re\n",
    "import spacy\n",
    "\n",
    "def lemma_concordance(text, token, window=50):\n",
    "    print(\"Loading spaCy model...\")\n",
    "    # change to \"en_core_web_md\" for medium model,\n",
    "    # or \"en_core_web_lg\" for large--no other code here needs changing\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    print(\"Loaded.\")\n",
    "    \n",
    "    # directly get the mapping of raw form to lemma from spaCy's \n",
    "    # tokenization/stemming\n",
    "    word_finder = re.compile(r\"[A-z0-9]+\", re.MULTILINE)\n",
    "    vocab_to_stem = {\n",
    "        i.lower_:i.lemma_\n",
    "        for i in nlp(text)\n",
    "    }\n",
    "    if token.lower().strip() not in vocab_to_stem:\n",
    "        print(\"Token is not in the vocabulary.  Please try again.\")\n",
    "        return\n",
    "    # now flip the dict to {stemmed_form:{set of unstemmed form}}\n",
    "    stem_to_vocab = {i:set() for i in vocab_to_stem.values()}\n",
    "    for i in vocab_to_stem:\n",
    "        stem_to_vocab[vocab_to_stem[i]].add(i.lower())\n",
    "    \n",
    "    # Now get the stemmed form of the input token and look up\n",
    "    # the list of possible unstemmed forms--this approximates\n",
    "    # finding other inflected forms of the same word.\n",
    "    stemmed_token = vocab_to_stem[token]\n",
    "    possible_forms = stem_to_vocab[stemmed_token]\n",
    "    \n",
    "    # and now run the previous concordance function.\n",
    "    for i in possible_forms:\n",
    "        concordance(text, i, window=window)\n",
    "\n",
    "glen_carrig = open(\"glen carrig.txt\", \"r\", encoding=\"utf8\").read()\n",
    "lemma_concordance(glen_carrig, \"think\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-gram frequencies\n",
    "\n",
    "Python has a number of ways we could find n-grams.  The first is using a pre-built tool, like NLTK's ngrams() function, or a Phrases()/Phraser() combination from Gensim, which are actually used to find multi-word phrases.  Or, we could hack it together ourselves with a few lines of code.\n",
    "\n",
    "First, let's hack it together ourselves.  Then we'll print out the top most common N-grams and plot the frequencies by rank (on a logarithmic scale, naturally.  We're not monsters, after all).  We'll use spaCy's tokenization for maximum accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NGRAM                         \tCOUNT\n",
      ",_and                         \t1533\n",
      "of_the                        \t879\n",
      "to_the                        \t396\n",
      "the_bo'sun                    \t371\n",
      ",_the                         \t358\n",
      "in_the                        \t343\n",
      ",_i                           \t328\n",
      ";_for                         \t313\n",
      "we_had                        \t277\n",
      ",_we                          \t275\n",
      "._and                         \t272\n",
      "and_so                        \t244\n",
      ";_but                         \t243\n",
      "upon_the                      \t198\n",
      "i_had                         \t184\n",
      "it_was                        \t183\n",
      "._now                         \t164\n",
      "the_weed                      \t160\n",
      "and_,                         \t156\n",
      ",_as                          \t156\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x20f8b31dc18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from collections import Counter\n",
    "from pprint import pprint\n",
    "import re\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "\n",
    "# Change this number manually to change the N in the ngrams\n",
    "NGRAM_N = 2\n",
    "\n",
    "# change the default matplotlib figure size for Jupyter's sake\n",
    "mpl.rcParams[\"figure.figsize\"] = (10,10)\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = open(\"glen carrig.txt\", \"r\", encoding=\"utf8\").read()\n",
    "# clean up line breaks and other whitespace issues\n",
    "doc = re.sub(r\"\\s+\", \" \", doc)\n",
    "doc = nlp(doc)\n",
    "# replace tokens with lowercase form\n",
    "doc = [i.lower_ for i in doc]\n",
    "# find n-grams--represent them as plain old strings\n",
    "ngrams = [\n",
    "    \"_\".join(doc[i:i+NGRAM_N]) \n",
    "    for i in range(0, len(doc) - NGRAM_N)\n",
    "]\n",
    "ngrams = Counter(ngrams)\n",
    "# do some prettier formatting than the default printing\n",
    "print(f\"{'NGRAM':<30s}\\tCOUNT\")\n",
    "for i in ngrams.most_common(20):\n",
    "    print(f\"{i[0]:<30s}\\t{i[1]}\")\n",
    "    \n",
    "# Now, let's just plot the counts by rank.\n",
    "counts = sorted(ngrams.values(), reverse=True)\n",
    "plt.plot(counts)\n",
    "plt.yscale(\"log\")\n",
    "plt.xscale(\"log\")\n",
    "plt.title(\"Look at this beautiful Zipf distribution!\")\n",
    "plt.xlabel(\"Rank (log)\")\n",
    "plt.ylabel(\"Count (log)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK's ngrams() function will find ngrams for us, like we just did by hand:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NGRAM                         \tCOUNT\n",
      ",_and                         \t1533\n",
      "of_the                        \t879\n",
      "to_the                        \t396\n",
      "the_bo'sun                    \t371\n",
      ",_the                         \t358\n",
      "in_the                        \t343\n",
      ",_i                           \t328\n",
      ";_for                         \t313\n",
      "we_had                        \t277\n",
      ",_we                          \t275\n",
      "._and                         \t272\n",
      "and_so                        \t244\n",
      ";_but                         \t243\n",
      "upon_the                      \t198\n",
      "i_had                         \t184\n",
      "it_was                        \t183\n",
      "._now                         \t164\n",
      "the_weed                      \t160\n",
      "and_,                         \t156\n",
      ",_as                          \t156\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x20f9d15bcc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from nltk import ngrams\n",
    "\n",
    "# Change this number manually to change the N in the ngrams\n",
    "NGRAM_N = 2\n",
    "\n",
    "# change the default matplotlib figure size for Jupyter's sake\n",
    "mpl.rcParams[\"figure.figsize\"] = (10,10)\n",
    "\n",
    "ngrams = ngrams(doc, NGRAM_N)\n",
    "ngrams = Counter(ngrams)\n",
    "# do some prettier formatting than the default printing\n",
    "print(f\"{'NGRAM':<30s}\\tCOUNT\")\n",
    "for i in ngrams.most_common(20):\n",
    "    # okay, so the format string changes a little too\n",
    "    print(f\"{'_'.join(i[0]):<30s}\\t{i[1]}\")\n",
    "    \n",
    "# Now, let's just plot the counts by rank.\n",
    "counts = sorted(ngrams.values(), reverse=True)\n",
    "plt.plot(counts)\n",
    "plt.yscale(\"log\")\n",
    "plt.xscale(\"log\")\n",
    "plt.title(\"Look at this beautiful Zipf distribution!\")\n",
    "plt.xlabel(\"Rank (log)\")\n",
    "plt.ylabel(\"Count (log)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phrase-finding/collocation analysis\n",
    "\n",
    "Ngrams are fun and all, but what if you want to find multi-word phrases that appear more often than they should by random chance alone (i.e., collocates)?  Well, as before, we could hack a bit of code together, or we could use a pre-built tool from the amazing Gensin library: the [Phrasing tools!](https://radimrehurek.com/gensim/models/phrases.html)  These tools let you scan a(n already processed) corpus of texts, and finds bigrams that are collocated more than the raw prior distributions would indicate.  Then, these tools let you transform your original corpus, replacing these bigrams with a single token.  You can repeat this process all you want to find arbitrarily long phrases.\n",
    "\n",
    "Before doing this, we should run our text through a basic preprocessing pipeline in Gensim.  We'll revisit this in a bit more detail later to talk about what it does; for now, just know that it automates a lot of the basic preprocessing steps for us, like lowercasing, removing stopwords, stemming, etc.\n",
    "\n",
    "We'll use all the default values for our phrasing models except for the threshold (to guarnatee we find at least _some_ phrases for this demo), but they'll be provided explicitly to show how much customization there is.  Note that the phraser expectes a _list of sentences_, i.e. a _list of lists of words._  We don't strictly need to make them the actual sentences; the only reason Gensim says to use sentences is to avoid collocation across sentence boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from gensim.models.phrases import Phrases\n",
    "from gensim.parsing.preprocessing import preprocess_string\n",
    "\n",
    "doc = open(\"glen carrig.txt\", \"r\", encoding=\"utf8\").read()\n",
    "# clean up line breaks and other whitespace issues\n",
    "doc = re.sub(r\"\\s+\", \" \", doc)\n",
    "doc = preprocess_string(doc)\n",
    "phrasing = Phrases(\n",
    "    [doc], # phrases() expects a list of tokenized sentences/documents\n",
    "    min_count=5,\n",
    "    threshold=10,\n",
    "    max_vocab_size=40000000,\n",
    "    delimiter=b\"_\", # this has to be a byte string--just a quirk of this model\n",
    "    progress_per=10000,\n",
    "    scoring=\"default\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can look at some of the phrases that our phrase model discovered.  This will print out the phrases in the order they're found in the text, so we might see duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(b'shook head', 51.86585365853659),\n",
      " (b'shook head', 51.86585365853659),\n",
      " (b'sun bade', 21.133969389783342),\n",
      " (b'sun bade', 21.133969389783342),\n",
      " (b'main cabin', 139.67159277504103),\n",
      " (b'main cabin', 139.67159277504103),\n",
      " (b'captain cabin', 77.1869328493648),\n",
      " (b'big cabin', 33.3307210031348),\n",
      " (b'aboard hulk', 36.35042735042735),\n",
      " (b'main cabin', 139.67159277504103),\n",
      " (b'main cabin', 139.67159277504103),\n",
      " (b'tot rum', 253.1547619047619),\n",
      " (b'captain cabin', 77.1869328493648),\n",
      " (b'captain cabin', 77.1869328493648),\n",
      " (b'captain cabin', 77.1869328493648)]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "found_phrases = list(phrasing.export_phrases([doc]))\n",
    "pprint(found_phrases[:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, we can transform our original document(s), replacing all of the discovered bigrams with a single token (e.g., `[\"the\", boat\"]` --> `\"the_boat\"`).  Gensim likes to use the indexing syntax to do transformations--it's a bit weird but you get used to it.\n",
    "\n",
    "Note that we'll get a warning from Gensim (warnings are not errors--they're more of a \"heads up, something looks weird here\" sort of notice).  Gensim also has Phraser() objects, which are initialized from a Phrases() object, and are much faster at transforming a corpus.  This only really matters when you're dealing with _massive_ corpora and datasets; for our single book, we don't really need to bother, but I'll show how it would be done anyways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\andersonh\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\gensim\\models\\phrases.py:486: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['befal', 'georg', 'youngest', 'prentic', 'boi', 'seat', 'pluck', 'sleev', 'inquir', 'troubl', 'voic', 'knowledg', 'cry', 'portend', 'shook_head', 'tell', 'know', 'comfort', 'said', 'wind', 'shook_head', 'plain', 'agenc', 'stark', 'calm', 'scarc', 'end', 'remark', 'sad', 'cry', 'appear', 'come', 'far', 'creek', 'far', 'creek', 'inland', 'land', 'sea', 'fill', 'even', 'air', 'dole', 'wail', 'remark', 'curiou', 'sob', 'human', 'despair', 'cry']\n",
      "\n",
      "\n",
      " ['befal', 'georg', 'youngest', 'prentic', 'boi', 'seat', 'pluck', 'sleev', 'inquir', 'troubl', 'voic', 'knowledg', 'cry', 'portend', 'shook_head', 'tell', 'know', 'comfort', 'said', 'wind', 'shook_head', 'plain', 'agenc', 'stark', 'calm', 'scarc', 'end', 'remark', 'sad', 'cry', 'appear', 'come', 'far', 'creek', 'far', 'creek', 'inland', 'land', 'sea', 'fill', 'even', 'air', 'dole', 'wail', 'remark', 'curiou', 'sob', 'human', 'despair', 'cry']\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.phrases import Phraser\n",
    "\n",
    "# transform the text with the original phrases object...\n",
    "phrased_doc = phrasing[doc]\n",
    "print(phrased_doc[500:550])\n",
    "\n",
    "# ...or by creating a new Phraser() from it first.\n",
    "phraser = Phraser(phrasing)\n",
    "phrased_doc = phraser[doc]\n",
    "print('\\n\\n', phrased_doc[500:550])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(as you can see, the results of Phrases() and Phraser() are the same--Phraser() will just be _much_ faster, and use much less memory, for very large phrasing passes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some more fun demos: Natural Language Processing 101"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting our data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous demos were very simple (even simplistic) and don't really leverage all the cool stuff Python--or programming in general--can do for you.  Let's work with a non-trivial dataset now and do some more NLP-like work.  We'll use the [Blog Authorship Corpus](http://u.cs.biu.ac.il/~koppel/BlogCorpus.htm).  Download it to the same folder as this notebook, then unzip it into a folder names \"blogs\" (case-sensitive!).\n",
    "\n",
    "Each author's blog posts are stored as a .xml file, named in the following format: \n",
    "\n",
    "`[ID number].[gender].[age].[industry of employment].[astrological sign].xml`\n",
    "\n",
    "And they contain blog data that looks like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```<Blog>\n",
    "\n",
    "<date>14,May,2004</date>\n",
    "<post>\n",
    "\n",
    " \n",
    "      why  i feel really empty and disappointed today.. i hate the teachers. i hate the stress i have to accept. i hate i'am so weak... i hate no one understand.\n",
    "     \n",
    "\n",
    "    \n",
    "</post>\n",
    "</Blog>```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First thing's first: we need to deal with the XML formatting.  Fortunately, Python has some excellent tools for that, e.g. `xml.etree.ElementTree`.  We'll also want to use a better data structure to represent this text.  There's an absolutely indispensible library called Pandas, which gives you R-style dataframes to work with (and Pandas is probably the single biggest reason that Python has taken over the data science world, demoting R to second-place)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['blogs\\\\1000331.female.37.indUnk.Leo.xml', 'blogs\\\\1000866.female.17.Student.Libra.xml', 'blogs\\\\1004904.male.23.Arts.Capricorn.xml', 'blogs\\\\1005076.female.25.Arts.Cancer.xml', 'blogs\\\\1005545.male.25.Engineering.Sagittarius.xml']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "files = [i.path for i in os.scandir(\"blogs\")]\n",
    "print(files[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a first pass where we just try to parse each file, to make sure there are no problems.  (Data validations is a step you _absolutely do not skip_ if you're doing any real data work, after all!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EXCEPTION ON FILE: blogs\\1000866.female.17.Student.Libra.xml\n",
      "EXCEPTION: not well-formed (invalid token): line 103, column 225\n"
     ]
    }
   ],
   "source": [
    "from xml.etree import ElementTree\n",
    "\n",
    "for i in files:\n",
    "    try:\n",
    "        ElementTree.parse(i)\n",
    "    except Exception as e:\n",
    "        print(\"\\nEXCEPTION ON FILE:\", i)\n",
    "        print(\"EXCEPTION:\", e)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uh-oh.  Let's take a look at the file that broke and go see where the problem is.  Fortunately, the error above gave us a line _and_ a column number, so we can go to the exact location where an issue was encountered.\n",
    "\n",
    "```\n",
    "<Blog>\n",
    "[...]\n",
    "\n",
    "<date>14,January,2003</date>\n",
    "<post>\n",
    "\n",
    "\n",
    "      Hehe, just finished dinner! Yum! I'm so happy right now. I don't even know why, I just...am! I'm talking to a bunch of my friends while writing this, which is always fun. Plus I'm doing homework, PLUS I'm watching Law & Order...how massively talented am I? Well, my day was pretty kick butt. Umm, no band OR music theory...very cool, so Kristen and I sat together and did homework and discussed Winter. Next period I hung out with Chris and Kelly for a bit (Alex too for a bit, since he was in Gym) and I left eventually. They started working on music, and...I just always end up feeling left out when they do, so, I try to stay away from those two together in general.Oh well, I went and sat alone in a practice room. Darn...no good stories for the newspaper to write about me. That was a great story, no matter how angry people are about the band comment. I wish people would have read the story actually, instead of reaching paragraph two and deciding it was terrible. Well, anyway, umm...EPVM was interesting. I'm getting nervous about my final. My brother said it was difficult...my brother with the perfect ACT AND SAT scores...arg. I just...wow, I'm so afraid of that test. I'm completely going to fail. Gym...oh jeez...two words  Commando Crawl  OWWWWWWWWWWWWWW  Good lord...that was one of the most painful...oh jeez...just thinking about it. Seriously, if you ever by some chance do that for high ropes...WEAR PANTS! Well, yes...wear pants normally, but don't wear shorts, make sure they are pants, because...it's quite painful if you don't. I made it through though, so it's all good! It just hurt...a lot.   Math, boring, shock shock.  Intervening thought: Why do I always write these while talking to Alex??  Lunch...was interesting. Just hung out with Kelly and Emily some, then Chris and Alex. It was cool...not much to it. Comparative Religion was boring...just, meh, presenting projects...and finally chem...boring, shock shock, and then, I came home, and did stuff, and that is the end of my day...therefore, I shall leave, and go kill alex...I mean...wait...you know NOTHING. You have no evidence...:)\n",
    "\n",
    "</post>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Column numbers aren't displayed in this notebook, but the error is at the ampersand in `Law & Order`.  As it turns out, ampersands are special characters in XML code, and need to be escaped specially (in this case, as `&amp;`).  We could do some manual replacement of characters with the appropriate XML escapes, but that sounds like a lot of work and a lot of room for error.\n",
    "\n",
    "Fortunately, our document structure is so simple that we can just hack this together with regular expressions.  This will be fast, but **is not** how we should in general deal with problematic XML or other markup files--we'd want to use some of Python's more rudimentary tools, like the base HTML or XML parsers, and overwrite their functionality (e.g. by subclassing).\n",
    "\n",
    "We'll create a list of dictionaries (think JSONs), which we can easily pass into Pandas to make a big, beautiful, glorious Dataframe object (which we'll save to a .csv so we can re-open it directly later).  We'll work from this Dataframe for the rest of this demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cec7119e2f14ccd9651a4dc156400df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Generating Dataframe', max=19320), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-6:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\andersonh\\appdata\\local\\programs\\python\\python36\\lib\\threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"c:\\users\\andersonh\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tqdm\\_tqdm.py\", line 144, in run\n",
      "    for instance in self.tqdm_cls._instances:\n",
      "  File \"c:\\users\\andersonh\\appdata\\local\\programs\\python\\python36\\lib\\_weakrefset.py\", line 60, in __iter__\n",
      "    for itemref in self.data:\n",
      "RuntimeError: Set changed size during iteration\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Casting Date column to datetime format.\n",
      "Saving blog dataframe to Blog Data.csv.\n",
      "Wall time: 2min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import os\n",
    "import re\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# pre-compile patterns we'll use a lot--for speed\n",
    "date_finder = re.compile(r\"^<date>(.*?)</date>$\", re.MULTILINE)\n",
    "post_finder = re.compile(r\"^<post>$(.*?)^</post>$\", re.MULTILINE|re.DOTALL)\n",
    "whitespace_cleaner = re.compile(r\"\\s+\", re.MULTILINE)\n",
    "\n",
    "def process_file(infile):\n",
    "    # get the user metadata from the filename\n",
    "    metadata = os.path.split(infile)[-1].split(\".\")\n",
    "    text = open(infile, \"r\", encoding=\"ISO-8859-1\").read()\n",
    "    dates = date_finder.findall(text)\n",
    "    posts = post_finder.findall(text)\n",
    "    # assert check will crash our program if it fails--use to make sure our approach works!\n",
    "    assert len(dates) == len(posts)\n",
    "    blog_data = [\n",
    "        {\n",
    "             \"Author ID\" : metadata[0],\n",
    "             \"Gender\"    : metadata[1],\n",
    "             \"Age\"       : int(metadata[2]),\n",
    "             \"Industry\"  : metadata[3],\n",
    "             \"Sign\"      : metadata[4],\n",
    "             \"Date\"      : dates[i],\n",
    "             # all whitespaces to a single space.\n",
    "             \"Post\"      : whitespace_cleaner.sub(\" \", posts[i])\n",
    "        }\n",
    "        for i in range(len(dates))\n",
    "    ]\n",
    "    \n",
    "    return blog_data\n",
    "\n",
    "blog_dataframe = pd.concat(\n",
    "    pd.DataFrame(process_file(i))\n",
    "    for i in tqdm(files, desc=\"Generating Dataframe\")\n",
    ")\n",
    "print(\"Casting Date column to datetime format.\")\n",
    "print(\"Saving blog dataframe to Blog Data.csv.\")\n",
    "blog_dataframe.to_csv(\"corpus data files/Blog Data.csv\", index=False)\n",
    "blog_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of entries in this dataframe.  And the saved CSV is 765MB (!!!) on my machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of posts:             681,288\n",
      "Number of authors:           19,320\n",
      "Approximate number of words: 136,854,709\n"
     ]
    }
   ],
   "source": [
    "num_words = sum(len(i.split()) for i in blog_dataframe[\"Post\"])\n",
    "num_authors = blog_dataframe['Author ID'].nunique()\n",
    "print(f\"Number of posts:             {blog_dataframe.shape[0]:,}\")\n",
    "print(f\"Number of authors:           {num_authors:,}\")\n",
    "print(f\"Approximate number of words: {num_words:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's look at some of the ways we can process this text with various libraries.  We'll use the very first blog post as an working example to show what some of these processes do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Well, everyone got up and going this morning. It's still raining, but that's okay with me. Sort of suits my mood. I could easily have stayed home in bed with my book and the cats. This has been a lot of rain though! People have wet basements, there are lakes where there should be golf courses and fields, everything is green, green, green. But, it is supposed to be 26 degrees by Friday, so we'll be dealing with mosquitos next week. I heard Winnipeg described as an \"Old Testament\" city on urlLink CBC Radio One last week and it sort of rings true. Floods, infestations, etc., etc.. \n"
     ]
    }
   ],
   "source": [
    "demo_post = list(blog_dataframe[\"Post\"])[0]\n",
    "print(demo_post)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For any NLP or data mining tasks with language, we want to reduce the complexity and dimensionality of the text.  This in turn reduces the _sparsity_ of our data when we get to whatever modeling work we want to do with it, which means our models will be less likely to overfit to our data.\n",
    "\n",
    "Most text mining is primarily centered around the _content_ of the text, moreso than it _structure_ or _form_.  As such, most preprocessing pipelines will focus on preserving the semantic content over other elements of the text.  Some common preprocessing steps:\n",
    "* Convert to lowercase\n",
    "* (Sometimes, but not always) Remove accented characters\n",
    "* Tokenize, i.e. split a doucment into a list of tokens (words, punctuation, etc)\n",
    "* Remove tokens:\n",
    "  * _Stopwords_ (generally, _function_ words), e.g. \"the\", \"a\", \"to\", etc.\n",
    "  * Words with very low frequencies (contain very little information, and are not useful)\n",
    "  * Words with extremely high _document-wise_ frequencies (these don't help us discriminate between documents in our corpus)\n",
    "* Stem, or lemmatize, the text (not always--depends on the analysis!)\n",
    "  * Stemming is significantly faster, but lemmatization can be more accurate.\n",
    "  * But for downstream tasks and analysis, both stemming and lemmatization tend to perform about the same.\n",
    "* Generate a _vectorized_ representation of the text--e.g. Bag-of-Words, Word2Vec, Doc2Vec, GloVe, etc.\n",
    "\n",
    "For other analyses, though, we might be interested in analyzing the structure elements of the text:\n",
    "* Identifying (named) entities\n",
    "* Identifying noun chunks (more or less NPs/DPs)\n",
    "* Syntactic parsing (dependency and constituent parsing are most common)\n",
    "* Part-of-speech tagging\n",
    "\n",
    "As for the actual analyses we might do, they are many:\n",
    "* Topic modeling\n",
    "* Sentiment analysis\n",
    "* Document scoring/classification (e.g., author identification)\n",
    "\n",
    "The rest of this notebook is a whirlwind tour through some of these capabilities in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "We can use the two excellent libraries we've already seen: spaCy and Gensim.\n",
    "\n",
    "Gensim is a much _faster_ library for preprocessing, since it only operates based on raw string patterns and is designed to be fast and scale to massive datasets.  There is the assumption that any error we induce through the very simplistic approaches will be balanced out by the amount of data we work with--generally a good, and correct, assumption.  Gensim does no syntactic parsing, POS tagging, or other such structure-related analysis, but it's not designed for that.\n",
    "\n",
    "spaCy is the much _more accurate_ library, since it parses text based on large, powerful pre-trained models (think Stanford's CoreNLP toolkit--it's very much analogous to that).  While still very fast, spaCy is painfully slow compared to Gensim.  But, it has a far more robust tokenizer, it can do part-of-speech tagging, lemmatization, dependency parsing, entity and noun chunk identification, and it even has pre-trained word vectors (and can easily compute vectors for strings of words or entire documents).\n",
    "\n",
    "Let's first look at Gensim's preprocessing.  There's one function--`gensim.parsing.preprocessing.preprocess_string()`--which encompasses all the basic functions we need: lowercasing, de-accenting, tokenizing, stemming (with the Porter stemmer), stopword removal, removal of numbers, and removal of very short words (which are generally noise to use)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original post:\n",
      " Well, everyone got up and going this morning. It's still raining, but that's okay with me. Sort of suits my mood. I could easily have stayed home in bed with my book and the cats. This has been a lot of rain though! People have wet basements, there are lakes where there should be golf courses and fields, everything is green, green, green. But, it is supposed to be 26 degrees by Friday, so we'll be dealing with mosquitos next week. I heard Winnipeg described as an \"Old Testament\" city on urlLink CBC Radio One last week and it sort of rings true. Floods, infestations, etc., etc.. \n",
      "\n",
      "After Gensim preprocessing:\n",
      "got go morn rain okai sort suit mood easili stai home bed book cat lot rain peopl wet basement lake golf cours field green green green suppos degre fridai deal mosquito week heard winnipeg describ old testament citi urllink cbc radio week sort ring true flood infest\n"
     ]
    }
   ],
   "source": [
    "from gensim.parsing.preprocessing import preprocess_string\n",
    "\n",
    "print(\"Original post:\")\n",
    "print(demo_post)\n",
    "\n",
    "print(\"\\nAfter Gensim preprocessing:\")\n",
    "print(\" \".join(preprocess_string(demo_post)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the Porter Stemmer finds the uninflected forms of words.  It bases its processing _purely_ on the _letters of a word_.  This makes it fast, but it doesn't always give real words or the most correct forms, e.g. `\"morning\"` --> `\"morn\"`, and `\"okay\"` --> `\"okai\"`.  But, for most text mining or NLP tasks, this is actually not that big of an issue, and later processing steps would filter out any weird edge cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spaCy\n",
    "\n",
    "spaCy requires us to load models in, as we saw earlier when doing stem-based concordancing.  As before, we'll use their small English model, but we would just change the model name in `spacy.load()` if we wanted a different model. The small model will generally be a bit faster, which is all we need for this demo's purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original post:\n",
      " Well, everyone got up and going this morning. It's still raining, but that's okay with me. Sort of suits my mood. I could easily have stayed home in bed with my book and the cats. This has been a lot of rain though! People have wet basements, there are lakes where there should be golf courses and fields, everything is green, green, green. But, it is supposed to be 26 degrees by Friday, so we'll be dealing with mosquitos next week. I heard Winnipeg described as an \"Old Testament\" city on urlLink CBC Radio One last week and it sort of rings true. Floods, infestations, etc., etc.. \n",
      "\n",
      "After spaCy preprocessing:\n",
      "  well get go morning -PRON- be rain be okay sort suit mood -PRON- easily stay home bed book cat this lot rain people wet basement lake golf course field green green green but suppose 26 degree friday will deal mosquito week -PRON- hear winnipeg describe old testament city urllink cbc radio one week sort ring true flood infestation etc etc\n",
      "Wall time: 736 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "processed_post = \" \".join(\n",
    "    i.lemma_\n",
    "    for i in nlp(demo_post)\n",
    "    if i.is_stop == False\n",
    "    and i.is_punct == False\n",
    ")\n",
    "\n",
    "print(\"Original post:\")\n",
    "print(demo_post)\n",
    "\n",
    "print(\"\\nAfter spaCy preprocessing:\")\n",
    "print(processed_post)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the processed text is much more _human-readable_ with this approach (this is due to the use of a lemmatizer, rather than a stemmer).  While nice for reporting and inspecting results, the extra overhead in runtime (not evident in this small example) might make this an unreasonable proposition for large datasets if time is an issue.  And, as mentioned earlier, if you're doing an _automated analysis_ of your text later, there isn't always a big difference, if any, in how well stemming versus lemmatization performs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linguistic Analysis with spaCy\n",
    "\n",
    "spaCy's language models have a LOT of functionality.  Let's look at just some of the most easily accessible ones.\n",
    "\n",
    "First, we've already seen spaCy's ability to do lemmatization, stopword tagging, and punctuation tagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token          \tLemma          \tIs stopword?   \t Is punctuation?\n",
      "               \t               \tFalse          \tFalse\n",
      "Well           \twell           \tFalse          \tFalse\n",
      ",              \t,              \tFalse          \tTrue\n",
      "everyone       \teveryone       \tTrue           \tFalse\n",
      "got            \tget            \tFalse          \tFalse\n",
      "up             \tup             \tTrue           \tFalse\n",
      "and            \tand            \tTrue           \tFalse\n",
      "going          \tgo             \tFalse          \tFalse\n",
      "this           \tthis           \tTrue           \tFalse\n",
      "morning        \tmorning        \tFalse          \tFalse\n",
      ".              \t.              \tFalse          \tTrue\n",
      "It             \t-PRON-         \tFalse          \tFalse\n",
      "'s             \tbe             \tFalse          \tFalse\n",
      "still          \tstill          \tTrue           \tFalse\n",
      "raining        \train           \tFalse          \tFalse\n",
      ",              \t,              \tFalse          \tTrue\n",
      "but            \tbut            \tTrue           \tFalse\n",
      "that           \tthat           \tTrue           \tFalse\n",
      "'s             \tbe             \tFalse          \tFalse\n",
      "okay           \tokay           \tFalse          \tFalse\n",
      "with           \twith           \tTrue           \tFalse\n",
      "me             \t-PRON-         \tTrue           \tFalse\n",
      ".              \t.              \tFalse          \tTrue\n",
      "Sort           \tsort           \tFalse          \tFalse\n",
      "of             \tof             \tTrue           \tFalse\n",
      "suits          \tsuit           \tFalse          \tFalse\n",
      "my             \t-PRON-         \tTrue           \tFalse\n",
      "mood           \tmood           \tFalse          \tFalse\n",
      ".              \t.              \tFalse          \tTrue\n",
      "I              \t-PRON-         \tFalse          \tFalse\n",
      "could          \tcould          \tTrue           \tFalse\n",
      "easily         \teasily         \tFalse          \tFalse\n",
      "have           \thave           \tTrue           \tFalse\n",
      "stayed         \tstay           \tFalse          \tFalse\n",
      "home           \thome           \tFalse          \tFalse\n",
      "in             \tin             \tTrue           \tFalse\n",
      "bed            \tbed            \tFalse          \tFalse\n",
      "with           \twith           \tTrue           \tFalse\n",
      "my             \t-PRON-         \tTrue           \tFalse\n",
      "book           \tbook           \tFalse          \tFalse\n",
      "and            \tand            \tTrue           \tFalse\n",
      "the            \tthe            \tTrue           \tFalse\n",
      "cats           \tcat            \tFalse          \tFalse\n",
      ".              \t.              \tFalse          \tTrue\n",
      "This           \tthis           \tFalse          \tFalse\n",
      "has            \thave           \tTrue           \tFalse\n",
      "been           \tbe             \tTrue           \tFalse\n",
      "a              \ta              \tTrue           \tFalse\n",
      "lot            \tlot            \tFalse          \tFalse\n",
      "of             \tof             \tTrue           \tFalse\n",
      "rain           \train           \tFalse          \tFalse\n",
      "though         \tthough         \tTrue           \tFalse\n",
      "!              \t!              \tFalse          \tTrue\n",
      "People         \tpeople         \tFalse          \tFalse\n",
      "have           \thave           \tTrue           \tFalse\n",
      "wet            \twet            \tFalse          \tFalse\n",
      "basements      \tbasement       \tFalse          \tFalse\n",
      ",              \t,              \tFalse          \tTrue\n",
      "there          \tthere          \tTrue           \tFalse\n",
      "are            \tbe             \tTrue           \tFalse\n",
      "lakes          \tlake           \tFalse          \tFalse\n",
      "where          \twhere          \tTrue           \tFalse\n",
      "there          \tthere          \tTrue           \tFalse\n",
      "should         \tshould         \tTrue           \tFalse\n",
      "be             \tbe             \tTrue           \tFalse\n",
      "golf           \tgolf           \tFalse          \tFalse\n",
      "courses        \tcourse         \tFalse          \tFalse\n",
      "and            \tand            \tTrue           \tFalse\n",
      "fields         \tfield          \tFalse          \tFalse\n",
      ",              \t,              \tFalse          \tTrue\n",
      "everything     \teverything     \tTrue           \tFalse\n",
      "is             \tbe             \tTrue           \tFalse\n",
      "green          \tgreen          \tFalse          \tFalse\n",
      ",              \t,              \tFalse          \tTrue\n",
      "green          \tgreen          \tFalse          \tFalse\n",
      ",              \t,              \tFalse          \tTrue\n",
      "green          \tgreen          \tFalse          \tFalse\n",
      ".              \t.              \tFalse          \tTrue\n",
      "But            \tbut            \tFalse          \tFalse\n",
      ",              \t,              \tFalse          \tTrue\n",
      "it             \t-PRON-         \tTrue           \tFalse\n",
      "is             \tbe             \tTrue           \tFalse\n",
      "supposed       \tsuppose        \tFalse          \tFalse\n",
      "to             \tto             \tTrue           \tFalse\n",
      "be             \tbe             \tTrue           \tFalse\n",
      "26             \t26             \tFalse          \tFalse\n",
      "degrees        \tdegree         \tFalse          \tFalse\n",
      "by             \tby             \tTrue           \tFalse\n",
      "Friday         \tfriday         \tFalse          \tFalse\n",
      ",              \t,              \tFalse          \tTrue\n",
      "so             \tso             \tTrue           \tFalse\n",
      "we             \t-PRON-         \tTrue           \tFalse\n",
      "'ll            \twill           \tFalse          \tFalse\n",
      "be             \tbe             \tTrue           \tFalse\n",
      "dealing        \tdeal           \tFalse          \tFalse\n",
      "with           \twith           \tTrue           \tFalse\n",
      "mosquitos      \tmosquito       \tFalse          \tFalse\n",
      "next           \tnext           \tTrue           \tFalse\n",
      "week           \tweek           \tFalse          \tFalse\n",
      ".              \t.              \tFalse          \tTrue\n",
      "I              \t-PRON-         \tFalse          \tFalse\n",
      "heard          \thear           \tFalse          \tFalse\n",
      "Winnipeg       \twinnipeg       \tFalse          \tFalse\n",
      "described      \tdescribe       \tFalse          \tFalse\n",
      "as             \tas             \tTrue           \tFalse\n",
      "an             \tan             \tTrue           \tFalse\n",
      "\"              \t\"              \tFalse          \tTrue\n",
      "Old            \told            \tFalse          \tFalse\n",
      "Testament      \ttestament      \tFalse          \tFalse\n",
      "\"              \t\"              \tFalse          \tTrue\n",
      "city           \tcity           \tFalse          \tFalse\n",
      "on             \ton             \tTrue           \tFalse\n",
      "urlLink        \turllink        \tFalse          \tFalse\n",
      "CBC            \tcbc            \tFalse          \tFalse\n",
      "Radio          \tradio          \tFalse          \tFalse\n",
      "One            \tone            \tFalse          \tFalse\n",
      "last           \tlast           \tTrue           \tFalse\n",
      "week           \tweek           \tFalse          \tFalse\n",
      "and            \tand            \tTrue           \tFalse\n",
      "it             \t-PRON-         \tTrue           \tFalse\n",
      "sort           \tsort           \tFalse          \tFalse\n",
      "of             \tof             \tTrue           \tFalse\n",
      "rings          \tring           \tFalse          \tFalse\n",
      "true           \ttrue           \tFalse          \tFalse\n",
      ".              \t.              \tFalse          \tTrue\n",
      "Floods         \tflood          \tFalse          \tFalse\n",
      ",              \t,              \tFalse          \tTrue\n",
      "infestations   \tinfestation    \tFalse          \tFalse\n",
      ",              \t,              \tFalse          \tTrue\n",
      "etc            \tetc            \tFalse          \tFalse\n",
      ".              \t.              \tFalse          \tTrue\n",
      ",              \t,              \tFalse          \tTrue\n",
      "etc            \tetc            \tFalse          \tFalse\n",
      "..             \t..             \tFalse          \tTrue\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "print(f\"{'Token':<15s}\\t{'Lemma':<15s}\\t{'Is stopword?':<15s}\\t Is punctuation?\")\n",
    "for i in nlp(demo_post):\n",
    "    token = i.text\n",
    "    lemma = i.lemma_\n",
    "    is_stop = str(i.is_stop)\n",
    "    is_punct = str(i.is_punct)\n",
    "    print(f\"{token:<15s}\\t{lemma:<15s}\\t{is_stop:<15s}\\t{is_punct}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, we can also do part-of-speech tagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOKEN          \tCOARSE POS       \tFINE POS\n",
      "               \tSPACE            \t\n",
      "Well           \tINTJ             \tUH\n",
      ",              \tPUNCT            \t,\n",
      "everyone       \tNOUN             \tNN\n",
      "got            \tVERB             \tVBD\n",
      "up             \tPART             \tRP\n",
      "and            \tCCONJ            \tCC\n",
      "going          \tVERB             \tVBG\n",
      "this           \tDET              \tDT\n",
      "morning        \tNOUN             \tNN\n",
      ".              \tPUNCT            \t.\n",
      "It             \tPRON             \tPRP\n",
      "'s             \tVERB             \tVBZ\n",
      "still          \tADV              \tRB\n",
      "raining        \tVERB             \tVBG\n",
      ",              \tPUNCT            \t,\n",
      "but            \tCCONJ            \tCC\n",
      "that           \tDET              \tDT\n",
      "'s             \tVERB             \tVBZ\n",
      "okay           \tADJ              \tJJ\n",
      "with           \tADP              \tIN\n",
      "me             \tPRON             \tPRP\n",
      ".              \tPUNCT            \t.\n",
      "Sort           \tADV              \tRB\n",
      "of             \tADV              \tRB\n",
      "suits          \tNOUN             \tNNS\n",
      "my             \tADJ              \tPRP$\n",
      "mood           \tNOUN             \tNN\n",
      ".              \tPUNCT            \t.\n",
      "I              \tPRON             \tPRP\n",
      "could          \tVERB             \tMD\n",
      "easily         \tADV              \tRB\n",
      "have           \tVERB             \tVB\n",
      "stayed         \tVERB             \tVBN\n",
      "home           \tADV              \tRB\n",
      "in             \tADP              \tIN\n",
      "bed            \tNOUN             \tNN\n",
      "with           \tADP              \tIN\n",
      "my             \tADJ              \tPRP$\n",
      "book           \tNOUN             \tNN\n",
      "and            \tCCONJ            \tCC\n",
      "the            \tDET              \tDT\n",
      "cats           \tNOUN             \tNNS\n",
      ".              \tPUNCT            \t.\n",
      "This           \tDET              \tDT\n",
      "has            \tVERB             \tVBZ\n",
      "been           \tVERB             \tVBN\n",
      "a              \tDET              \tDT\n",
      "lot            \tNOUN             \tNN\n",
      "of             \tADP              \tIN\n",
      "rain           \tNOUN             \tNN\n",
      "though         \tADV              \tRB\n",
      "!              \tPUNCT            \t.\n",
      "People         \tNOUN             \tNNS\n",
      "have           \tVERB             \tVBP\n",
      "wet            \tADJ              \tJJ\n",
      "basements      \tNOUN             \tNNS\n",
      ",              \tPUNCT            \t,\n",
      "there          \tADV              \tEX\n",
      "are            \tVERB             \tVBP\n",
      "lakes          \tNOUN             \tNNS\n",
      "where          \tADV              \tWRB\n",
      "there          \tADV              \tEX\n",
      "should         \tVERB             \tMD\n",
      "be             \tVERB             \tVB\n",
      "golf           \tNOUN             \tNN\n",
      "courses        \tNOUN             \tNNS\n",
      "and            \tCCONJ            \tCC\n",
      "fields         \tNOUN             \tNNS\n",
      ",              \tPUNCT            \t,\n",
      "everything     \tNOUN             \tNN\n",
      "is             \tVERB             \tVBZ\n",
      "green          \tADJ              \tJJ\n",
      ",              \tPUNCT            \t,\n",
      "green          \tADJ              \tJJ\n",
      ",              \tPUNCT            \t,\n",
      "green          \tADJ              \tJJ\n",
      ".              \tPUNCT            \t.\n",
      "But            \tCCONJ            \tCC\n",
      ",              \tPUNCT            \t,\n",
      "it             \tPRON             \tPRP\n",
      "is             \tVERB             \tVBZ\n",
      "supposed       \tVERB             \tVBN\n",
      "to             \tPART             \tTO\n",
      "be             \tVERB             \tVB\n",
      "26             \tNUM              \tCD\n",
      "degrees        \tNOUN             \tNNS\n",
      "by             \tADP              \tIN\n",
      "Friday         \tPROPN            \tNNP\n",
      ",              \tPUNCT            \t,\n",
      "so             \tADV              \tRB\n",
      "we             \tPRON             \tPRP\n",
      "'ll            \tVERB             \tMD\n",
      "be             \tVERB             \tVB\n",
      "dealing        \tVERB             \tVBG\n",
      "with           \tADP              \tIN\n",
      "mosquitos      \tNOUN             \tNNS\n",
      "next           \tADJ              \tJJ\n",
      "week           \tNOUN             \tNN\n",
      ".              \tPUNCT            \t.\n",
      "I              \tPRON             \tPRP\n",
      "heard          \tVERB             \tVBD\n",
      "Winnipeg       \tPROPN            \tNNP\n",
      "described      \tVERB             \tVBD\n",
      "as             \tADP              \tIN\n",
      "an             \tDET              \tDT\n",
      "\"              \tPUNCT            \t``\n",
      "Old            \tPROPN            \tNNP\n",
      "Testament      \tPROPN            \tNNP\n",
      "\"              \tPUNCT            \t''\n",
      "city           \tNOUN             \tNN\n",
      "on             \tADP              \tIN\n",
      "urlLink        \tVERB             \tVB\n",
      "CBC            \tPROPN            \tNNP\n",
      "Radio          \tNOUN             \tNN\n",
      "One            \tNUM              \tCD\n",
      "last           \tADJ              \tJJ\n",
      "week           \tNOUN             \tNN\n",
      "and            \tCCONJ            \tCC\n",
      "it             \tPRON             \tPRP\n",
      "sort           \tADV              \tRB\n",
      "of             \tADV              \tRB\n",
      "rings          \tNOUN             \tNNS\n",
      "true           \tADJ              \tJJ\n",
      ".              \tPUNCT            \t.\n",
      "Floods         \tNOUN             \tNNS\n",
      ",              \tPUNCT            \t,\n",
      "infestations   \tNOUN             \tNNS\n",
      ",              \tPUNCT            \t,\n",
      "etc            \tX                \tFW\n",
      ".              \tPUNCT            \t.\n",
      ",              \tPUNCT            \t,\n",
      "etc            \tX                \tFW\n",
      "..             \tPUNCT            \t.\n"
     ]
    }
   ],
   "source": [
    "print(f\"{'TOKEN':<15s}\\t{'COARSE POS':<17s}\\t{'FINE POS'}\")\n",
    "for i in nlp(demo_post):\n",
    "    token = i.text\n",
    "    coarse = i.pos_\n",
    "    fine = i.tag_\n",
    "    print(f\"{token:<15s}\\t{coarse:<17s}\\t{fine}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entity recognition..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[this morning,\n",
      " 26 degrees,\n",
      " Friday,\n",
      " next week,\n",
      " Winnipeg,\n",
      " an \"Old Testament\",\n",
      " CBC Radio,\n",
      " One last week,\n",
      " Floods]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "ents = nlp(demo_post).ents\n",
    "pprint(list(ents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noun chunk identification..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[everyone,\n",
      " It,\n",
      " me,\n",
      " Sort of suits,\n",
      " my mood,\n",
      " I,\n",
      " bed,\n",
      " my book,\n",
      " the cats,\n",
      " a lot,\n",
      " rain,\n",
      " People,\n",
      " wet basements,\n",
      " lakes,\n",
      " golf courses,\n",
      " fields,\n",
      " everything,\n",
      " it,\n",
      " 26 degrees,\n",
      " Friday,\n",
      " we,\n",
      " mosquitos,\n",
      " I,\n",
      " Winnipeg,\n",
      " an \"Old Testament\" city,\n",
      " it,\n",
      " Floods,\n",
      " infestations]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "noun_chunks = nlp(demo_post).noun_chunks\n",
    "pprint(list(noun_chunks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, as we might suspect from the above information, spaCy also does dependency parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOKEN          \tHEAD           \tDEPENDENCY RELATION\n",
      "               \tWell           \t               \n",
      "Well           \tgot            \tintj           \n",
      ",              \tgot            \tpunct          \n",
      "everyone       \tgot            \tnsubj          \n",
      "got            \tgot            \tROOT           \n",
      "up             \tgot            \tprt            \n",
      "and            \tgot            \tcc             \n",
      "going          \tgot            \tconj           \n",
      "this           \tmorning        \tdet            \n",
      "morning        \tgoing          \tnpadvmod       \n",
      ".              \tgot            \tpunct          \n",
      "It             \training        \tnsubj          \n",
      "'s             \training        \taux            \n",
      "still          \training        \tadvmod         \n",
      "raining        \training        \tROOT           \n",
      ",              \training        \tpunct          \n",
      "but            \training        \tcc             \n",
      "that           \t's             \tnsubj          \n",
      "'s             \training        \tconj           \n",
      "okay           \t's             \tacomp          \n",
      "with           \t's             \tprep           \n",
      "me             \twith           \tpobj           \n",
      ".              \t's             \tpunct          \n",
      "Sort           \tof             \tadvmod         \n",
      "of             \tsuits          \tadvmod         \n",
      "suits          \tsuits          \tROOT           \n",
      "my             \tmood           \tposs           \n",
      "mood           \tsuits          \tdobj           \n",
      ".              \tsuits          \tpunct          \n",
      "I              \tstayed         \tnsubj          \n",
      "could          \tstayed         \taux            \n",
      "easily         \tstayed         \tadvmod         \n",
      "have           \tstayed         \taux            \n",
      "stayed         \tstayed         \tROOT           \n",
      "home           \tstayed         \tadvmod         \n",
      "in             \tstayed         \tprep           \n",
      "bed            \tin             \tpobj           \n",
      "with           \tstayed         \tprep           \n",
      "my             \tbook           \tposs           \n",
      "book           \twith           \tpobj           \n",
      "and            \tbook           \tcc             \n",
      "the            \tcats           \tdet            \n",
      "cats           \tbook           \tconj           \n",
      ".              \tstayed         \tpunct          \n",
      "This           \tbeen           \tnsubj          \n",
      "has            \tbeen           \taux            \n",
      "been           \tbeen           \tROOT           \n",
      "a              \tlot            \tdet            \n",
      "lot            \tbeen           \tattr           \n",
      "of             \tlot            \tprep           \n",
      "rain           \tof             \tpobj           \n",
      "though         \tbeen           \tadvmod         \n",
      "!              \tbeen           \tpunct          \n",
      "People         \thave           \tnsubj          \n",
      "have           \tare            \tccomp          \n",
      "wet            \tbasements      \tamod           \n",
      "basements      \thave           \tdobj           \n",
      ",              \tare            \tpunct          \n",
      "there          \tare            \texpl           \n",
      "are            \tis             \tccomp          \n",
      "lakes          \tare            \tattr           \n",
      "where          \tbe             \tadvmod         \n",
      "there          \tbe             \texpl           \n",
      "should         \tbe             \taux            \n",
      "be             \tlakes          \trelcl          \n",
      "golf           \tcourses        \tcompound       \n",
      "courses        \tbe             \tattr           \n",
      "and            \tcourses        \tcc             \n",
      "fields         \tcourses        \tconj           \n",
      ",              \tis             \tpunct          \n",
      "everything     \tis             \tnsubj          \n",
      "is             \tis             \tROOT           \n",
      "green          \tgreen          \tamod           \n",
      ",              \tgreen          \tpunct          \n",
      "green          \tgreen          \tconj           \n",
      ",              \tgreen          \tpunct          \n",
      "green          \tis             \tacomp          \n",
      ".              \tis             \tpunct          \n",
      "But            \tsupposed       \tcc             \n",
      ",              \tsupposed       \tpunct          \n",
      "it             \tsupposed       \tnsubjpass      \n",
      "is             \tsupposed       \tauxpass        \n",
      "supposed       \tsupposed       \tROOT           \n",
      "to             \tbe             \taux            \n",
      "be             \tsupposed       \txcomp          \n",
      "26             \tdegrees        \tnummod         \n",
      "degrees        \tbe             \tattr           \n",
      "by             \tbe             \tprep           \n",
      "Friday         \tby             \tpobj           \n",
      ",              \tsupposed       \tpunct          \n",
      "so             \tdealing        \tadvmod         \n",
      "we             \tdealing        \tnsubj          \n",
      "'ll            \tdealing        \taux            \n",
      "be             \tdealing        \taux            \n",
      "dealing        \tsupposed       \tconj           \n",
      "with           \tdealing        \tprep           \n",
      "mosquitos      \twith           \tpobj           \n",
      "next           \tweek           \tamod           \n",
      "week           \tdealing        \tnpadvmod       \n",
      ".              \tsupposed       \tpunct          \n",
      "I              \theard          \tnsubj          \n",
      "heard          \theard          \tROOT           \n",
      "Winnipeg       \tdescribed      \tnsubj          \n",
      "described      \theard          \tccomp          \n",
      "as             \tdescribed      \tprep           \n",
      "an             \tcity           \tdet            \n",
      "\"              \tcity           \tpunct          \n",
      "Old            \tTestament      \tnmod           \n",
      "Testament      \tcity           \tnmod           \n",
      "\"              \tcity           \tpunct          \n",
      "city           \tas             \tpobj           \n",
      "on             \tcity           \tprep           \n",
      "urlLink        \tRadio          \tcompound       \n",
      "CBC            \tRadio          \tcompound       \n",
      "Radio          \tOne            \tcompound       \n",
      "One            \ton             \tpobj           \n",
      "last           \tweek           \tamod           \n",
      "week           \tdescribed      \tnpadvmod       \n",
      "and            \theard          \tcc             \n",
      "it             \trings          \tnsubj          \n",
      "sort           \tof             \tadvmod         \n",
      "of             \trings          \tadvmod         \n",
      "rings          \theard          \tconj           \n",
      "true           \trings          \tamod           \n",
      ".              \trings          \tpunct          \n",
      "Floods         \tFloods         \tROOT           \n",
      ",              \tFloods         \tpunct          \n",
      "infestations   \tFloods         \tconj           \n",
      ",              \tinfestations   \tpunct          \n",
      "etc            \tinfestations   \tconj           \n",
      ".              \t,              \tpunct          \n",
      ",              \tetc            \tpunct          \n",
      "etc            \tFloods         \tconj           \n",
      "..             \tetc            \tpunct          \n"
     ]
    }
   ],
   "source": [
    "print(f\"{'TOKEN':<15s}\\t{'HEAD':<15s}\\t{'DEPENDENCY RELATION'}\")\n",
    "for i in nlp(demo_post):\n",
    "    token = i.text\n",
    "    head = i.head.text\n",
    "    dep = i.dep_\n",
    "    print(f\"{token:<15s}\\t{head:<15s}\\t{dep:<15s}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the built-in disiplaCy tool to generate a visualization of the dependency parse (though only of the first sentence, for space's sake):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"0\" class=\"displacy\" width=\"1450\" height=\"399.5\" style=\"max-width: none; height: 399.5px; color: #000000; background: #ffffff; font-family: Arial\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Well,</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">INTJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">everyone</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">got</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">up</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">PART</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">and</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">CCONJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">going</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">this</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">morning.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-0\" stroke-width=\"2px\" d=\"M70,264.5 C70,89.5 395.0,89.5 395.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-0\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">intj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,266.5 L62,254.5 78,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-1\" stroke-width=\"2px\" d=\"M245,264.5 C245,177.0 390.0,177.0 390.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-1\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,266.5 L237,254.5 253,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-2\" stroke-width=\"2px\" d=\"M420,264.5 C420,177.0 565.0,177.0 565.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-2\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">prt</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M565.0,266.5 L573.0,254.5 557.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-3\" stroke-width=\"2px\" d=\"M420,264.5 C420,89.5 745.0,89.5 745.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-3\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">cc</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M745.0,266.5 L753.0,254.5 737.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-4\" stroke-width=\"2px\" d=\"M420,264.5 C420,2.0 925.0,2.0 925.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-4\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">conj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M925.0,266.5 L933.0,254.5 917.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-5\" stroke-width=\"2px\" d=\"M1120,264.5 C1120,177.0 1265.0,177.0 1265.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-5\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1120,266.5 L1112,254.5 1128,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-6\" stroke-width=\"2px\" d=\"M945,264.5 C945,89.5 1270.0,89.5 1270.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-6\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">npadvmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1270.0,266.5 L1278.0,254.5 1262.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "displacy.render(\n",
    "    nlp(\"Well, everyone got up and going this morning.\"), \n",
    "    style=\"dep\",\n",
    "    jupyter=True # to make this render correctly in the Jupyter notebook\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's more that spaCy can do, and there are other models and libraries available for doing this sort of automated parsing and annotation of text (e.g., there are interfaces to Stanford's CoreNLP suite), but spaCy is always a good bet since it's fast (for the amount of work it does), pretty accurate, easy to use, and flexible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling\n",
    "\n",
    "Topic Modeling refers to a wide variety of algorithms that are used to explore and discover \"topics\" within a corpus.  \"Topic\" is being used with a very specific meaning here--a topic is a _statistical distribution of words_.  You might already be familiar with Latent Semantic Analysis (LSA; sometimes called Latent Semantic Indexing, or LSI), which is an older model for this sort of analysis.\n",
    "\n",
    "Most modern algorithms are based on [Latent Dirichlet Allocation (LDA)](http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf), which uses word co-occurrences within documents to determine the topics.  LDA has given rise to a number of subsequent topic models: \n",
    "* Author-Topic models, which are LDA with metadata (usually, but not always, the author of a piece)\n",
    "* Dynamic topic models, which include time metadata in the modeling process\n",
    "* Hierarchical Dirichlet Process, an extension of LDA that is _nonparametric_ with regards to the number of topics (but can give less clear results in some cases).\n",
    "\n",
    "All of these models are implemented in Genim.  Due to the size of our corpus and the fact that this is a live demo, we'll use Gensim's speedier preprocessing tools to work with our data and prepate it for topic modeling.\n",
    "\n",
    "Gensim requires that the corpus be in a _bag of words_ format for topic modeling, so we'll need to put our documents in that format first.  Fortunately this requires little code: just do our preprocessing, then use some pre-built tools from Gensim to do the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82bb9fb14b094f04b25a37cb6ad61801",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Preprocessing', max=681288), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d765f71c2184aaca1fe7fe4bc4e1207",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='id2word', max=681288), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Removed 507430 tokens based on frequency criteria.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95b8b4dafe574e48ac35243bd26e9990",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='BoW', max=681288), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Wall time: 13min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.corpora.mmcorpus import MmCorpus\n",
    "from gensim.parsing.preprocessing import preprocess_string\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "# preprocess our corpus\n",
    "corpus = [\n",
    "    preprocess_string(i) \n",
    "    for i in tqdm(blog_dataframe[\"Post\"], desc=\"Preprocessing\")\n",
    "]\n",
    "id2word = Dictionary(tqdm(corpus, desc=\"id2word\"))\n",
    "# remove tokens with extremely high or low frequencies\n",
    "vocabsize = len(id2word)\n",
    "id2word.filter_extremes(\n",
    "    no_above=.5,  # remove tokens in > 50% of the documents (default)\n",
    "    no_below=5,   # remove tokens in < 5 documents (default)\n",
    "    keep_n=500000 # only keep 500k tokens, max--up from default 100k for good measure\n",
    ")\n",
    "# Reset index spacings for better efficiency\n",
    "id2word.compactify()\n",
    "print(f\"Removed {vocabsize - len(id2word)} tokens based on frequency criteria.\")\n",
    "corpus = [\n",
    "    id2word.doc2bow(i) \n",
    "    for i in tqdm(corpus, desc=\"BoW\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Around this time we should also notice that we're using a _huge_ amount of RAM.  So much, in fact, that the computer might be close to running out.  (It turns out that it's almost entirely from the dataframe still in memory, but let's just pretend for the moment that it's our actual corpus).  How do we deal with this?  Simple: _streaming._  Gensim is built around the concept of working with data one chunk at a time--so we can save our data to a file, then read one \"chunk\" of that file at a time!  When dealing with a few hundred, or a few thousand, documents this isn't needed.  But when dealing with _millions_ of documents or more, it's absolutely required, unless you want to spend thousands of dollars on extremely high-end computing hardware or cloud computing time/space.\n",
    "\n",
    "Our corpus doesn't actually require this.  (The cell below will show it's only using a few megabytes--perfectly reasonable).  But we'll do it anyways, just for demonstration's sake, since this same idea and approach) would scale up nicely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id2word dictionary size in memory: 0.05KB\n",
      "Gensim corpus size in memory: 5.83MB\n",
      "Blog Dataframe dictionary size in memory: 969.19MB\n",
      "Corpus and id2word dict saved.\n"
     ]
    }
   ],
   "source": [
    "from sys import getsizeof\n",
    "\n",
    "# getsizeof returns bytes--convert to megabytes\n",
    "# nb: factors of 1024, not 1000\n",
    "id2word_size = getsizeof(id2word) / 1024\n",
    "corpus_size = getsizeof(corpus) / 1048576\n",
    "blog_size = getsizeof(blog_dataframe) / 1048576\n",
    "\n",
    "print(f\"id2word dictionary size in memory: {id2word_size:.2f}KB\")\n",
    "print(f\"Gensim corpus size in memory: {corpus_size:.2f}MB\")\n",
    "print(f\"Blog Dataframe dictionary size in memory: {blog_size:.2f}MB\")\n",
    "\n",
    "# Delete that massive blog dataframe first--we already saved\n",
    "# it to file.\n",
    "del blog_dataframe\n",
    "\n",
    "# Save the dictionary\n",
    "id2word.save(\"corpus data files/id2word\")\n",
    "# Save the corpus in matrix market format.\n",
    "MmCorpus.serialize(\n",
    "    fname=\"corpus data files/gensim_corpus.mm\",\n",
    "    corpus=corpus,\n",
    "    id2word=id2word\n",
    ")\n",
    "print(\"Corpus and id2word dict saved.\")\n",
    "\n",
    "# we can reload the id2word and corpus data from the files\n",
    "# we just saved, too.\n",
    "del corpus\n",
    "del id2word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When re-reload the corpus, we'll see that it's _much_ smaller in memory.  This is because, using Gensim's tools, we're only looking at the size of the _thing that accesses the data,_ but which does not currently store any of the data--it's all in a file on disk.  \n",
    "\n",
    "Accessing data from disk will be slow, though.  Even the fastest SSDs are still at least 10x slower than even slow RAM.  So our runtime will be limited by _disk access speed_.  BUT, disk space is _significantly_ cheaper than RAM space!  At the time of writing, 16 gigabytes of RAM costs about \\$200*.  Meanwhile, $200 can get you between 7 and 10 _terabytes_ of hard drive space (less if you want SSDs, though).  So while reading corpora off disk is very slow, it lets us work with _much_ larger datasets.\n",
    "\n",
    "We can re-load the corpus and dictionary we just saved.  The dictionary didn't take up much memory at all, but saving a copy was a good idea anyways.  The corpus also didn't take much space.\n",
    "\n",
    "\\*_RAM prices are currently very highly inflated, though; normally this much RAM should only cost about $100.  But the point still stands: RAM is expensive, hard drives are dirt cheap._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = Dictionary.load(\"corpus data files/id2word\")\n",
    "corpus = MmCorpus(\"corpus data files/gensim_corpus.mm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aside: spaCy bag-of-words pipeline\n",
    "\n",
    "We won't run this during this demo, but just for comparison, here's a spaCy preprocessing pipeline that does the same thing.  The only thing that changes is the first pass through the corpus (the first `corpus = [...]` bit)--the bag-of-words steps are as before.  The change is shown below--all the other code would be the same.\n",
    "\n",
    "When I ran this next cell on my computer it took about an hour and a half to run (~200 documents per second), compared to the Gensim pipeline which took about 11 minutes (~1000 documents per second).  Of course, if you remove the `disable` line, you'll get much higher-quality results, but it will run about 10x slower (~20 documents per second on my computer, nearly 12 hours total runtime)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading spaCy model and post data.\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59d7b5fbc8d146d2b8455208b90dfbd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Preprocessing', max=681288), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import spacy\n",
    "\n",
    "print(\"Loading spaCy model and post data.\")\n",
    "nlp = spacy.load(\n",
    "    \"en_core_web_lg\", \n",
    "    disable=[\"parser\",  \"tagger\", \"ner\"]\n",
    ")\n",
    "texts = list(pd.read_csv(\n",
    "    \"corpus data files/Blog Data.csv\",\n",
    "    usecols=[\"Post\"],\n",
    "    squeeze=True\n",
    "))\n",
    "print(\"Done.\")\n",
    "\n",
    "corpus_spacy = [\n",
    "    [\n",
    "        i.lemma_\n",
    "        for i in nlp(j)\n",
    "        if i.is_stop == False\n",
    "        and i.is_punct == False\n",
    "    ]\n",
    "    for j in tqdm(texts, desc=\"Preprocessing\")\n",
    "]\n",
    "del corpus_spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back to topic models\n",
    "\n",
    "Now, let's run some of these topic models.  We won't bother tweaking any of the default settings (except for chunksize, which should give us a bit more speed), meaning each one will search for 100 topics.  This is the most important parameter in the models, by far--and sadly, the only really good way to find a good value is to run it at a range of different topic numbers and see what gives you useful results.  You _can_ look at the _coherence_ of each topic (calculated by Gensum automatically) and use that to evaluate your model, but the be-all-end-all is the human interpretability and the usefulness of your topics.\n",
    "\n",
    "These models will take a while.  So we can skip down to the next cell and just load the models back up from disk.\n",
    "\n",
    "We also need to do a _[term frequency-inverse document frequency](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)_ transform on the corpus.  This is a weighting scheme that converts the raw word counts into values that _decrease_ the weight of a word based on how many documents it appears in (more documents --> proves less informatio about any individual document, so decrease the weight), and _increases_ it based on how often it appears _within the current document_ (more occurrences --> more important to the document)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85647d15a84b4b2cbec02c60fb14daf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='TFIDF Fitting', max=681288), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c76fec35b7a40b8a55c0af0efdecf08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='TFIDF Transforming', max=681288), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "from gensim.corpora.mmcorpus import MmCorpus\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "from gensim.models.atmodel import AuthorTopicModel\n",
    "from gensim.models.hdpmodel import HdpModel\n",
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "\n",
    "id2word = Dictionary.load(\"corpus data files/id2word\")\n",
    "corpus = MmCorpus(\"corpus data files/gensim_corpus.mm\")\n",
    "tfidf_model = TfidfModel(tqdm(corpus, desc=\"TFIDF Fitting\"))\n",
    "# TfidfModel returns a generator.  We want it as a list to \n",
    "# re-use it for all the models.\n",
    "corpus = list(tfidf_model[tqdm(corpus, desc=\"TFIDF Transforming\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do LDA first.  Gensim has a multi-threaded implementation that can take advantage of multi-core processors for significant speedups; we'll use that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running LDA model.  This will take some time.\n",
      "Saving LDA model to file.\n",
      "Done.\n",
      "Wall time: 1min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(\"Running LDA model.  This will take some time.\")\n",
    "lda = LdaMulticore(corpus, workers=3, id2word=id2word, chunksize=50000)\n",
    "print(\"Saving LDA model to file.\")\n",
    "lda.save(\"topic models/LDA.model\")\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some of the LDA outputs and see if we can interpret them.  We'll look at only the top ten highest-likelihood topics, sorted by decreasing likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[([(0.00092093373, 'rain'),\n",
      "   (0.0008035689, 'guess'),\n",
      "   (0.00078206795, 'write'),\n",
      "   (0.0005686296, 'right'),\n",
      "   (0.0005508533, 'go'),\n",
      "   (0.00054732605, 'joann'),\n",
      "   (0.0005405161, 'think'),\n",
      "   (0.00049317506, 'decid'),\n",
      "   (0.00048514587, 'haha'),\n",
      "   (0.00047865085, 'clare')],\n",
      "  -1.5384370477804794),\n",
      " ([(0.0008991228, 'matthew'),\n",
      "   (0.00072009064, 'dian'),\n",
      "   (0.0006500657, 'argh'),\n",
      "   (0.00058465527, 'kathryn'),\n",
      "   (0.00055620045, 'loser'),\n",
      "   (0.00053475646, 'yeah'),\n",
      "   (0.00050318113, 'want'),\n",
      "   (0.0004962259, 'break'),\n",
      "   (0.0004900501, 'joann'),\n",
      "   (0.00047979664, 'wow')],\n",
      "  -1.7607257388925908),\n",
      " ([(0.0009812762, 'todai'),\n",
      "   (0.0009787227, 'dai'),\n",
      "   (0.0008750251, 'like'),\n",
      "   (0.0008672549, 'tonight'),\n",
      "   (0.0007693343, 'pretend'),\n",
      "   (0.00076476566, 'talk'),\n",
      "   (0.00070628105, 'act'),\n",
      "   (0.0006439158, 'annoi'),\n",
      "   (0.00063826435, 'perfect'),\n",
      "   (0.0006011717, 'smile')],\n",
      "  -3.5720586013140587),\n",
      " ([(0.0006726454, 'damn'),\n",
      "   (0.00064148917, 'bite'),\n",
      "   (0.000550192, 'sick'),\n",
      "   (0.0005334273, 'jane'),\n",
      "   (0.00041711578, 'friend'),\n",
      "   (0.00040287588, 'think'),\n",
      "   (0.00037928886, 'year'),\n",
      "   (0.00036172222, 'miss'),\n",
      "   (0.00036116198, 'pretti'),\n",
      "   (0.0003552427, 'go')],\n",
      "  -4.729673786228957),\n",
      " ([(0.0005468447, 'talk'),\n",
      "   (0.0005338631, 'written'),\n",
      "   (0.00053063536, 'actual'),\n",
      "   (0.0005222653, 'evil'),\n",
      "   (0.000508446, 'dieter'),\n",
      "   (0.00050454546, 'clean'),\n",
      "   (0.000492237, 'amaz'),\n",
      "   (0.00048263467, 'book'),\n",
      "   (0.000468811, 'emot'),\n",
      "   (0.00046597794, 'go')],\n",
      "  -4.87709355333691),\n",
      " ([(0.00061702053, 'prei'),\n",
      "   (0.0005764359, 'actual'),\n",
      "   (0.0005415455, 'brandon'),\n",
      "   (0.00054087216, 'go'),\n",
      "   (0.00053883495, 'write'),\n",
      "   (0.0005336208, 'gah'),\n",
      "   (0.0005297189, 'sleep'),\n",
      "   (0.00051873253, 'definit'),\n",
      "   (0.0005044983, 'want'),\n",
      "   (0.00048369687, 'kid')],\n",
      "  -4.96351599977348),\n",
      " ([(0.0013569207, 'chri'),\n",
      "   (0.0009978891, 'haha'),\n",
      "   (0.0008258575, 'wow'),\n",
      "   (0.0008249744, 'talk'),\n",
      "   (0.00073737535, 'carson'),\n",
      "   (0.00070142513, 'funer'),\n",
      "   (0.0006709297, 'napervil'),\n",
      "   (0.00063921825, 'peter'),\n",
      "   (0.0006338778, 'poo'),\n",
      "   (0.00062776776, 'stuff')],\n",
      "  -4.9787171514901045),\n",
      " ([(0.00093732553, 'bryan'),\n",
      "   (0.0009084656, 'tonight'),\n",
      "   (0.0009064399, 'sick'),\n",
      "   (0.00065345416, 'aw'),\n",
      "   (0.0005912326, 'believ'),\n",
      "   (0.0005707381, 'fun'),\n",
      "   (0.0005264524, 'catalogu'),\n",
      "   (0.0005243875, 'mood'),\n",
      "   (0.0004981388, 'lot'),\n",
      "   (0.00049355807, 'tomorrow')],\n",
      "  -5.092025902206698),\n",
      " ([(0.0006994233, 'love'),\n",
      "   (0.00055403507, 'dian'),\n",
      "   (0.000493976, 'casevil'),\n",
      "   (0.00046011544, 'march'),\n",
      "   (0.0004511361, 'school'),\n",
      "   (0.0004482528, 'peopl'),\n",
      "   (0.00043098372, 'weird'),\n",
      "   (0.00039785999, 'complain'),\n",
      "   (0.00038097482, 'let'),\n",
      "   (0.00037237103, 'dissapoint')],\n",
      "  -5.092327709761528),\n",
      " ([(0.0007297373, 'yum'),\n",
      "   (0.00072043296, 'crayon'),\n",
      "   (0.00067232555, 'go'),\n",
      "   (0.00060581847, 'trust'),\n",
      "   (0.00058740104, 'think'),\n",
      "   (0.0005541895, 'dian'),\n",
      "   (0.00055318896, 'dinner'),\n",
      "   (0.0005162506, 'great'),\n",
      "   (0.00051287666, 'saltin'),\n",
      "   (0.000498784, 'start')],\n",
      "  -5.149922250458689),\n",
      " ([(0.0008745203, 'eeek'),\n",
      "   (0.0006622648, 'joann'),\n",
      "   (0.0005919525, 'bryan'),\n",
      "   (0.00053722406, 'market'),\n",
      "   (0.0005176523, 'feel'),\n",
      "   (0.00042202583, 'kathryn'),\n",
      "   (0.0004077481, 'think'),\n",
      "   (0.00039397145, 'todai'),\n",
      "   (0.0003939047, 'brett'),\n",
      "   (0.00036884312, 'dian')],\n",
      "  -5.42293076356595),\n",
      " ([(0.0009182596, 'heh'),\n",
      "   (0.00074860017, 'kathryn'),\n",
      "   (0.0005197059, 'peopl'),\n",
      "   (0.00050724525, 'lot'),\n",
      "   (0.00048227434, 'expect'),\n",
      "   (0.00047178523, 'like'),\n",
      "   (0.0004558401, 'argh'),\n",
      "   (0.000455835, 'drainpip'),\n",
      "   (0.0004414981, 'kelli'),\n",
      "   (0.0004380725, 'know')],\n",
      "  -5.45693265286201),\n",
      " ([(0.0007112542, 'makeov'),\n",
      "   (0.00029700578, 'makeup'),\n",
      "   (0.00029439584, 'dork'),\n",
      "   (0.0002531568, 'leav'),\n",
      "   (0.00023011604, 'fun'),\n",
      "   (0.00021227612, 'wear'),\n",
      "   (0.00020797744, 'blogger'),\n",
      "   (0.00018534237, 'crash'),\n",
      "   (0.00018305208, 'special'),\n",
      "   (0.00017382081, 'kel')],\n",
      "  -5.681005681091768),\n",
      " ([(0.00088590966, 'write'),\n",
      "   (0.00073334697, 'need'),\n",
      "   (0.00058320485, 'peopl'),\n",
      "   (0.00058006524, 'whooo'),\n",
      "   (0.00057888264, 'chri'),\n",
      "   (0.0005570404, 'soir'),\n",
      "   (0.00054653856, 'talk'),\n",
      "   (0.0005090652, 'wont'),\n",
      "   (0.00046958684, 'cruel'),\n",
      "   (0.00044493447, 'person')],\n",
      "  -6.130489048265196),\n",
      " ([(0.0008183472, 'like'),\n",
      "   (0.0007518832, 'kati'),\n",
      "   (0.0007155275, 'cool'),\n",
      "   (0.00068597595, 'come'),\n",
      "   (0.00059106166, 'amaz'),\n",
      "   (0.000585173, 'let'),\n",
      "   (0.00048672958, 'go'),\n",
      "   (0.00046635538, 'sleep'),\n",
      "   (0.00045576275, 'swell'),\n",
      "   (0.00044475848, 'coolest')],\n",
      "  -6.393899637446207),\n",
      " ([(0.00065486354, 'fun'),\n",
      "   (0.00063009514, 'peter'),\n",
      "   (0.00059921516, 'yai'),\n",
      "   (0.0005496473, 'ensembl'),\n",
      "   (0.00054780336, 'write'),\n",
      "   (0.0004862994, 'bryan'),\n",
      "   (0.0004738838, 'page'),\n",
      "   (0.00044538747, 'bowl'),\n",
      "   (0.00043850127, 'templat'),\n",
      "   (0.00043097817, 'tonight')],\n",
      "  -6.590960171005422),\n",
      " ([(0.0009135352, 'win'),\n",
      "   (0.0008050191, 'homecom'),\n",
      "   (0.0007203943, 'go'),\n",
      "   (0.00071112555, 'bad'),\n",
      "   (0.0006529824, 'muscl'),\n",
      "   (0.0006528688, 'think'),\n",
      "   (0.00061364484, 'natur'),\n",
      "   (0.00061229215, 'feel'),\n",
      "   (0.00058572897, 'mother'),\n",
      "   (0.00051840383, 'serious')],\n",
      "  -6.625785988497155),\n",
      " ([(0.00064297795, 'write'),\n",
      "   (0.00054595014, 'later'),\n",
      "   (0.0005234094, 'defenestr'),\n",
      "   (0.00048696122, 'hmmmm'),\n",
      "   (0.0004621348, 'nah'),\n",
      "   (0.00045335392, 'think'),\n",
      "   (0.00044369354, 'allll'),\n",
      "   (0.00042941608, 'blogger'),\n",
      "   (0.00042357523, 'outsid'),\n",
      "   (0.00041728327, 'homework')],\n",
      "  -6.810671150029868),\n",
      " ([(0.00055016275, 'wont'),\n",
      "   (0.00049482344, 'shall'),\n",
      "   (0.0004684652, 'fondli'),\n",
      "   (0.0004614587, 'think'),\n",
      "   (0.00041086535, 'realiz'),\n",
      "   (0.00040558277, 'happi'),\n",
      "   (0.00040171333, 'furnitur'),\n",
      "   (0.0004014497, 'hmmmm'),\n",
      "   (0.00037193327, 'know'),\n",
      "   (0.0003694404, 'best')],\n",
      "  -6.975793572636597),\n",
      " ([(0.0007053961, 'shower'),\n",
      "   (0.0006984094, 'good'),\n",
      "   (0.00067923166, 'verrrri'),\n",
      "   (0.0006111613, 'nice'),\n",
      "   (0.0005585225, 'mayb'),\n",
      "   (0.00054873474, 'kelli'),\n",
      "   (0.00054057257, 'meani'),\n",
      "   (0.0005339454, 'thinkin'),\n",
      "   (0.0005017333, 'go'),\n",
      "   (0.0005005966, 'tomorrow')],\n",
      "  -7.206068515618327),\n",
      " ([(0.00088616373, 'blah'),\n",
      "   (0.00066355214, 'argh'),\n",
      "   (0.0005713315, 'weellll'),\n",
      "   (0.0005191693, 'pretti'),\n",
      "   (0.0005017907, 'lot'),\n",
      "   (0.0004915866, 'necessarili'),\n",
      "   (0.00047702115, 'sleep'),\n",
      "   (0.00045698386, 'crappi'),\n",
      "   (0.00045506816, 'talk'),\n",
      "   (0.00042296498, 'kelli')],\n",
      "  -7.236143562051343),\n",
      " ([(0.0008816044, 'swear'),\n",
      "   (0.00060314115, 'horizont'),\n",
      "   (0.00052396546, 'darn'),\n",
      "   (0.0004594123, 'year'),\n",
      "   (0.00044763353, 'cheer'),\n",
      "   (0.00043634104, 'lose'),\n",
      "   (0.0004356401, 'know'),\n",
      "   (0.00041415432, 'far'),\n",
      "   (0.00040514054, 'speech'),\n",
      "   (0.00039223404, 'homework')],\n",
      "  -7.4877588021744375),\n",
      " ([(0.00078771875, 'chang'),\n",
      "   (0.0006780179, 'think'),\n",
      "   (0.0006001383, 'ankl'),\n",
      "   (0.00058188377, 'want'),\n",
      "   (0.0005364936, 'got'),\n",
      "   (0.00050850183, 'brrrr'),\n",
      "   (0.0004980984, 'tonight'),\n",
      "   (0.00045720063, 'optimist'),\n",
      "   (0.00045487002, 'whatnot'),\n",
      "   (0.0004419229, 'go')],\n",
      "  -7.579540534750538),\n",
      " ([(0.00063961133, 'good'),\n",
      "   (0.00058056915, 'dian'),\n",
      "   (0.0005434508, 'sleep'),\n",
      "   (0.0005260117, 'morn'),\n",
      "   (0.000518246, 'grrr'),\n",
      "   (0.0004797501, 'tra'),\n",
      "   (0.0004781448, 'yeahh'),\n",
      "   (0.00045606785, 'nabbit'),\n",
      "   (0.0004248938, 'time'),\n",
      "   (0.00041672503, 'sorri')],\n",
      "  -7.70034756642847),\n",
      " ([(0.00092496024, 'googl'),\n",
      "   (0.0006630731, 'histori'),\n",
      "   (0.00058728305, 'self'),\n",
      "   (0.00055256614, 'note'),\n",
      "   (0.0005479128, 'upset'),\n",
      "   (0.0005038233, 'good'),\n",
      "   (0.00048687335, 'product'),\n",
      "   (0.00044481366, 'inspir'),\n",
      "   (0.0004410224, 'march'),\n",
      "   (0.00039098837, 'love')],\n",
      "  -8.256516736255957),\n",
      " ([(0.0007564084, 'bad'),\n",
      "   (0.0007394919, 'gosh'),\n",
      "   (0.00069477846, 'think'),\n",
      "   (0.0006897228, 'happen'),\n",
      "   (0.0006830248, 'enum'),\n",
      "   (0.0006046832, 'go'),\n",
      "   (0.0005312775, 'forgiv'),\n",
      "   (0.0004816285, 'jso'),\n",
      "   (0.0004652941, 'joann'),\n",
      "   (0.00045625627, 'phone')],\n",
      "  -8.351938130952876),\n",
      " ([(0.00086442963, 'kathryn'),\n",
      "   (0.0007039281, 'glad'),\n",
      "   (0.0006433308, 'defenestr'),\n",
      "   (0.00059931056, 'annoi'),\n",
      "   (0.0005924012, 'heh'),\n",
      "   (0.00053618196, 'feel'),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   (0.0005284203, 'stai'),\n",
      "   (0.0005120593, 'uneed'),\n",
      "   (0.00050410256, 'alstadt'),\n",
      "   (0.00049523334, 'bad')],\n",
      "  -8.468354304104993),\n",
      " ([(0.0006813523, 'lover'),\n",
      "   (0.0006089997, 'rubbish'),\n",
      "   (0.00058881735, 'shock'),\n",
      "   (0.0005130175, 'like'),\n",
      "   (0.00049153215, 'cuter'),\n",
      "   (0.00048427735, 'think'),\n",
      "   (0.00048104842, 'actual'),\n",
      "   (0.0004659242, 'angri'),\n",
      "   (0.00046214528, 'todai'),\n",
      "   (0.00046195133, 'dave')],\n",
      "  -8.669206790862125),\n",
      " ([(0.0008260789, 'kel'),\n",
      "   (0.00063300185, 'miss'),\n",
      "   (0.00058167946, 'matthew'),\n",
      "   (0.0005557205, 'qute'),\n",
      "   (0.00053706014, 'cavi'),\n",
      "   (0.000535947, 'deserv'),\n",
      "   (0.0004998833, 'entri'),\n",
      "   (0.0004668867, 'like'),\n",
      "   (0.0004513029, 'mood'),\n",
      "   (0.00044603983, 'word')],\n",
      "  -8.751176175829368),\n",
      " ([(0.0007684123, 'punk'),\n",
      "   (0.0007530994, 'cati'),\n",
      "   (0.0006335628, 'leav'),\n",
      "   (0.00062862306, 'mom'),\n",
      "   (0.0006072522, 'ummm'),\n",
      "   (0.000560705, 'calfornia'),\n",
      "   (0.0005062097, 'haha'),\n",
      "   (0.0005037128, 'rock'),\n",
      "   (0.00047650738, 'tomorrow'),\n",
      "   (0.00045640243, 'morn')],\n",
      "  -8.752772218562548),\n",
      " ([(0.0013436968, 'bryan'),\n",
      "   (0.0008997175, 'vespa'),\n",
      "   (0.0005212086, 'actual'),\n",
      "   (0.0005084115, 'whyyyi'),\n",
      "   (0.00049308786, 'march'),\n",
      "   (0.0004791243, 'errand'),\n",
      "   (0.0004678168, 'kelli'),\n",
      "   (0.00046045083, 'talk'),\n",
      "   (0.0004598115, 'yai'),\n",
      "   (0.00045264614, 'tomorrow')],\n",
      "  -9.084462055897577),\n",
      " ([(0.00090424664, 'cub'),\n",
      "   (0.0007130992, 'yai'),\n",
      "   (0.00068734156, 'homework'),\n",
      "   (0.0006810614, 'einstein'),\n",
      "   (0.00060665293, 'heh'),\n",
      "   (0.0005438106, 'untrust'),\n",
      "   (0.00053136953, 'clan'),\n",
      "   (0.00052870926, 'good'),\n",
      "   (0.00052283634, 'pretti'),\n",
      "   (0.00047348483, 'spend')],\n",
      "  -9.107923387111756),\n",
      " ([(0.00084125984, 'chri'),\n",
      "   (0.0008208428, 'write'),\n",
      "   (0.0007033333, 'amaz'),\n",
      "   (0.0006417203, 'lot'),\n",
      "   (0.0005884232, 'alllll'),\n",
      "   (0.0005588044, 'comp'),\n",
      "   (0.0005529092, 'haven'),\n",
      "   (0.0005414575, 'excit'),\n",
      "   (0.00052506523, 'i\\x92m'),\n",
      "   (0.00052424864, 'attent')],\n",
      "  -9.162365051381876),\n",
      " ([(0.00073779025, 'oakbrook'),\n",
      "   (0.00066519453, 'joann'),\n",
      "   (0.0005339523, 'excit'),\n",
      "   (0.000532031, 'fun'),\n",
      "   (0.0005262263, 'definit'),\n",
      "   (0.0005150944, 'give'),\n",
      "   (0.00050458795, 'hater'),\n",
      "   (0.0004656209, 'dian'),\n",
      "   (0.00045637146, 'think'),\n",
      "   (0.00045077462, 'terrif')],\n",
      "  -9.252988812975632),\n",
      " ([(0.00067253865, 'argh'),\n",
      "   (0.0006628756, 'hour'),\n",
      "   (0.00062358606, 'abl'),\n",
      "   (0.0006124432, 'write'),\n",
      "   (0.0005244667, 'ye'),\n",
      "   (0.00052114535, 'chart'),\n",
      "   (0.0005192883, 'love'),\n",
      "   (0.00051557174, 'talk'),\n",
      "   (0.00050044054, 'googl'),\n",
      "   (0.0004953754, 'chri')],\n",
      "  -9.570173592557369),\n",
      " ([(0.0006261575, 'yeah'),\n",
      "   (0.0006220353, 'letter'),\n",
      "   (0.0006175622, 'ryan'),\n",
      "   (0.0005890019, 'tell'),\n",
      "   (0.0005668522, 'flint'),\n",
      "   (0.0005362544, 'lot'),\n",
      "   (0.00047999568, 'blasphem'),\n",
      "   (0.00046458197, 'fun'),\n",
      "   (0.00045414324, 'haha'),\n",
      "   (0.00045371088, 'finish')],\n",
      "  -9.57129227841869),\n",
      " ([(0.00079716527, 'fathom'),\n",
      "   (0.00060485717, 'telletubbi'),\n",
      "   (0.00059924787, 'section'),\n",
      "   (0.0005463827, 'hasn'),\n",
      "   (0.00052344374, 'idiot'),\n",
      "   (0.0005027167, 'terribl'),\n",
      "   (0.00049444305, 'dad'),\n",
      "   (0.00046782847, 'girlfriend'),\n",
      "   (0.00046043497, 'glad'),\n",
      "   (0.00044596955, 'wow')],\n",
      "  -9.735987233729464),\n",
      " ([(0.0011001941, 'bryan'),\n",
      "   (0.0010909603, 'mood'),\n",
      "   (0.0007720411, 'wow'),\n",
      "   (0.0006324807, 'chri'),\n",
      "   (0.0006073396, 'onlin'),\n",
      "   (0.00057222816, 'better'),\n",
      "   (0.00047975915, 'checklist'),\n",
      "   (0.00047194626, 'homework'),\n",
      "   (0.00045545018, 'entrepreneur'),\n",
      "   (0.00042632406, 'tonight')],\n",
      "  -9.879728391237284),\n",
      " ([(0.0012154548, 'happi'),\n",
      "   (0.0007034917, 'todai'),\n",
      "   (0.0005445545, 'dilemma'),\n",
      "   (0.00046624822, 'reallli'),\n",
      "   (0.00045628424, 'guess'),\n",
      "   (0.00045126752, 'bipolar'),\n",
      "   (0.00041082047, 'roulett'),\n",
      "   (0.00040647556, 'moral'),\n",
      "   (0.0003788625, 'mood'),\n",
      "   (0.00037572725, 'ugh')],\n",
      "  -9.896970258299827),\n",
      " ([(0.00078878144, 'aohel'),\n",
      "   (0.00072517846, 'bryan'),\n",
      "   (0.0007030359, 'heaven'),\n",
      "   (0.0006880929, 'zati'),\n",
      "   (0.00065538543, 'fine'),\n",
      "   (0.00059235323, 'good'),\n",
      "   (0.00057679263, 'perfectli'),\n",
      "   (0.00057161966, 'khaki'),\n",
      "   (0.0005634397, 'think'),\n",
      "   (0.0005586492, 'haha')],\n",
      "  -10.006072051703168),\n",
      " ([(0.00056580885, 'ticket'),\n",
      "   (0.00049528264, 'indi'),\n",
      "   (0.00039960048, 'chick'),\n",
      "   (0.00036540863, 'exclam'),\n",
      "   (0.00036488607, 'summertim'),\n",
      "   (0.00035972227, 'ivan'),\n",
      "   (0.00035893157, 'pug'),\n",
      "   (0.00030307696, 'colorado'),\n",
      "   (0.0002979991, 'rocki'),\n",
      "   (0.00027001966, 'warp')],\n",
      "  -10.221900734402569),\n",
      " ([(0.00079541834, 'michigan'),\n",
      "   (0.000681319, 'audit'),\n",
      "   (0.0006027347, 'iron'),\n",
      "   (0.00058671867, 'heh'),\n",
      "   (0.0005625176, 'nice'),\n",
      "   (0.0005134074, 'good'),\n",
      "   (0.00047977202, 'looni'),\n",
      "   (0.0004685939, 'slept'),\n",
      "   (0.00044660622, 'brain'),\n",
      "   (0.0004456265, 'stuck')],\n",
      "  -10.363474768082341),\n",
      " ([(0.00077354326, 'kristin'),\n",
      "   (0.00072754483, 'bummer'),\n",
      "   (0.000538124, 'cow'),\n",
      "   (0.00046780877, 'trust'),\n",
      "   (0.0004604392, 'stupid'),\n",
      "   (0.0004527785, 'holi'),\n",
      "   (0.00044152664, 'hour'),\n",
      "   (0.00044001985, 'week'),\n",
      "   (0.00043965984, 'go'),\n",
      "   (0.00043491652, 'breakfast')],\n",
      "  -10.506052053397173),\n",
      " ([(0.0006638596, 'exhiler'),\n",
      "   (0.0006569523, 'heheheheheheh'),\n",
      "   (0.0005649996, 'alex'),\n",
      "   (0.0005522474, 'ryan'),\n",
      "   (0.00045523784, 'mom'),\n",
      "   (0.00044661446, 'profil'),\n",
      "   (0.00038143646, 'nikki'),\n",
      "   (0.0003748856, 'went'),\n",
      "   (0.00035559785, 'whatnot'),\n",
      "   (0.00035241168, 'yai')],\n",
      "  -10.530300052920492),\n",
      " ([(0.0007599982, 'dream'),\n",
      "   (0.0005236115, 'knowwww'),\n",
      "   (0.00045213266, 'mmm'),\n",
      "   (0.00044358562, 'weird'),\n",
      "   (0.00044015388, 'shock'),\n",
      "   (0.000437162, 'stuff'),\n",
      "   (0.0004257888, 'guess'),\n",
      "   (0.00042415824, 'sleepi'),\n",
      "   (0.00040836236, 'rrealli'),\n",
      "   (0.00038576152, 'sleep')],\n",
      "  -11.001144777720954),\n",
      " ([(0.0007658861, 'comment'),\n",
      "   (0.0007437844, 'sleep'),\n",
      "   (0.0006650006, 'guess'),\n",
      "   (0.00066499505, 'casevil'),\n",
      "   (0.0005931865, 'page'),\n",
      "   (0.0005779775, 'castrat'),\n",
      "   (0.00055198587, 'haha'),\n",
      "   (0.00050853763, 'figur'),\n",
      "   (0.0005056808, 'tank'),\n",
      "   (0.0005003494, 'astro')],\n",
      "  -11.036334332497946),\n",
      " ([(0.00071891176, 'sick'),\n",
      "   (0.00053795957, 'chri'),\n",
      "   (0.0004899943, 'talk'),\n",
      "   (0.00047136287, 'niro'),\n",
      "   (0.00046067988, 'hurt'),\n",
      "   (0.000414924, 'arg'),\n",
      "   (0.000410314, 'outsourc'),\n",
      "   (0.00040540987, 'india'),\n",
      "   (0.00039567755, 'upset'),\n",
      "   (0.00038172788, 'joann')],\n",
      "  -11.05599789115794),\n",
      " ([(0.0010981535, 'nabbit'),\n",
      "   (0.0010782303, 'dian'),\n",
      "   (0.0010765091, 'sleep'),\n",
      "   (0.0008814837, 'dag'),\n",
      "   (0.00087293825, 'chicken'),\n",
      "   (0.0007848518, 'cancun'),\n",
      "   (0.00072713447, 'mood'),\n",
      "   (0.0006977941, 'miss'),\n",
      "   (0.0006550684, 'joann'),\n",
      "   (0.00059127435, 'kathryn')],\n",
      "  -11.20241482728007),\n",
      " ([(0.0011674542, 'blah'),\n",
      "   (0.0006030145, 'yahoo'),\n",
      "   (0.0006020656, 'haha'),\n",
      "   (0.0005661541, 'carson'),\n",
      "   (0.0005643933, 'miss'),\n",
      "   (0.0005413419, 'cavi'),\n",
      "   (0.0005319781, 'loath'),\n",
      "   (0.000507747, 'buttercup'),\n",
      "   (0.00050063035, 'joann'),\n",
      "   (0.00046025286, 'lot')],\n",
      "  -11.301718355354236),\n",
      " ([(0.0009785816, 'sort'),\n",
      "   (0.00079502125, 'rock'),\n",
      "   (0.0007667029, 'punk'),\n",
      "   (0.00070313725, 'brandon'),\n",
      "   (0.00057774765, 'erica'),\n",
      "   (0.00048263822, 'upset'),\n",
      "   (0.0004746621, 'reallllli'),\n",
      "   (0.00044811208, 'yesterdai'),\n",
      "   (0.0004479745, 'romant'),\n",
      "   (0.00043717938, 'hurt')],\n",
      "  -11.412641356301215),\n",
      " ([(0.0009265318, 'bryan'),\n",
      "   (0.000840935, 'despond'),\n",
      "   (0.00077590725, 'definit'),\n",
      "   (0.0005451678, 'lalalalalala'),\n",
      "   (0.0004916805, 'hei'),\n",
      "   (0.0004698326, 'like'),\n",
      "   (0.00044840545, 'rain'),\n",
      "   (0.00044291676, 'ahahahaha'),\n",
      "   (0.0004057419, 'wow'),\n",
      "   (0.00037550478, 'serious')],\n",
      "  -11.447267229584858),\n",
      " ([(0.001249564, 'yai'),\n",
      "   (0.00082254177, 'mad'),\n",
      "   (0.0007637355, 'cupid'),\n",
      "   (0.00076114805, 'definit'),\n",
      "   (0.0007523536, 'fun'),\n",
      "   (0.00064104097, 'alstadt'),\n",
      "   (0.0006354884, 'ryan'),\n",
      "   (0.0006354733, 'wheeee'),\n",
      "   (0.0006294386, 'haha'),\n",
      "   (0.00060050265, 'inanim')],\n",
      "  -11.556683070063519),\n",
      " ([(0.0006477212, 'game'),\n",
      "   (0.0005947641, 'song'),\n",
      "   (0.0005924758, 'neverend'),\n",
      "   (0.0004773982, 'remak'),\n",
      "   (0.0004380409, 'clean'),\n",
      "   (0.00041012059, 'fun'),\n",
      "   (0.00040306544, 'muscl'),\n",
      "   (0.00039039517, 'project'),\n",
      "   (0.00037953796, 'tom'),\n",
      "   (0.00036944068, 'cool')],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -11.59405210282396),\n",
      " ([(0.0006868997, 'medic'),\n",
      "   (0.0006212433, 'kathryn'),\n",
      "   (0.00049795717, 'origami'),\n",
      "   (0.0004239874, 'chri'),\n",
      "   (0.00038046922, 'haha'),\n",
      "   (0.00036068718, 'graduat'),\n",
      "   (0.0003599938, 'boulder'),\n",
      "   (0.00035299308, 'friend'),\n",
      "   (0.00034610633, 'kelli'),\n",
      "   (0.00032520946, 'fun')],\n",
      "  -11.962357725424518),\n",
      " ([(0.0006491093, 'paul'),\n",
      "   (0.0006218772, 'saddest'),\n",
      "   (0.0005766752, 'seen'),\n",
      "   (0.0005657177, 'mood'),\n",
      "   (0.0005375258, 'unintention'),\n",
      "   (0.00051342155, 'movin'),\n",
      "   (0.00050990534, 'wow'),\n",
      "   (0.00046238984, 'funni'),\n",
      "   (0.00045488658, 'gosh'),\n",
      "   (0.00044574094, 'tasti')],\n",
      "  -12.061349095635311),\n",
      " ([(0.0007748554, 'blowin'),\n",
      "   (0.0006834064, 'brilliant'),\n",
      "   (0.00058269006, 'mad'),\n",
      "   (0.0005123634, 'thng'),\n",
      "   (0.00046708505, 'smart'),\n",
      "   (0.00046425234, 'peopl'),\n",
      "   (0.0004143366, 'serious'),\n",
      "   (0.0004083117, 'rar'),\n",
      "   (0.00038295664, 'complain'),\n",
      "   (0.00038198868, 'inspir')],\n",
      "  -12.655229165937511),\n",
      " ([(0.00081287744, 'mood'),\n",
      "   (0.0007649011, 'wheee'),\n",
      "   (0.00070791936, 'albino'),\n",
      "   (0.0005798372, 'screw'),\n",
      "   (0.00057312584, 'hangov'),\n",
      "   (0.00051796157, 'like'),\n",
      "   (0.00051357487, 'jane'),\n",
      "   (0.00050649367, 'write'),\n",
      "   (0.00049759075, 'guess'),\n",
      "   (0.00048734483, 'profil')],\n",
      "  -12.703434548382251),\n",
      " ([(0.0007985926, 'voic'),\n",
      "   (0.0005207365, 'pat'),\n",
      "   (0.0005144666, 'watermellon'),\n",
      "   (0.00048161988, 'tonight'),\n",
      "   (0.00044538558, 'reason'),\n",
      "   (0.0004442188, 'moment'),\n",
      "   (0.0003898511, 'write'),\n",
      "   (0.00037828306, 'quizilla'),\n",
      "   (0.0003637596, 'toi'),\n",
      "   (0.00035559348, 'cati')],\n",
      "  -12.759228715606467),\n",
      " ([(0.000974053, 'wrong'),\n",
      "   (0.0009126197, 'clean'),\n",
      "   (0.00084051955, 'ryan'),\n",
      "   (0.00057585957, 'wai'),\n",
      "   (0.0005683307, 'chri'),\n",
      "   (0.00051125005, 'car'),\n",
      "   (0.0005012268, 'yippe'),\n",
      "   (0.00048727947, 'journalist'),\n",
      "   (0.00047852076, 'polish'),\n",
      "   (0.00047336318, 'tire')],\n",
      "  -12.768620171281684),\n",
      " ([(0.0011203993, 'blogger'),\n",
      "   (0.0009509138, 'breath'),\n",
      "   (0.0007659689, 'mood'),\n",
      "   (0.0006700406, 'odd'),\n",
      "   (0.0006691834, 'invest'),\n",
      "   (0.0006396208, 'hurt'),\n",
      "   (0.0006392496, 'crappi'),\n",
      "   (0.0005550427, 'latlei'),\n",
      "   (0.0005336124, 'argh'),\n",
      "   (0.0005134386, 'mayb')],\n",
      "  -12.914536175989857),\n",
      " ([(0.0008165262, 'weekend'),\n",
      "   (0.0006978814, 'nokia'),\n",
      "   (0.0006310804, 'interest'),\n",
      "   (0.0006193727, 'grammar'),\n",
      "   (0.0005967895, 'powershot'),\n",
      "   (0.0005878337, 'anem'),\n",
      "   (0.00056491216, 'trumpet'),\n",
      "   (0.0005248428, 'mood'),\n",
      "   (0.00051920087, 'talk'),\n",
      "   (0.0004925236, 'want')],\n",
      "  -12.949589946319142),\n",
      " ([(0.0010047639, 'urllink'),\n",
      "   (0.0006830731, 'go'),\n",
      "   (0.00058837264, 'crappi'),\n",
      "   (0.00057500304, 'grrrr'),\n",
      "   (0.00057144655, 'bad'),\n",
      "   (0.0005533292, 'matthew'),\n",
      "   (0.00051675667, 'saturdai'),\n",
      "   (0.00050692185, 'foreman'),\n",
      "   (0.0004916731, 'ergh'),\n",
      "   (0.0004768223, 'tonight')],\n",
      "  -13.006663701028113),\n",
      " ([(0.0008798361, 'clap'),\n",
      "   (0.00072437857, 'kathryn'),\n",
      "   (0.00056144263, 'reduct'),\n",
      "   (0.00053890323, 'chri'),\n",
      "   (0.0005299772, 'heh'),\n",
      "   (0.00050258567, 'i\\x92m'),\n",
      "   (0.00047675736, 'homecom'),\n",
      "   (0.00047625395, 'casbah'),\n",
      "   (0.0004580675, 'funni'),\n",
      "   (0.0004391006, 'guess')],\n",
      "  -13.016451672696235),\n",
      " ([(0.00072002696, 'gmail'),\n",
      "   (0.00047394395, 'dream'),\n",
      "   (0.0003947833, 'talk'),\n",
      "   (0.0003915874, 'bluffer'),\n",
      "   (0.0003577538, 'britpop'),\n",
      "   (0.00034333396, 'hah'),\n",
      "   (0.00033074705, 'kwanza'),\n",
      "   (0.00032653453, 'fun'),\n",
      "   (0.00031978873, 'disillusion'),\n",
      "   (0.00030963833, 'actual')],\n",
      "  -13.11215943353421),\n",
      " ([(0.0008095959, 'sleep'),\n",
      "   (0.00068921247, 'librarian'),\n",
      "   (0.0006203247, 'errr'),\n",
      "   (0.0006172267, 'chicken'),\n",
      "   (0.0005980124, 'essai'),\n",
      "   (0.0005780825, 'suck'),\n",
      "   (0.00054685667, 'write'),\n",
      "   (0.0005263416, 'argh'),\n",
      "   (0.00051743287, 'hiccup'),\n",
      "   (0.000492981, 'weight')],\n",
      "  -13.238244344788965),\n",
      " ([(0.0009118867, 'ahhhhh'),\n",
      "   (0.0006478379, 'pooei'),\n",
      "   (0.00060016103, 'rar'),\n",
      "   (0.00057835894, 'paralyz'),\n",
      "   (0.00056581566, 'angri'),\n",
      "   (0.00055595284, 'person'),\n",
      "   (0.00050237536, 'catch'),\n",
      "   (0.00049924076, 'band'),\n",
      "   (0.00044318906, 'trueli'),\n",
      "   (0.00044284854, 'clean')],\n",
      "  -13.29433729453264),\n",
      " ([(0.0005671264, 'atm'),\n",
      "   (0.0005550402, 'dian'),\n",
      "   (0.0005263864, 'cop'),\n",
      "   (0.0004067253, 'matthew'),\n",
      "   (0.00036474498, 'downtown'),\n",
      "   (0.00035550643, 'hmmm'),\n",
      "   (0.00035141624, 'lot'),\n",
      "   (0.00034243363, 'anecdot'),\n",
      "   (0.00031313772, 'italian'),\n",
      "   (0.00031207988, 'core')],\n",
      "  -13.29821335771466),\n",
      " ([(0.0009210259, 'rar'),\n",
      "   (0.0007630873, 'lifelong'),\n",
      "   (0.00061874086, 'good'),\n",
      "   (0.0006087129, 'wow'),\n",
      "   (0.000603329, 'quizz'),\n",
      "   (0.0005905748, 'joann'),\n",
      "   (0.0005831123, 'ember'),\n",
      "   (0.0005418556, 'happen'),\n",
      "   (0.0005208992, 'go'),\n",
      "   (0.0004953213, 'marx')],\n",
      "  -13.586014041093454),\n",
      " ([(0.0006460773, 'trust'),\n",
      "   (0.0006255695, 'concess'),\n",
      "   (0.00052979117, 'pilsen'),\n",
      "   (0.0005053339, 'itali'),\n",
      "   (0.0004962194, 'blah'),\n",
      "   (0.00042419464, 'dena'),\n",
      "   (0.00041258027, 'ding'),\n",
      "   (0.00037154593, 'cool'),\n",
      "   (0.0003558008, 'evil'),\n",
      "   (0.0003465148, 'wont')],\n",
      "  -13.657390120206875),\n",
      " ([(0.00047814287, 'dian'),\n",
      "   (0.000415012, 'realllli'),\n",
      "   (0.00038890177, 'upset'),\n",
      "   (0.00032444994, 'firework'),\n",
      "   (0.00030358834, 'grandma'),\n",
      "   (0.00029618782, 'wiedersehen'),\n",
      "   (0.00027386003, 'farmacia'),\n",
      "   (0.00027386003, 'drowzi'),\n",
      "   (0.0002707841, 'necropoli'),\n",
      "   (0.00026564402, 'enlgish')],\n",
      "  -13.789363332742541),\n",
      " ([(0.0009218728, 'grr'),\n",
      "   (0.0007693665, 'argh'),\n",
      "   (0.000608985, 'rip'),\n",
      "   (0.0005880272, 'goodnight'),\n",
      "   (0.00057188084, 'ahhh'),\n",
      "   (0.0004633008, 'apart'),\n",
      "   (0.0004183662, 'confus'),\n",
      "   (0.00036933887, 'date'),\n",
      "   (0.00033264922, 'vocal'),\n",
      "   (0.00032100335, 'bed')],\n",
      "  -13.799399706494677),\n",
      " ([(0.0005993469, 'guess'),\n",
      "   (0.00057521247, 'dekalb'),\n",
      "   (0.00055992603, 'nice'),\n",
      "   (0.00051385094, 'wish'),\n",
      "   (0.00051198865, 'jane'),\n",
      "   (0.00051190355, 'definit'),\n",
      "   (0.0004955545, 'creepi'),\n",
      "   (0.00048450704, 'chri'),\n",
      "   (0.00048123777, 'repetit'),\n",
      "   (0.00046578163, 'strep')],\n",
      "  -13.862087380080863),\n",
      " ([(0.00066572893, 'want'),\n",
      "   (0.00061964634, 'goodi'),\n",
      "   (0.00059805287, 'leav'),\n",
      "   (0.0005723237, 'toooo'),\n",
      "   (0.0005199499, 'hmmmm'),\n",
      "   (0.0005055095, 'march'),\n",
      "   (0.000504274, 'horizont'),\n",
      "   (0.00046338412, 'argh'),\n",
      "   (0.00043658935, 'page'),\n",
      "   (0.00043355327, 'outsourc')],\n",
      "  -14.058388303902316),\n",
      " ([(0.0006788168, 'pornographi'),\n",
      "   (0.0006450211, 'broken'),\n",
      "   (0.00060381024, 'miss'),\n",
      "   (0.0005595609, 'hardcor'),\n",
      "   (0.00055097963, 'surpris'),\n",
      "   (0.0004883445, 'hooch'),\n",
      "   (0.00046839565, 'cooki'),\n",
      "   (0.0004634293, 'cat'),\n",
      "   (0.00046036084, 'heart'),\n",
      "   (0.00043922183, 'sound')],\n",
      "  -14.21562454531581),\n",
      " ([(0.00077740283, 'sooooooo'),\n",
      "   (0.00073734473, 'shirt'),\n",
      "   (0.0006085928, 'heh'),\n",
      "   (0.00055359135, 'nice'),\n",
      "   (0.00052357663, 'tire'),\n",
      "   (0.0005226091, 'bring'),\n",
      "   (0.000510497, 'googl'),\n",
      "   (0.00048357795, 'humili'),\n",
      "   (0.00048015607, 'nascent'),\n",
      "   (0.00047170607, 'sai')],\n",
      "  -14.223125313335125),\n",
      " ([(0.00060677197, 'serious'),\n",
      "   (0.0005834114, 'casevil'),\n",
      "   (0.00051253376, 'corni'),\n",
      "   (0.0004829223, 'go'),\n",
      "   (0.00043815232, 'wow'),\n",
      "   (0.00039894902, 'corner'),\n",
      "   (0.00039137295, 'vol'),\n",
      "   (0.00038973353, 'dumb'),\n",
      "   (0.00038774908, 'flint'),\n",
      "   (0.0003669704, 'william')],\n",
      "  -14.407462855616448),\n",
      " ([(0.0006029917, 'outsid'),\n",
      "   (0.00054836913, 'glad'),\n",
      "   (0.00045087247, 'week'),\n",
      "   (0.000445482, 'bit'),\n",
      "   (0.00043385892, 'snooz'),\n",
      "   (0.00043265827, 'footbal'),\n",
      "   (0.00043064693, 'underscor'),\n",
      "   (0.00041196425, 'chang'),\n",
      "   (0.00039791883, 'wholesom'),\n",
      "   (0.00039044707, 'exquisit')],\n",
      "  -14.575615951298186),\n",
      " ([(0.0006278243, 'clean'),\n",
      "   (0.0005876908, 'templat'),\n",
      "   (0.00055524154, 'vol'),\n",
      "   (0.00046381788, 'wowzer'),\n",
      "   (0.00040868804, 'polyphon'),\n",
      "   (0.0003943035, 'yai'),\n",
      "   (0.00039166462, 'talk'),\n",
      "   (0.00038760222, 'drumlin'),\n",
      "   (0.00038455782, 'fun'),\n",
      "   (0.00036913357, 'class')],\n",
      "  -14.637960333927069),\n",
      " ([(0.0007594917, 'hate'),\n",
      "   (0.00072935707, 'tonight'),\n",
      "   (0.00063100265, 'puzzl'),\n",
      "   (0.0005913104, 'windscreen'),\n",
      "   (0.00058402034, 'alabama'),\n",
      "   (0.0004941364, 'peter'),\n",
      "   (0.00043422292, 'believ'),\n",
      "   (0.00042455248, 'glad'),\n",
      "   (0.00041767725, 'brainstorm'),\n",
      "   (0.00037864855, 'lauren')],\n",
      "  -14.94263618198263),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ([(0.00080119923, 'bush'),\n",
      "   (0.00059223134, 'sleep'),\n",
      "   (0.00056553405, 'lobster'),\n",
      "   (0.00052274344, 'maureen'),\n",
      "   (0.0005007983, 'rememb'),\n",
      "   (0.00047449977, 'said'),\n",
      "   (0.00044460618, 'toalli'),\n",
      "   (0.00044010833, 'realllli'),\n",
      "   (0.0004400785, 'bundt'),\n",
      "   (0.00043843355, 'googl')],\n",
      "  -15.012385442490684),\n",
      " ([(0.000815056, 'matthew'),\n",
      "   (0.00081146194, 'scare'),\n",
      "   (0.0007728831, 'lobster'),\n",
      "   (0.0006302655, 'darnit'),\n",
      "   (0.00058081135, 'hangov'),\n",
      "   (0.0005483271, 'foghorn'),\n",
      "   (0.0005319924, 'todai'),\n",
      "   (0.00052539236, 'fun'),\n",
      "   (0.0005135629, 'slight'),\n",
      "   (0.0004939447, 'reallllllli')],\n",
      "  -15.019910397852403),\n",
      " ([(0.0008364299, 'bryan'),\n",
      "   (0.0005752341, 'liver'),\n",
      "   (0.0005581034, 'document'),\n",
      "   (0.0005571578, 'jame'),\n",
      "   (0.0005174334, 'note'),\n",
      "   (0.00045427412, 'comment'),\n",
      "   (0.00043086213, 'style'),\n",
      "   (0.00041285687, 'quicktop'),\n",
      "   (0.00040050503, 'sober'),\n",
      "   (0.00038258565, 'yum')],\n",
      "  -15.340629519094088),\n",
      " ([(0.00082019524, 'midget'),\n",
      "   (0.0007386758, 'cub'),\n",
      "   (0.00072167866, 'ocur'),\n",
      "   (0.0006976163, 'spymac'),\n",
      "   (0.00063173246, 'homework'),\n",
      "   (0.00045464074, 'wow'),\n",
      "   (0.0004223441, 'went'),\n",
      "   (0.00041785868, 'repres'),\n",
      "   (0.00041406878, 'lot'),\n",
      "   (0.00038575742, 'naivet')],\n",
      "  -15.98472560096529),\n",
      " ([(0.00073765893, 'rasputina'),\n",
      "   (0.0007244106, 'need'),\n",
      "   (0.0005742322, 'isn'),\n",
      "   (0.0005065306, 'music'),\n",
      "   (0.00044110263, 'downsid'),\n",
      "   (0.00042445725, 'hawaii'),\n",
      "   (0.00041206958, 'warsaw'),\n",
      "   (0.0004049566, 'got'),\n",
      "   (0.00040381483, 'miss'),\n",
      "   (0.0003976617, 'virginia')],\n",
      "  -16.18685093208063),\n",
      " ([(0.0006861292, 'text'),\n",
      "   (0.0006824983, 'abstin'),\n",
      "   (0.00066470355, 'thinkin'),\n",
      "   (0.00058568927, 'clap'),\n",
      "   (0.0005811598, 'chri'),\n",
      "   (0.00056445383, 'cla'),\n",
      "   (0.00056372525, 'parallel'),\n",
      "   (0.00056172675, 'dinner'),\n",
      "   (0.00051091437, 'good'),\n",
      "   (0.0004903084, 'honest')],\n",
      "  -16.514371461393768),\n",
      " ([(0.00065836083, 'slightli'),\n",
      "   (0.0005550371, 'sniffer'),\n",
      "   (0.00055462786, 'mood'),\n",
      "   (0.0005165585, 'liar'),\n",
      "   (0.00048898224, 'significantli'),\n",
      "   (0.00044103633, 'enlarg'),\n",
      "   (0.00043966435, 'eat'),\n",
      "   (0.000434796, 'mexico'),\n",
      "   (0.00041712966, 'fantast'),\n",
      "   (0.00041216478, 'sad')],\n",
      "  -16.88246285711652),\n",
      " ([(0.0004629286, 'lawyer'),\n",
      "   (0.00045322545, 'ramon'),\n",
      "   (0.00044957123, 'poland'),\n",
      "   (0.00044728554, 'insur'),\n",
      "   (0.00041010723, 'pretti'),\n",
      "   (0.0004069079, 'think'),\n",
      "   (0.00039949897, 'vibrat'),\n",
      "   (0.00039911308, 'joann'),\n",
      "   (0.000394218, 'cigar'),\n",
      "   (0.0003926021, 'nadu')],\n",
      "  -16.93503922252419),\n",
      " ([(0.0005103785, 'yai'),\n",
      "   (0.00049570476, 'edgewat'),\n",
      "   (0.00046268338, 'dekalb'),\n",
      "   (0.00045824278, 'babysit'),\n",
      "   (0.000437364, 'bribe'),\n",
      "   (0.00040401408, 'your'),\n",
      "   (0.00039343367, 'flatter'),\n",
      "   (0.0003823369, 'write'),\n",
      "   (0.0003809524, 'irv'),\n",
      "   (0.00037410425, 'whatnot')],\n",
      "  -17.053006422807474),\n",
      " ([(0.0007551523, 'softbal'),\n",
      "   (0.00069915684, 'accumul'),\n",
      "   (0.0006092102, 'miss'),\n",
      "   (0.0005886192, 'heather'),\n",
      "   (0.00048876164, 'roxass'),\n",
      "   (0.00044642313, 'year'),\n",
      "   (0.00043590818, 'kelli'),\n",
      "   (0.00042825847, 'taco'),\n",
      "   (0.00042817494, 'bluebird'),\n",
      "   (0.00040713273, 'iit')],\n",
      "  -17.0796026386667),\n",
      " ([(0.0009138431, 'write'),\n",
      "   (0.0007749732, 'absolut'),\n",
      "   (0.0005896361, 'pimpin'),\n",
      "   (0.00053820055, 'understat'),\n",
      "   (0.0004941595, 'march'),\n",
      "   (0.00048078102, 'enegi'),\n",
      "   (0.00046619322, 'joust'),\n",
      "   (0.0004612279, 'ahhhh'),\n",
      "   (0.0004516145, 'morn'),\n",
      "   (0.0004449824, 'subtl')],\n",
      "  -17.133722759383247),\n",
      " ([(0.00070375996, 'jso'),\n",
      "   (0.00062070903, 'wayyyyi'),\n",
      "   (0.0005525256, 'yai'),\n",
      "   (0.00054955203, 'earn'),\n",
      "   (0.000548959, 'sleep'),\n",
      "   (0.00050051895, 'nme'),\n",
      "   (0.00048635452, 'sleepytim'),\n",
      "   (0.00043027714, 'ingredient'),\n",
      "   (0.00042272976, 'right'),\n",
      "   (0.0004181849, 'yah')],\n",
      "  -17.341174494635386),\n",
      " ([(0.0006831275, 'happi'),\n",
      "   (0.0006216846, 'birthdai'),\n",
      "   (0.0005424349, 'senior'),\n",
      "   (0.00047003428, 'brmc'),\n",
      "   (0.0004419198, 'sov'),\n",
      "   (0.00037886784, 'hawai'),\n",
      "   (0.0003749707, 'harrass'),\n",
      "   (0.00036212723, 'eee'),\n",
      "   (0.00032625138, 'fur'),\n",
      "   (0.0003197032, 'joni')],\n",
      "  -17.52595762675485),\n",
      " ([(0.0008041157, 'shock'),\n",
      "   (0.00067392376, 'bank'),\n",
      "   (0.0006436238, 'darnit'),\n",
      "   (0.00057179335, 'evil'),\n",
      "   (0.0004808514, 'dissapoint'),\n",
      "   (0.0004752665, 'entrepreneur'),\n",
      "   (0.000458334, 'infosi'),\n",
      "   (0.00043575934, 'board'),\n",
      "   (0.00042606742, 'peter'),\n",
      "   (0.00034986733, 'zone')],\n",
      "  -17.69875516697091),\n",
      " ([(0.00071949355, 'whoooooo'),\n",
      "   (0.00059180276, 'googl'),\n",
      "   (0.0005192695, 'splendid'),\n",
      "   (0.0004573382, 'yeahhhhhh'),\n",
      "   (0.00045565664, 'halloween'),\n",
      "   (0.00041074867, 'final'),\n",
      "   (0.0003992476, 'sungtin'),\n",
      "   (0.00039924757, 'bankruptc'),\n",
      "   (0.0003951757, 'tonight'),\n",
      "   (0.00037908816, 'highwai')],\n",
      "  -18.27322704629197),\n",
      " ([(0.0006375803, 'bike'),\n",
      "   (0.0005563567, 'nihilist'),\n",
      "   (0.00048143472, 'sad'),\n",
      "   (0.00038411317, 'frequent'),\n",
      "   (0.00038302495, 'bumm'),\n",
      "   (0.00037889867, 'peyot'),\n",
      "   (0.00033913724, 'honest'),\n",
      "   (0.00033109463, 'upset'),\n",
      "   (0.0003287417, 'indeedi'),\n",
      "   (0.0003038205, 'loathsom')],\n",
      "  -18.329122801065097),\n",
      " ([(0.0006734238, 'lyric'),\n",
      "   (0.000622599, 'guess'),\n",
      "   (0.0006024026, 'bryan'),\n",
      "   (0.000586918, 'haha'),\n",
      "   (0.00057628297, 'shifti'),\n",
      "   (0.0005517312, 'night'),\n",
      "   (0.000548476, 'ouyt'),\n",
      "   (0.0005326359, 'complt'),\n",
      "   (0.000527546, 'pierr'),\n",
      "   (0.0005137481, 'spero')],\n",
      "  -18.331760146564353),\n",
      " ([(0.0008432185, 'conman'),\n",
      "   (0.00072225736, 'bloglin'),\n",
      "   (0.00069677434, 'fun'),\n",
      "   (0.00067896134, 'heh'),\n",
      "   (0.00067797, 'soooooooooooooo'),\n",
      "   (0.0006233712, 'pbr'),\n",
      "   (0.00061596034, 'taxi'),\n",
      "   (0.00059380045, 'jameson'),\n",
      "   (0.00058558676, 'go'),\n",
      "   (0.00053384964, 'kel')],\n",
      "  -18.366731250415658),\n",
      " ([(0.00048431868, 'dun'),\n",
      "   (0.0004504166, 'cancun'),\n",
      "   (0.0004205793, 'hmmmmmm'),\n",
      "   (0.00041663626, 'anim'),\n",
      "   (0.00041268225, 'stuf'),\n",
      "   (0.00036293236, 'dovi'),\n",
      "   (0.00035991348, 'brilliant'),\n",
      "   (0.00035866737, 'asp'),\n",
      "   (0.0003354416, 'pooei'),\n",
      "   (0.00033530418, 'hunter')],\n",
      "  -18.3884038725678),\n",
      " ([(0.00071792415, 'whee'),\n",
      "   (0.00060178706, 'haha'),\n",
      "   (0.000599721, 'iit'),\n",
      "   (0.00058910734, 'gloss'),\n",
      "   (0.0005466903, 'jazz'),\n",
      "   (0.00054017495, 'sneaki'),\n",
      "   (0.0005117506, 'theo'),\n",
      "   (0.0005044662, 'shower'),\n",
      "   (0.00048430063, 'vanilla'),\n",
      "   (0.0004585557, 'babysit')],\n",
      "  -18.737948598567222),\n",
      " ([(0.0005940771, 'margarita'),\n",
      "   (0.000590718, 'quiet'),\n",
      "   (0.00056947407, 'pitcher'),\n",
      "   (0.0004827988, 'goodnight'),\n",
      "   (0.00045998814, 'ecstat'),\n",
      "   (0.00043251456, 'weiner'),\n",
      "   (0.00041015452, 'perfectli'),\n",
      "   (0.0003700025, 'gatsbi'),\n",
      "   (0.00034623468, 'hahahahahahaha'),\n",
      "   (0.00034496785, 'slap')],\n",
      "  -19.511253921457516)]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(lda.top_topics(corpus, topn=10))\n",
    "\n",
    "# delete the model to conserve RAM space.\n",
    "# We've already saved it to disk, so it can be reloaded.\n",
    "del lda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now author-topic models.  These are LDA with fancy metadata capabilities--in this case, the author is the metadata.  These models will let us retrieve the overall topics in our corpus (which, of course, will be a bit different from LDA's, since they include the author as metadata during the modelling process), but it also lets us see the topics on a per-author basis!\n",
    "\n",
    "Of course, we need to tell the model who write each document.  So we need to quickly read the big Dataframe back off the disk, but fortunately, we can tell it to only pull a single column and not the entire file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running author-topic model...\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "arrays used as indices must be of integer (or boolean) type",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mc:\\users\\andersonh\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\gensim\\models\\atmodel.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, corpus, num_topics, id2word, author2doc, doc2author, chunksize, passes, iterations, decay, offset, alpha, eta, update_every, eval_every, gamma_threshold, serialized, serialization_path, minimum_probability, random_state)\u001b[0m\n\u001b[0;32m    304\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mauthor2doc\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mdoc2author\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    305\u001b[0m             \u001b[0muse_numpy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatcher\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 306\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mauthor2doc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdoc2author\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunks_as_numpy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_numpy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    307\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    308\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__str__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\andersonh\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\gensim\\models\\atmodel.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, corpus, author2doc, doc2author, chunksize, decay, offset, passes, update_every, eval_every, iterations, gamma_threshold, chunks_as_numpy)\u001b[0m\n\u001b[0;32m    743\u001b[0m                     \u001b[1;31m# log_perplexity requires the indexes of the documents being evaluated, to know what authors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    744\u001b[0m                     \u001b[1;31m# correspond to the documents.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 745\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_perplexity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunk_doc_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_docs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlencorpus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    746\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    747\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatcher\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\andersonh\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\gensim\\models\\atmodel.py\u001b[0m in \u001b[0;36mlog_perplexity\u001b[1;34m(self, chunk, chunk_doc_idx, total_docs)\u001b[0m\n\u001b[0;32m    516\u001b[0m         \u001b[0mcorpus_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcnt\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdocument\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mchunk\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcnt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdocument\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    517\u001b[0m         \u001b[0msubsample_ratio\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1.0\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mtotal_docs\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 518\u001b[1;33m         \u001b[0mperwordbound\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbound\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunk_doc_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubsample_ratio\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msubsample_ratio\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    519\u001b[0m                        \u001b[1;33m(\u001b[0m\u001b[0msubsample_ratio\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mcorpus_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    520\u001b[0m         logger.info(\n",
      "\u001b[1;32mc:\\users\\andersonh\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\gensim\\models\\atmodel.py\u001b[0m in \u001b[0;36mbound\u001b[1;34m(self, chunk, chunk_doc_idx, subsample_ratio, author2doc, doc2author)\u001b[0m\n\u001b[0;32m    878\u001b[0m             \u001b[1;31m# Computing the bound requires summing over expElogtheta[a, k] * expElogbeta[k, v], which\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m             \u001b[1;31m# is the same computation as in normalizing phi.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 880\u001b[1;33m             \u001b[0mphinorm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_phinorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexpElogtheta\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mauthors_d\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexpElogbeta\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    881\u001b[0m             \u001b[0mword_score\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.0\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mauthors_d\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcts\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mcts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mphinorm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: arrays used as indices must be of integer (or boolean) type"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# read the author ID list and make a dictionary\n",
    "authors = pd.read_csv(\n",
    "    \"corpus data files/Blog Data.csv\",\n",
    "    usecols=[\"Author ID\"],\n",
    "    squeeze=True # return a series, not dataframe, if only one column\n",
    ").values\n",
    "doc2author = dict(\n",
    "    (i, [j])\n",
    "    for i,j in enumerate(authors)\n",
    ")\n",
    "\n",
    "print(\"Running author-topic model...\")\n",
    "at_model = AuthorTopicModel(\n",
    "    corpus, \n",
    "    id2word=id2word, \n",
    "    doc2author=doc2author, \n",
    "    chunksize=50000\n",
    ")\n",
    "print(\"Saving Author-Topic model...\")\n",
    "atmodel.save(\"topic models/AuthorTopic.model\")\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'at_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-55-810d4328c988>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# their associated topics\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0muser_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mauthors\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mat_model\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0muser_id\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mtopic_num\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mtopic_prob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'at_model' is not defined"
     ]
    }
   ],
   "source": [
    "# pick the first author in our list and print\n",
    "# their associated topics\n",
    "user_id = authors[0]\n",
    "for i in at_model[user_id]:\n",
    "    topic_num = i[0]\n",
    "    topic_prob = i[1]\n",
    "    print(f\"Topic #{topic_num} with weight {topic_prob} for author {user_id}\")\n",
    "    pprint(at_model.show_topic(topic_num, topn=10))\n",
    "    # delete to conserve memory--author-topic models tend to be large\n",
    "del at_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now finally, the Hierarchical Dirichlet Process, which is an extension to LDA that doesn't require us to specify the number of topics.  But, the outputs can be a bit less interpretable.  HDP may not be very useful on its own, but it can be helpful to get a sense of how many topics might be in your corpus, rathre than you having to try a few values and just see what looks best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, let's look at HDP's output, again sorted by the most likely topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running HDP model...\n",
      "Saving HDP model...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# HDP is nonparametric--don't need to specify topic number\n",
    "print(\"Running HDP model...\")\n",
    "hdp = HdpModel(\n",
    "    corpus,\n",
    "    id2word=id2word,\n",
    "    chunksize=50000\n",
    ")\n",
    "print(\"Saving HDP model...\")\n",
    "hdp.save(\"topic models/HDP.model\")\n",
    "print(\"Done.\")\n",
    "\n",
    "# print(hdp.top_topics(corpus, topn=10))\n",
    "# # delete to conserve memory--HDP tends to be large\n",
    "# del htp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll do a bit of trickery to sort the topics by their total probability before printing them--we sum the per-word probabilities for each topic and use that as the total \"topic probability,\" sort topics by that value, then print them out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e4705245582468e9afd10b7f8784a2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=150), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0.00765860511501269,\n",
       "  119,\n",
       "  [('heimel', 0.00010786828621361057),\n",
       "   ('teva', 0.00010611203794939805),\n",
       "   ('whoopdeedoo', 0.00010410025999594411),\n",
       "   ('settl', 9.665632099591238e-05),\n",
       "   ('drink\\x80\\x9d', 9.607057045794308e-05),\n",
       "   ('rideabl', 9.553550593412965e-05),\n",
       "   ('chivalri', 9.538295023949914e-05),\n",
       "   ('bezzi', 9.311551590961935e-05),\n",
       "   ('kristoff', 9.279642996368985e-05),\n",
       "   ('noddin', 9.2792792764035e-05)]),\n",
       " (0.007649368392438683,\n",
       "  54,\n",
       "  [('biotch', 0.00014561632159303974),\n",
       "   ('importun', 0.00013532468270990552),\n",
       "   ('filipiak', 0.00011717478767125579),\n",
       "   ('liligawan', 0.00010961590427880921),\n",
       "   ('tww', 0.0001067660413201152),\n",
       "   ('deuteronomi', 0.0001058992192994904),\n",
       "   ('iwm', 0.00010389708567431784),\n",
       "   ('sticki', 9.939387425847887e-05),\n",
       "   ('inkl', 9.922328718036606e-05),\n",
       "   ('me\\x80\\x80\\x80', 9.445776073847712e-05)]),\n",
       " (0.007647974348110592,\n",
       "  90,\n",
       "  [('ecard', 0.00011226489426619887),\n",
       "   ('apoyo', 0.00010523708513800138),\n",
       "   ('gibbo', 9.841082011486654e-05),\n",
       "   ('hmmmmmmmmmm', 9.703984789409034e-05),\n",
       "   ('crone', 9.540651760645326e-05),\n",
       "   ('isent', 9.403065085331443e-05),\n",
       "   ('overun', 9.399765148416782e-05),\n",
       "   ('escort', 9.265364214990321e-05),\n",
       "   ('dyin\\x80\\x99', 9.127239749651687e-05),\n",
       "   ('percoset', 9.074358183093527e-05)]),\n",
       " (0.007627726027584553,\n",
       "  108,\n",
       "  [('nelh', 0.00011837147416964092),\n",
       "   ('diel', 0.00011010133116275452),\n",
       "   ('omfg', 0.00010507443714817385),\n",
       "   ('nek', 9.886770205005297e-05),\n",
       "   ('\\x80\\x9cless', 9.804133446170576e-05),\n",
       "   ('blik', 9.511913615324449e-05),\n",
       "   ('blackcatgam', 9.325566893947066e-05),\n",
       "   ('tmz', 9.271670822981863e-05),\n",
       "   ('preoccupi', 9.24296351746023e-05),\n",
       "   ('\\x80\\x9cbrass', 9.012834633528653e-05)]),\n",
       " (0.0076245278821696835,\n",
       "  79,\n",
       "  [('spesif', 0.00011478820360879493),\n",
       "   ('trollei', 0.00010891847300056537),\n",
       "   ('kav', 0.00010669081771727976),\n",
       "   ('cono', 0.00010516439927157027),\n",
       "   ('afr', 0.00010200834293767277),\n",
       "   ('asuka', 0.00010139029140646033),\n",
       "   ('cahoot', 9.878787866334132e-05),\n",
       "   ('darkess', 9.700463841558137e-05),\n",
       "   ('sbi', 9.256671318649122e-05),\n",
       "   ('mikado', 9.201998424041239e-05)]),\n",
       " (0.007610690162531216,\n",
       "  81,\n",
       "  [('aaahh', 0.0001210523888584825),\n",
       "   ('eckstein', 0.00010198317201942135),\n",
       "   ('rickshaw', 0.00010156137534547281),\n",
       "   ('pallett', 9.909024820999278e-05),\n",
       "   ('denen', 9.769821749600582e-05),\n",
       "   ('butthurt', 9.719777644949041e-05),\n",
       "   ('slex', 9.510462842281914e-05),\n",
       "   ('\\x80\\x98come', 9.426939623555641e-05),\n",
       "   ('clamdigg', 9.404788549335883e-05),\n",
       "   ('scid', 9.28633379570955e-05)]),\n",
       " (0.007573202318772645,\n",
       "  51,\n",
       "  [('nuddi', 0.00010006258481607201),\n",
       "   ('maanick', 9.753441323380666e-05),\n",
       "   ('conrad', 9.618450320660652e-05),\n",
       "   ('footer', 9.505948389880431e-05),\n",
       "   ('sumatra', 9.381343945209997e-05),\n",
       "   ('excis', 9.224108758118334e-05),\n",
       "   ('staid', 9.139277927054124e-05),\n",
       "   ('skor', 9.040761252333299e-05),\n",
       "   ('dorei', 9.012827769525086e-05),\n",
       "   ('bova', 8.745672506201313e-05)]),\n",
       " (0.007561024929401015,\n",
       "  21,\n",
       "  [('rashad', 0.00011979034410901741),\n",
       "   ('stephon', 0.00011414566445416904),\n",
       "   ('bmb', 0.00010894953806768214),\n",
       "   ('nessecari', 0.0001052127890004015),\n",
       "   ('calico', 0.00010418244386592985),\n",
       "   ('\\x80\\x9c\\x80\\x9d', 0.0001033351005824307),\n",
       "   ('vant', 0.00010329587846183635),\n",
       "   ('linker', 9.32995615667e-05),\n",
       "   ('nosh', 9.308280534135387e-05),\n",
       "   ('shrew', 8.891860707638792e-05)]),\n",
       " (0.007554738697877519,\n",
       "  106,\n",
       "  [('nige', 0.00010772962316447252),\n",
       "   ('hellrais', 9.811339431923525e-05),\n",
       "   ('steamwork', 9.805627744801564e-05),\n",
       "   ('hollidai', 9.597134495798398e-05),\n",
       "   ('nonent', 9.41513656416218e-05),\n",
       "   ('shirtless', 9.398185024097602e-05),\n",
       "   ('jangli', 9.32808350222759e-05),\n",
       "   ('parat', 9.302812084889734e-05),\n",
       "   ('brea', 9.136906001388581e-05),\n",
       "   ('democrazi', 8.926796665932431e-05)]),\n",
       " (0.007547860397666796,\n",
       "  116,\n",
       "  [('maintan', 0.00010618503628237747),\n",
       "   ('babyshambl', 0.000105379584091985),\n",
       "   ('jeunet', 0.00010347054383001659),\n",
       "   ('autum', 0.00010135889088128651),\n",
       "   ('winthrop', 0.00010115865445524256),\n",
       "   ('dew', 9.714567225354647e-05),\n",
       "   ('shibboleth', 9.599259206389207e-05),\n",
       "   ('quizilla', 9.236762747887022e-05),\n",
       "   ('slant', 9.166677801526614e-05),\n",
       "   ('worse\\x80', 9.056367803239121e-05)])]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hdp.optimal_ordering()\n",
    "topics = hdp.show_topics(-1, formatted=False, num_words=100)\n",
    "# get topic probabilities\n",
    "probs = [\n",
    "    sum(i[1] for i in j[1])\n",
    "    for j in topics\n",
    "]\n",
    "# reset the topics to only show the top 10 words\n",
    "topics = hdp.show_topics(-1, formatted=False, num_words=10)\n",
    "# zip them together\n",
    "topics = [\n",
    "    (probs[i], topics[i][0], topics[i][1]) \n",
    "    for i in range(len(topics))\n",
    "]\n",
    "topics = sorted(\n",
    "    topics,\n",
    "    key=lambda x: x[0],\n",
    "    reverse=True\n",
    ")\n",
    "topics[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings\n",
    "\n",
    "Word embeddings have absolutely taken over the entire field of NLP, starting with Mikolov et al's 2013 paper on [Word2Vec](https://arxiv.org/abs/1301.3781).  The basic idea of these embeddings:\n",
    "* Some notion of \"meaning\" is recoverable from the _contexts_ that a word occurs in\n",
    "* It is possible to generate a _vector_ representation of a word such that _words appearing in similar contexts have similar vectors_ (or, more intuitively, \"are close to each other\").\n",
    "\n",
    "Word vectors are used for almost every kind of task: document scoring and classification, sentiment analysis, document and word clustering, machine translation (though less commmonly), and more.  Plus, they've been demonstrated to excel at a number of lexical similarity and analogy tasks.\n",
    "\n",
    "Gensim has an impelementation of Word2Vec that we can use on our corpus.  spaCy, meanwhile, comes pre-bundled with word vectors computed using Stanford's GloVe algorithm (which works differently under the hood, but in terms of how well the vectors actually perform in any application, is basically identical).\n",
    "\n",
    "### Brief aside: Poincare Embeddings\n",
    "In May 2017, Maximilian Nickel and Douwe Kiela published an extremely exciting paper on embeddings performed in _hyperbolic space_ rather than _Euclidean space_ (all prior models were in Euclidean space).  These embeddings--while not yet well-suited to extracting word similarities and meanings from large bodies of unlabeled text--show an extreme aptitude for embedding _graph_- and _network_-like data, e.g. WordNet, and can capture various relations like hypernymy/hyponymy and synonymy quite well.  Keep an eye on \"Poincare Embeddings\"--lots of interesting things are sure to happen there soon.\n",
    "\n",
    "### spaCy word vectors\n",
    "We'll work with spaCy's pre-trained vectors just for time's sake--training word2vec models in Gensim is surprisingly fast, but we'll be doing basically the same thing downstream.  First, as always, we need to load our data and our model.  We'll just use a single blog post, and we'll turn off a bunch of the spaCy parsing stuff for speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# just use the very first post for demos\n",
    "demo_post = pd.read_csv(\n",
    "    \"corpus data files/Blog Data.csv\",\n",
    "    usecols=[\"Post\"],\n",
    "    nrows=1,\n",
    "    squeeze=True\n",
    ")[0]\n",
    "demo_post = nlp(demo_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Well\n",
      "[-1.2486e-01  6.9180e-02 -3.1364e-01 -3.1354e-01  1.4388e-01  1.6573e-01\n",
      " -4.0073e-02 -3.4590e-01 -1.7483e-01  2.6147e+00  9.1120e-03  1.8054e-02\n",
      "  8.3494e-02 -9.3186e-02 -1.0852e-01 -1.4856e-01  1.4402e-01  1.1995e+00\n",
      " -5.1814e-01 -4.3844e-02 -2.8039e-01  1.3527e-01 -3.6054e-02 -1.3734e-01\n",
      "  3.1807e-02  1.7668e-02  4.7540e-02 -3.0738e-02  1.7169e-01 -1.0349e-01\n",
      "  1.0784e-01  1.9757e-02  6.9675e-02 -1.5200e-01 -1.9508e-01 -1.7867e-01\n",
      "  1.1583e-01  4.7459e-02 -4.5048e-02 -1.0148e-02 -6.7003e-02  3.0717e-02\n",
      " -9.5259e-02 -2.2538e-02  8.2868e-02  1.9983e-01 -1.2923e-01 -1.3680e-01\n",
      "  2.9010e-02  1.8272e-01  2.2101e-02  1.5804e-01  3.7986e-02  3.1765e-02\n",
      " -3.0443e-03  3.1779e-02 -9.1168e-02  3.6951e-02  4.4161e-02  1.0407e-01\n",
      "  1.5687e-02 -9.7470e-02  4.2405e-02  2.6701e-01 -1.0596e-01 -1.4289e-01\n",
      " -6.7763e-02  1.7992e-01  2.6175e-01  4.5349e-02  2.5674e-01  1.2484e-01\n",
      "  3.6875e-01  7.5486e-02 -1.4867e-01  8.2897e-03  8.9685e-02 -1.3242e-01\n",
      " -2.4698e-01  2.3017e-01 -2.4372e-03  1.8298e-01 -2.2386e-01  1.7260e-01\n",
      " -2.6223e-02 -3.0136e-01 -1.8803e-01  1.0426e-01 -4.1197e-02  6.9884e-02\n",
      " -3.3139e-03 -1.4249e-01  4.4817e-02  3.6930e-01  5.2759e-01 -1.4461e-01\n",
      "  2.2260e-01 -2.1296e-01 -2.1575e-01 -2.1324e-02 -2.3094e-01 -9.1427e-02\n",
      " -1.3952e-01 -6.5416e-02  9.9365e-02 -1.1914e+00 -1.0889e-01 -3.6120e-01\n",
      " -6.4289e-02 -1.4312e-01  9.1272e-03 -1.6777e-01  2.1744e-01 -4.6958e-02\n",
      "  7.1629e-03 -8.1238e-03  4.3175e-02 -5.4813e-02 -1.1981e-01  4.2927e-02\n",
      "  2.3405e-01  1.0804e-01  1.9480e-01 -5.6464e-02  1.2108e-01  1.0770e-01\n",
      " -9.7876e-02 -2.4924e-01  2.4288e-01 -2.4311e-02 -8.8623e-02 -3.1197e-01\n",
      " -1.8214e-01  2.1236e-01  2.4771e-01  3.9045e-01  1.9372e-01 -4.0742e-01\n",
      "  1.3977e-01  1.5621e-01 -1.2266e+00  2.0881e-01  4.3454e-01 -1.0045e-01\n",
      " -1.3093e-01 -2.2994e-01 -2.5303e-01 -5.7255e-02  4.4188e-02  3.9772e-03\n",
      " -2.1284e-01  2.0268e-01  1.5135e-01 -8.1819e-03 -1.3508e-01 -6.2223e-02\n",
      "  1.2250e-01 -7.0757e-02 -5.4248e-02 -4.5870e-01  2.6266e-01  1.4630e-01\n",
      "  4.0067e-02 -1.7087e-01 -2.3487e-02 -2.6435e-01  5.8678e-02 -7.8438e-02\n",
      "  3.0261e-01 -1.4065e-01 -5.5911e-02  1.8330e-01 -1.0979e-01 -7.1047e-02\n",
      " -1.7407e-01 -1.4674e-01 -2.8062e-01  1.3984e-01  1.1339e-01  1.0495e-01\n",
      " -1.4357e-01 -1.2663e-01 -3.4565e-01 -1.4041e-01 -1.2706e-01 -2.8720e-01\n",
      " -2.9107e-02  5.2271e-02  1.9180e-01 -5.9188e-02  1.1118e-02 -3.6953e-02\n",
      " -4.6103e-02 -5.5561e-02 -4.2408e-02 -5.4556e-02 -8.5381e-02 -2.5072e-01\n",
      "  1.3272e-01  8.9385e-02  5.0679e-02 -1.4402e-01  1.5704e-02  8.2595e-02\n",
      "  2.8950e-01  1.1995e-01  8.2218e-03  8.6943e-02 -1.4487e-01 -6.4140e-02\n",
      " -2.8995e-01 -4.3396e-02  1.0147e-01 -3.6916e-01  1.3197e-01  2.2112e-01\n",
      "  2.2586e-01 -1.3024e-01  3.4924e-02  1.7595e-01 -5.5850e-04  1.4002e-01\n",
      "  7.2031e-02  9.4518e-02  2.5377e-01  1.5240e-01 -9.7878e-02  8.5805e-02\n",
      "  3.9108e-02  1.6801e-01 -2.7371e-01 -8.4246e-02 -1.9891e-01  1.9488e-01\n",
      "  1.0244e-01 -2.1371e-01  1.9158e-01 -4.9702e-01 -7.6887e-02  1.3054e-01\n",
      "  1.4384e-01  9.2271e-02 -2.4776e-01  2.9863e-01  4.5713e-01  6.3761e-02\n",
      " -2.8826e-01 -1.8834e-01 -6.4870e-02  2.0241e-01  2.9338e-01 -1.1416e-01\n",
      " -2.9192e-01 -1.3655e-01  5.2752e-02  2.9894e-02  3.6916e-01  1.2743e-02\n",
      " -2.2503e-01  8.0966e-02  3.2966e-01  1.3930e-01 -7.8549e-02  1.1004e-01\n",
      " -9.0624e-02 -1.9984e-02  2.2853e-02  3.1763e-03  6.1005e-01  2.6677e-01\n",
      "  4.9331e-02 -2.5631e-01 -2.4592e-01 -3.0870e-01 -4.1584e-01  3.6741e-01\n",
      "  1.0777e-01 -1.3235e-02 -8.0141e-02  4.4847e-01  2.7414e-01  1.1039e-01\n",
      " -8.1114e-02 -1.6639e-01  1.8136e-02  7.6002e-02  2.0605e-01 -1.8203e-01\n",
      "  2.9575e-01  5.4778e-02 -4.6968e-01  1.5817e-02 -2.2619e-01  1.1062e-02\n",
      "  1.8545e-01 -1.1914e-01  2.1583e-01 -4.0342e-01  1.7759e-01  8.9240e-02]\n"
     ]
    }
   ],
   "source": [
    "print(demo_post[1])\n",
    "print(demo_post[1].vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token          \tSimilarity to 'Well'\n",
      "Well           \t1.000\n",
      "everyone       \t0.648\n",
      "got            \t0.552\n",
      "up             \t0.602\n",
      "and            \t0.682\n",
      "going          \t0.631\n",
      "this           \t0.604\n",
      "morning        \t0.387\n",
      "It             \t0.714\n",
      "'s             \t0.412\n",
      "still          \t0.710\n",
      "raining        \t0.262\n",
      "but            \t0.818\n",
      "that           \t0.724\n",
      "'s             \t0.412\n",
      "okay           \t0.578\n",
      "with           \t0.610\n",
      "me             \t0.549\n",
      "Sort           \t0.516\n",
      "of             \t0.440\n",
      "suits          \t0.380\n",
      "my             \t0.503\n",
      "mood           \t0.424\n",
      "could          \t0.681\n",
      "easily         \t0.561\n",
      "have           \t0.713\n",
      "stayed         \t0.429\n",
      "home           \t0.466\n",
      "in             \t0.478\n",
      "bed            \t0.333\n",
      "with           \t0.610\n",
      "my             \t0.503\n",
      "book           \t0.397\n",
      "and            \t0.682\n",
      "the            \t0.530\n",
      "cats           \t0.330\n",
      "This           \t0.604\n",
      "has            \t0.597\n",
      "been           \t0.653\n",
      "lot            \t0.624\n",
      "of             \t0.440\n",
      "rain           \t0.312\n",
      "though         \t0.786\n",
      "People         \t0.576\n",
      "have           \t0.713\n",
      "wet            \t0.373\n",
      "basements      \t0.185\n",
      "there          \t0.740\n",
      "are            \t0.647\n",
      "lakes          \t0.296\n",
      "where          \t0.644\n",
      "there          \t0.740\n",
      "should         \t0.659\n",
      "be             \t0.697\n",
      "golf           \t0.289\n",
      "courses        \t0.369\n",
      "and            \t0.682\n",
      "fields         \t0.370\n",
      "everything     \t0.653\n",
      "is             \t0.575\n",
      "green          \t0.350\n",
      "green          \t0.350\n",
      "green          \t0.350\n",
      "But            \t0.818\n",
      "it             \t0.714\n",
      "is             \t0.575\n",
      "supposed       \t0.584\n",
      "to             \t0.521\n",
      "be             \t0.697\n",
      "26             \t0.125\n",
      "degrees        \t0.296\n",
      "by             \t0.273\n",
      "Friday         \t0.212\n",
      "so             \t0.800\n",
      "we             \t0.652\n",
      "'ll            \t0.609\n",
      "be             \t0.697\n",
      "dealing        \t0.479\n",
      "with           \t0.610\n",
      "mosquitos      \t0.110\n",
      "next           \t0.472\n",
      "week           \t0.402\n",
      "heard          \t0.490\n",
      "Winnipeg       \t0.018\n",
      "described      \t0.457\n",
      "as             \t0.746\n",
      "an             \t0.482\n",
      "Old            \t0.396\n",
      "Testament      \t0.324\n",
      "city           \t0.342\n",
      "on             \t0.429\n",
      "urlLink        \t0.000\n",
      "CBC            \t-0.045\n",
      "Radio          \t0.283\n",
      "One            \t0.688\n",
      "last           \t0.486\n",
      "week           \t0.402\n",
      "and            \t0.682\n",
      "it             \t0.714\n",
      "sort           \t0.516\n",
      "of             \t0.440\n",
      "rings          \t0.292\n",
      "true           \t0.512\n",
      "Floods         \t0.191\n",
      "infestations   \t0.126\n",
      "etc            \t0.522\n",
      "etc            \t0.522\n",
      "..             \t0.000\n"
     ]
    }
   ],
   "source": [
    "print(f\"{'Token':<15s}\\tSimilarity to '{demo_post[1].text}'\")\n",
    "\n",
    "for i in demo_post:\n",
    "    # try/except because some token comparisons throw errors on spaCy's end\n",
    "    try:\n",
    "        print(f\"{i.text:<15s}\\t{demo_post[1].similarity(i):.3f}\")\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word vectors can also be extended to computer vectors for entire documents.  Usually, this is done by simply adding or averaging the vectors for each individual word in the document.  (Incidentally, this exact same approach lets you get a vector for any arbitrary bit of text!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.31642865e-02  2.37451762e-01 -1.33002967e-01 -1.58166736e-01\n",
      "  9.16091949e-02  4.56015505e-02 -1.51400210e-03 -1.62576497e-01\n",
      " -7.07884729e-02  2.12497878e+00 -1.87663123e-01  2.05625482e-02\n",
      "  7.54114017e-02 -6.97415769e-02 -1.99725553e-01 -5.24708331e-02\n",
      " -2.69564018e-02  1.06616330e+00 -1.82011917e-01 -4.86053340e-02\n",
      " -5.35848886e-02  2.18726844e-02 -5.59682995e-02 -3.07797678e-02\n",
      "  2.45223865e-02 -1.28578171e-02 -1.20025635e-01 -3.46809365e-02\n",
      "  7.66581818e-02 -7.82760903e-02 -2.33661868e-02  3.36482115e-02\n",
      " -4.14343439e-02  2.24824902e-02  5.23805320e-02 -4.10539694e-02\n",
      "  4.96976711e-02  3.10765095e-02 -5.10399900e-02 -3.25182676e-02\n",
      " -1.58267170e-02  3.63390930e-02  2.94042882e-02 -3.33312154e-02\n",
      "  2.69403644e-02  1.00509606e-01 -1.64681092e-01 -4.72940765e-02\n",
      "  5.93143664e-02 -5.76425642e-02 -5.66710643e-02  9.40750092e-02\n",
      " -2.73930281e-02  2.85059139e-02  4.11496833e-02 -6.96792779e-03\n",
      " -1.54412789e-02 -4.68339510e-02  5.63570932e-02 -3.10750045e-02\n",
      " -6.35085851e-02 -5.31633161e-02 -4.81112249e-04  1.96307719e-01\n",
      "  1.30728194e-02 -7.01612234e-02 -2.12039668e-02  4.46476564e-02\n",
      " -2.95005795e-02  1.79688618e-01  4.91093583e-02  1.00777403e-01\n",
      "  1.98810115e-01 -6.47725305e-03  8.86943787e-02  5.91295697e-02\n",
      "  9.21648890e-02  3.84680904e-03 -6.41978085e-02  1.93143263e-01\n",
      " -4.62955497e-02  8.67331773e-02 -6.86863884e-02 -1.38426665e-02\n",
      "  4.33658510e-02 -2.46838838e-01  1.19494922e-01 -7.30767846e-02\n",
      "  2.77736813e-01  1.34293601e-01 -1.80763267e-02 -1.45396953e-02\n",
      "  4.63578245e-03  5.40753379e-02  1.41394556e-01 -2.58688144e-02\n",
      "  1.91447865e-02  5.76385530e-03 -5.65039255e-02  1.07032163e-02\n",
      " -2.87960656e-02  4.17623743e-02 -1.27336815e-01 -5.02941720e-02\n",
      "  6.15321919e-02 -5.78176796e-01  5.08880280e-02 -7.64341326e-03\n",
      "  1.52477883e-02  1.23977093e-02  3.75351422e-02 -1.55868232e-01\n",
      "  1.34658828e-01 -1.35198385e-01 -4.05474156e-02 -4.07232419e-02\n",
      "  1.43934898e-02 -8.09895433e-03 -1.22347800e-02 -9.73605067e-02\n",
      "  6.38979748e-02  2.94547584e-02  1.99218541e-02 -7.03123733e-02\n",
      "  7.94450473e-03  7.23707601e-02  1.59296431e-02 -1.17330894e-01\n",
      "  1.45509187e-02 -4.16304134e-02  2.19151489e-02 -8.63423571e-02\n",
      " -7.60464519e-02  5.95238097e-02  1.16800264e-01  4.88185436e-02\n",
      " -1.56893209e-03 -1.40665406e-02  2.85443738e-02  2.45357323e-02\n",
      " -1.26371944e+00  1.50656730e-01  2.11000651e-01 -2.11142749e-03\n",
      "  1.73429642e-02  2.67431065e-02 -6.83486834e-02  2.42024250e-02\n",
      "  5.20412624e-03 -5.24368025e-02 -5.55904321e-02  2.24909615e-02\n",
      "  1.34837732e-01  4.61816378e-02 -9.35395807e-02 -7.21071884e-02\n",
      " -8.52866769e-02 -6.69751465e-02 -4.74636741e-02 -7.50315413e-02\n",
      "  7.53237261e-03  2.78376527e-02 -3.41446958e-02 -5.79334050e-02\n",
      " -3.71873602e-02 -1.12035863e-01  2.50705909e-02 -5.78852706e-02\n",
      "  1.34372517e-01  1.50861293e-02 -3.01528946e-02  4.10258621e-02\n",
      " -9.67732910e-03 -4.56370153e-02 -1.12796240e-01  6.34944364e-02\n",
      "  6.69909967e-03 -3.51219364e-02  8.46422929e-03 -9.01845619e-02\n",
      "  2.66135065e-03 -7.27335662e-02 -1.45241603e-01 -4.90702353e-02\n",
      " -2.65349410e-02 -9.75504294e-02 -4.19219211e-02  1.10685546e-02\n",
      "  8.61035287e-02  2.06585191e-02  4.93311137e-02  7.12807402e-02\n",
      " -1.00086510e-01  2.63198689e-02 -1.67035796e-02  1.25243828e-01\n",
      " -1.05668139e-02 -1.19949043e-01 -2.14458443e-02  1.60552979e-01\n",
      " -9.00433362e-02 -7.66610503e-02 -5.50686717e-02  1.10140545e-02\n",
      "  1.95168465e-01 -1.84920114e-02  4.93176617e-02  1.34883001e-02\n",
      "  7.02754483e-02 -3.39424312e-02 -8.85552689e-02 -7.08585307e-02\n",
      "  4.05594474e-03 -1.70666635e-01  2.76947170e-02  9.22666490e-02\n",
      " -4.02171798e-02 -3.87982689e-02 -1.57628641e-01  7.38738105e-02\n",
      "  1.36203943e-02  6.59089442e-03 -2.76750401e-02  6.13207519e-02\n",
      "  2.38623507e-02 -2.92682089e-03 -5.13713285e-02  8.41584802e-02\n",
      " -5.21717183e-02  2.70015523e-02 -6.06019832e-02  6.67507872e-02\n",
      "  9.67155546e-02  9.47949141e-02 -1.70890838e-02 -6.73476160e-02\n",
      "  2.93387454e-02 -1.62215143e-01 -3.64345051e-02  1.06720850e-01\n",
      "  1.13053983e-02  5.38144931e-02 -5.98892234e-02  8.59192312e-02\n",
      "  9.49497223e-02 -5.38322888e-02 -6.84643313e-02 -1.06010884e-01\n",
      " -9.19809863e-02  1.01062536e-01  4.23326939e-02 -8.91820937e-02\n",
      " -9.48470924e-03 -5.99794555e-03  5.45678847e-02  1.95466697e-01\n",
      "  9.99818891e-02 -8.39695185e-02 -1.06593117e-01  6.10944480e-02\n",
      "  1.49142385e-01  1.10690340e-01 -3.90812680e-02  7.86308348e-02\n",
      "  9.38537791e-02 -2.91924439e-02 -5.98246008e-02  1.69358682e-02\n",
      "  2.10155666e-01  5.94129860e-02 -5.30707985e-02 -5.44334985e-02\n",
      " -9.95674431e-02 -1.54171541e-01 -1.36872202e-01  3.95603441e-02\n",
      " -3.21762450e-02  7.44344369e-02 -1.45444786e-03  1.98339060e-01\n",
      "  2.58016914e-01 -7.31731132e-02 -2.78430469e-02 -8.77422020e-02\n",
      " -4.37610596e-02 -3.28980982e-02  1.48785040e-01 -1.25699237e-01\n",
      "  1.48799390e-01 -2.67258789e-02 -1.65224969e-01  1.99796744e-02\n",
      " -1.94624401e-04 -2.79833637e-02 -1.45414320e-03  1.93717834e-02\n",
      " -3.79961953e-02 -6.66726455e-02 -2.77482066e-02  3.50829475e-02]\n"
     ]
    }
   ],
   "source": [
    "print(demo_post.vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token          \tSimilarity to whole document\n",
      "               \t0.000\n",
      "Well           \t0.832\n",
      ",              \t0.559\n",
      "everyone       \t0.743\n",
      "got            \t0.699\n",
      "up             \t0.755\n",
      "and            \t0.714\n",
      "going          \t0.788\n",
      "this           \t0.715\n",
      "morning        \t0.581\n",
      ".              \t0.626\n",
      "It             \t0.840\n",
      "'s             \t0.530\n",
      "still          \t0.805\n",
      "raining        \t0.413\n",
      ",              \t0.559\n",
      "but            \t0.863\n",
      "that           \t0.832\n",
      "'s             \t0.530\n",
      "okay           \t0.625\n",
      "with           \t0.655\n",
      "me             \t0.708\n",
      ".              \t0.626\n",
      "Sort           \t0.671\n",
      "of             \t0.601\n",
      "suits          \t0.408\n",
      "my             \t0.663\n",
      "mood           \t0.488\n",
      ".              \t0.626\n",
      "I              \t0.606\n",
      "could          \t0.796\n",
      "easily         \t0.576\n",
      "have           \t0.773\n",
      "stayed         \t0.516\n",
      "home           \t0.597\n",
      "in             \t0.592\n",
      "bed            \t0.463\n",
      "with           \t0.655\n",
      "my             \t0.663\n",
      "book           \t0.490\n",
      "and            \t0.714\n",
      "the            \t0.678\n",
      "cats           \t0.429\n",
      ".              \t0.626\n",
      "This           \t0.715\n",
      "has            \t0.661\n",
      "been           \t0.712\n",
      "a              \t0.614\n",
      "lot            \t0.685\n",
      "of             \t0.601\n",
      "rain           \t0.498\n",
      "though         \t0.821\n",
      "!              \t0.495\n",
      "People         \t0.689\n",
      "have           \t0.773\n",
      "wet            \t0.470\n",
      "basements      \t0.230\n",
      ",              \t0.559\n",
      "there          \t0.856\n",
      "are            \t0.658\n",
      "lakes          \t0.339\n",
      "where          \t0.775\n",
      "there          \t0.856\n",
      "should         \t0.735\n",
      "be             \t0.778\n",
      "golf           \t0.361\n",
      "courses        \t0.350\n",
      "and            \t0.714\n",
      "fields         \t0.410\n",
      ",              \t0.559\n",
      "everything     \t0.757\n",
      "is             \t0.676\n",
      "green          \t0.497\n",
      ",              \t0.559\n",
      "green          \t0.497\n",
      ",              \t0.559\n",
      "green          \t0.497\n",
      ".              \t0.626\n",
      "But            \t0.863\n",
      ",              \t0.559\n",
      "it             \t0.840\n",
      "is             \t0.676\n",
      "supposed       \t0.714\n",
      "to             \t0.660\n",
      "be             \t0.778\n",
      "26             \t0.256\n",
      "degrees        \t0.365\n",
      "by             \t0.349\n",
      "Friday         \t0.364\n",
      ",              \t0.559\n",
      "so             \t0.846\n",
      "we             \t0.773\n",
      "'ll            \t0.728\n",
      "be             \t0.778\n",
      "dealing        \t0.530\n",
      "with           \t0.655\n",
      "mosquitos      \t0.177\n",
      "next           \t0.655\n",
      "week           \t0.577\n",
      ".              \t0.626\n",
      "I              \t0.606\n",
      "heard          \t0.610\n",
      "Winnipeg       \t0.085\n",
      "described      \t0.480\n",
      "as             \t0.760\n",
      "an             \t0.581\n",
      "\"              \t0.402\n",
      "Old            \t0.548\n",
      "Testament      \t0.358\n",
      "\"              \t0.402\n",
      "city           \t0.520\n",
      "on             \t0.539\n",
      "urlLink        \t0.000\n",
      "CBC            \t-0.006\n",
      "Radio          \t0.356\n",
      "One            \t0.798\n",
      "last           \t0.667\n",
      "week           \t0.577\n",
      "and            \t0.714\n",
      "it             \t0.840\n",
      "sort           \t0.671\n",
      "of             \t0.601\n",
      "rings          \t0.368\n",
      "true           \t0.593\n",
      ".              \t0.626\n",
      "Floods         \t0.298\n",
      ",              \t0.559\n",
      "infestations   \t0.114\n",
      ",              \t0.559\n",
      "etc            \t0.521\n",
      ".              \t0.626\n",
      ",              \t0.559\n",
      "etc            \t0.521\n",
      "..             \t0.000\n"
     ]
    }
   ],
   "source": [
    "print(f\"{'Token':<15s}\\tSimilarity to whole document\")\n",
    "for i in demo_post:\n",
    "    print(f\"{i.text:<15s}\\t{demo_post.similarity(i):<.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Classification, Scoring, and Clustering\n",
    "Since we have vectors representing documents (we can also use the bag-of-words representation for this, too!), we can do any classical statistical test or modelling we want, like trying to build a model to predict the age of a post's author from its contents, or predicting their astrological sign.  But we won't use statistics, because we have tools better suited to the tasks at hand: machine learning.\n",
    "\n",
    "Without getting too deep in the weeds, machine learning models are essentially extensions of statistics that are based on _computational mathematics_ rather than _classical mathematics_.  They tend to scale better to massive datasets (i.e. run faster and predict better) and are often more powerful for pure prediction, but they are designed for larger datasets and may not work well at lower sizes.\n",
    "\n",
    "However, ultimately, the distinction between \"machine learning\" and \"statistics\" is a false one.  The real difference is \"experimentalists\" (who seek to understand _causation_) and \"data miners,\" who only want to build models with good _predictive power_.  (I won't get any further into this--but Leo Breiman's 2001 paper [Statistical Modeling: The Two Cultures](https://projecteuclid.org/euclid.ss/1009213726) is an interesting exploration of this divide).\n",
    "\n",
    "Python has some absoltuely top-tier machine learning libraries: Scikit-Learn for non-neural models (e.g. decision trees, random forests, support vector machines), and various environments like Keras, Tensorflow, and Pytorch for building neural networks.  We'll use non-neural models for this demo--they both run faster (as in, by a factor of days, sometimes) and are _much_ easier to interpret.\n",
    "\n",
    "We'll build four models, using two different sets of target variables and two different sets of predictors.\n",
    "\n",
    "Targets: author age (as a regression task); author astrological sign (as a classification task)\n",
    "Predictors: document vectors, generated by spaCy; bag-of-words model with tf-idf weighting.\n",
    "\n",
    "We'll just use one model--Support Vector Machines--because they tend to be fast and very powerful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need a lot more imports for this than before\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.corpora.mmcorpus import MmCorpus\n",
    "from gensim.matutils import corpus2csc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, MaxAbsScaler\n",
    "from sklearn.svm import LinearSVC, LinearSVR\n",
    "from scipy.sparse import csr_matrix\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading in ages.\n",
      "Reading in astrological signs.\n"
     ]
    }
   ],
   "source": [
    "print(\"Reading in ages.\")\n",
    "ages = pd.read_csv(\n",
    "    \"corpus data files/Blog Data.csv\",\n",
    "    usecols=[\"Age\"],\n",
    "    squeeze=True\n",
    ").values.astype(np.float32)\n",
    "print(\"Reading in astrological signs.\")\n",
    "signs = pd.read_csv(\n",
    "    \"corpus data files/Blog Data.csv\",\n",
    "    usecols=[\"Sign\"],\n",
    "    squeeze=True\n",
    ").values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading bag-of-words corpus.\n",
      "Converting bag-of-words corpus to csr_matrix.\n"
     ]
    }
   ],
   "source": [
    "# make the bag-of-words matrix.\n",
    "print(\"Loading bag-of-words corpus.\")\n",
    "csr_corpus = list(MmCorpus(\"corpus data files/gensim_corpus.mm\"))\n",
    "print(\"Converting bag-of-words corpus to csr_matrix.\")\n",
    "# need to convert term-document matrix to document-term\n",
    "# with transpose, then cast to csr for better speed\n",
    "# in scikit-learn\n",
    "csr_corpus = corpus2csc(csr_corpus).transpose().tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Make the spaCy vector matrix--this will take ~1 hour\n",
    "print(\"Loading spaCy model.\")\n",
    "nlp = spacy.load(\n",
    "    \"en_core_web_sm\",\n",
    "    disable=[\"parser\", \"tagger\", \"ner\"] # don't need these for vectors\n",
    ")\n",
    "print(\"Loading blog posts.\")\n",
    "vector_corpus = list(pd.read_csv(\n",
    "    \"corpus data files/Blog Data.csv\",\n",
    "    usecols=[\"Post\"],\n",
    "    squeeze=True\n",
    "))\n",
    "print(\"Retrieving vectors.\")\n",
    "vector_corpus = np.array([\n",
    "    nlp(i).vector\n",
    "    for i in tqdm(vector_corpus, desc=\"spaCy vectors\")\n",
    "])\n",
    "\n",
    "# Save this beast of a matrix to re-load later--\n",
    "# faster than re-running spaCy's NLP pipeline.\n",
    "np.save(\"corpus data files/GloVe matrix.npy\", vector_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vector Machines are sensitive to _feature scaling._  That is, it expects that all of your features will be _on the same numeric scale._  Fortunately, for our bag-of-words corpus, this is already the case.  For our spaCy/GloVe vector corpus, we'll just assume it's the case for this demo.  We'll use a _linear support vector machine kernel._  SVM's can have different _kernel functions_ that apply some transforms to the data--linear is far and away the fastest one commonly used, but others (like Radial Basis Function kernels, or RBF) can have higher performance, but simply will not run in any reasonably period of time for most use cases.\n",
    "\n",
    "Now let's do the actual modeling.\n",
    "* Use a grid search to exhaustively search for good parameters (within a pre-defined search space).  This is called _(hyper)parameter optimization._\n",
    "* Use 5-fold cross-validation to assess model performance for each parameter combination we try.\n",
    "  * Data is split into 5 equally sized \"folds.\"\n",
    "  * Four folds are used to train the model; the fifth is used to assess its performance.\n",
    "  * All permutations of \"train-on-4, test-on-1\" are performed, and scores are averaged to get the overall model performance.\n",
    "* Print out the best model parameters and the best score for each task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning CSR-Age fit.\n",
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:   58.6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "svr_optimizer = GridSearchCV(\n",
    "    estimator=LinearSVR(verbose=1),\n",
    "    param_grid={\n",
    "        \"C\":10.**np.arange(-5,5),\n",
    "        \"epsilon\":np.logspace(-5, 0, 10)\n",
    "    },\n",
    "    n_jobs=3,\n",
    "    verbose=3,\n",
    "    cv=5,\n",
    "    error_score=0\n",
    ")\n",
    "\n",
    "svc_optimizer = GridSearchCV(\n",
    "    estimator=LinearSVC(verbose=1),\n",
    "    param_grid={\n",
    "        \"C\":10.**np.arange(-5,5),\n",
    "    },\n",
    "    n_jobs=3,\n",
    "    verbose=3,\n",
    "    cv=5,\n",
    "    scoring=make_scorer(f1_score, averaging=\"macro\"),\n",
    "    error_score=0\n",
    ")\n",
    "\n",
    "# fit bow and age\n",
    "print(\"Beginning CSR-Age fit.\")\n",
    "best_age_bow = svr_optimizer.fit(\n",
    "    csr_corpus,\n",
    "    ages\n",
    ")\n",
    "\n",
    "print(\"Beginning GloVe-Age fit.\")\n",
    "best_age_glove = svr_optimizer.fit(\n",
    "    vector_corpus,\n",
    "    ages\n",
    ")\n",
    "\n",
    "print(\"Beginning CSR-Astrological Sign fit.\")\n",
    "best_sign_bow = svc_optimizer.fit(\n",
    "    csr_corpus,\n",
    "    signs\n",
    ")\n",
    "\n",
    "print(\"Beginning GloVe-Astrological Sign fit.\")\n",
    "best_sign_glove = svc_optimizer.fit(\n",
    "    vector_corpus,\n",
    "    signs\n",
    ")\n",
    "\n",
    "print(\"CSR-Age model:\")\n",
    "print(best_age_bow.best_score_)\n",
    "print(best_age_bow.best_params_)\n",
    "\n",
    "print(\"\\nGloVe-Age model:\")\n",
    "print(best_age_glove.best_score_)\n",
    "print(best_age_glove.best_params_)\n",
    "\n",
    "print(\"\\nCSR-Sign model:\")\n",
    "print(best_sign_bow.best_score_)\n",
    "print(best_sign_bow.best_params_)\n",
    "\n",
    "print(\"\\nCSR-Sign model:\")\n",
    "print(best_sign_glove.best_score_)\n",
    "print(best_sign_glove.best_params_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
